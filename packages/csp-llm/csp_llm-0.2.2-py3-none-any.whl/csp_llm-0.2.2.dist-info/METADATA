Metadata-Version: 2.4
Name: csp_llm
Version: 0.2.2
Summary: Python app using llm via MCP for modeling and solving a csp problem in pycsp3
Author-email: Alain Kemgue <kemgue@cril.fr>
Maintainer-email: Alain Kemgue <kemgue@cril.fr>
License: MIT
Project-URL: Homepage, https://github.com/kemgue/csp_llm
Project-URL: Repository, https://github.com/kemgue/csp_llm
Project-URL: Bug Tracker, https://github.com/kemgue/csp_llm/issues
Keywords: pycsp3,csp,ai,llm,openai,anthropic
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: streamlit>=1.28.0
Requires-Dist: streamlit-ace>=0.1.1
Requires-Dist: pycsp3>=2.3.0
Requires-Dist: openai>=1.0.0
Requires-Dist: anthropic>=0.7.0
Requires-Dist: python-dotenv>=1.0.1
Requires-Dist: aiohttp>=3.12.15
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: flake8>=6.0.0; extra == "dev"
Requires-Dist: mypy>=1.0.0; extra == "dev"
Requires-Dist: pre-commit>=3.0.0; extra == "dev"
Provides-Extra: docs
Requires-Dist: sphinx>=5.0.0; extra == "docs"
Requires-Dist: sphinx-rtd-theme>=1.0.0; extra == "docs"
Dynamic: license-file

## ğŸ’¡ About csp-llm

csp-llm is a python package running as an AI agent to enable the automatic generation and execution of [PyCSP3](https://github.com/xcsp3team/pycsp3/) code for a constraint problem.

It provides an interactive and customizable web user interface, which allows the user to enter or import the description of a constraint problem in natural language. The problem description is then sent to a pre-configured LLM model, which will generate and display the code to the user. The user can either request that the code be executed directly, or make modifications to the generated code before requesting its execution.


The application incorporates modern technologies to run any type of LLM model (LLM models deployed within CRIL, Anthropic models, OpenAI models, Google models, etc.).

You can run the tool in mutiple environments including your local LLM environment with LMSTUDIO, your local environment with OLLAMA and finally with the default environment composed of external modern LLMs and those deployed in the CRIL environment. To get good results in your local environment, make sure you have a machine with good GPU performance where you have installed large and recent versions of the LLM models. Thus, you have the possibility here to test different models.

You will see further down in this document the option to choose to launch in a particular environment.


Once installed, the application offers a few examples of constraint problems with which the user can have fun testing.


**P.S:** *It may happen that the generated code contains errors, in which case the user is given the opportunity to correct it via the interface. Also note that LLMs can make mistakes and hallucinations, so more efficient model is, more likely the generated code will contain fewer errors*.

## ğŸ›‘ Requirements

- Run on Linux and Mac platforms (tested on bash linux and zsh mac).
- Have access to an LLM platform. LLM templates from CRIL are proposed by default. Those with a LAN account can use their API key. For further information, please contact Alain Kemgue( kemgue@cril.fr )
- Have installed a version of python3 (**3.10** or higher)
- Have installed a version of java to run pycsp3 (java 8 or higher)

## ğŸ“¦ Installation

We recommend installing the application in a python virtual environment.

### Virtual environment installation

```bash
python3 -m venv venv
source venv/bin/activate

```

### Installing the csp-llm package on PyPi

```bash
pip install csp-llm

```

### Launch the application

```bash
(venv) ordi@alain% launch-csp-llm     
ğŸš€ Launching the application...
ğŸ’¡ Application dependencies
missing ScriptRunContext! This warning can be ignored when running in bare mode.
âœ… anthropic 0.60.0
âœ… openai 1.97.1
âœ… streamlit 1.47.1
âœ… streamlit_ace 0.1.1
âœ… dotenv
âœ… aiohttp 3.12.15
âœ… pycsp3 2.5.1
âœ… Java 21.0.8 detected (>= 8)
ğŸŒ Application available at: http://localhost:8501
ğŸ’¡ LLM environment : GENERIC_CRIL_LLM
ğŸ’¡ Press Ctrl+C to stop
--------------------------------------------------

  You can now view your Streamlit app in your browser.

  URL: http://localhost:8501
```

The application is then available at http://localhost:8501 with generic external llm models.

You can change port and host by passing parameters to the launch script.

```bash

(venv) ordi@alain% launch-csp-llm --help              
usage: csp_llm.py [-h] [--host HOST] [--port PORT] [--llm_env {local_ollama,local_lmstudio,generic_cril_llm}] [-ev]

Launch the application

options:
  -h, --help            show this help message and exit
  --host HOST           ip host of the web server. Default host is 'localhost'. Use 0.0.0.0 to make app accessible on the entire network 
  --port PORT           server's listening port. Default port is 8501
  --llm_env {local_ollama,local_lmstudio,generic_cril_llm}
                        default value is 'generic_cril_llm' 
                        use --llm_env option to specify your LLM environnement 
                        value 'local_ollama' to use local ollama installed on your computer 
                        value 'local_lmstudio' to use local lmstudio installed on your computer 
                        value 'generic_cril_llm' to use external LLM + CRIL plateform
  -ev                   verbose mode
```
Example of launching the application on port **3000** and host **0.0.0.0**( makes the application accessible on the entire network ) and in a local **lmstudio** environment

```bash
(venv) ordi@alain% launch-csp-llm --port 3000 --host 0.0.0.0  --llm_env local_lmstudio
ğŸš€ Launching the application...
ğŸ’¡ Application dependencies
âœ… anthropic 0.60.0
âœ… openai 1.97.1
âœ… streamlit 1.47.1
âœ… streamlit_ace 0.1.1
âœ… dotenv
âœ… aiohttp 3.12.15
âœ… pycsp3 2.5.1
âœ… Java 21.0.8 detected (>= 8)
ğŸŒ Application available at: http://0.0.0.0:3000
ğŸ’¡ LLM environment : LOCAL_LMSTUDIO
ğŸ’¡ Press Ctrl+C to stop
--------------------------------------------------

  You can now view your Streamlit app in your browser.

  URL: http://0.0.0.0:3000

```
