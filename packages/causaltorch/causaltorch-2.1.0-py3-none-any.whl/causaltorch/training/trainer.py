"""
Production-Ready Causal Model Trainer
====================================

A robust, production-grade training system for causal AI models.
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from typing import Dict, List, Optional, Union, Callable, Any
import logging
from pathlib import Path
import json
import time
from contextlib import contextmanager

# Optional W&B integration
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    wandb = None

from ..metrics import calculate_cfs, temporal_consistency
from ..rules import CausalRuleSet


class CausalTrainer:
    """Production-ready trainer for causal AI models.
    
    Features:
    - Robust error handling and recovery
    - Comprehensive logging and monitoring
    - Automatic checkpointing and model versioning
    - Multiple optimization strategies
    - Built-in validation and early stopping
    - Integration with experiment tracking (W&B, MLflow)
    """
    
    def __init__(
        self,
        model: nn.Module,
        causal_rules: CausalRuleSet,
        config: Dict[str, Any],
        experiment_name: Optional[str] = None,
        checkpoint_dir: Optional[str] = None
    ):
        self.model = model
        self.causal_rules = causal_rules
        self.config = config
        self.experiment_name = experiment_name or f"causal_exp_{int(time.time())}"
        self.checkpoint_dir = Path(checkpoint_dir or "./checkpoints")
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        # Setup logging
        self.logger = self._setup_logging()
        
        # Initialize training components
        self.optimizer = self._setup_optimizer()
        self.scheduler = self._setup_scheduler()
        self.criterion = self._setup_loss_functions()
        
        # Training state
        self.current_epoch = 0
        self.global_step = 0
        self.best_metric = float('inf')
        self.patience_counter = 0
        
        # Metrics tracking
        self.train_metrics = []
        self.val_metrics = []
        
        # Initialize experiment tracking
        if config.get('use_wandb', False) and WANDB_AVAILABLE:
            self._init_wandb()
        elif config.get('use_wandb', False) and not WANDB_AVAILABLE:
            self.logger.warning("W&B requested but not available. Install with: pip install wandb")
    
    def _setup_logging(self) -> logging.Logger:
        """Setup comprehensive logging."""
        logger = logging.getLogger(f"causaltorch.trainer.{self.experiment_name}")
        logger.setLevel(logging.INFO)
        
        # File handler
        log_file = self.checkpoint_dir / f"{self.experiment_name}.log"
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.INFO)
        
        # Console handler
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # Formatter
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        
        return logger
    
    def _setup_optimizer(self) -> optim.Optimizer:
        """Setup optimizer with causal-aware parameter grouping."""
        causal_params = []
        regular_params = []
        
        for name, param in self.model.named_parameters():
            if 'causal' in name.lower() or 'rule' in name.lower():
                causal_params.append(param)
            else:
                regular_params.append(param)
        
        optimizer_class = getattr(optim, self.config.get('optimizer', 'AdamW'))
        
        # Different learning rates for causal vs regular parameters
        param_groups = [
            {
                'params': causal_params, 
                'lr': self.config.get('causal_lr', 1e-4),
                'weight_decay': self.config.get('causal_weight_decay', 1e-5)
            },
            {
                'params': regular_params, 
                'lr': self.config.get('lr', 1e-3),
                'weight_decay': self.config.get('weight_decay', 1e-4)
            }
        ]
        
        return optimizer_class(param_groups)
    
    def _setup_scheduler(self) -> Optional[optim.lr_scheduler._LRScheduler]:
        """Setup learning rate scheduler."""
        if not self.config.get('use_scheduler', True):
            return None
            
        scheduler_type = self.config.get('scheduler', 'cosine')
        
        if scheduler_type == 'cosine':
            return optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer, 
                T_max=self.config.get('max_epochs', 100)
            )
        elif scheduler_type == 'plateau':
            return optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer, 
                mode='min', 
                factor=0.5, 
                patience=5
            )
        else:
            raise ValueError(f"Unsupported scheduler: {scheduler_type}")
    
    def _setup_loss_functions(self) -> Dict[str, nn.Module]:
        """Setup multiple loss functions for causal training."""
        losses = {}
        
        # Primary task loss
        if self.config.get('task') == 'classification':
            losses['task'] = nn.CrossEntropyLoss()
        elif self.config.get('task') == 'regression':
            losses['task'] = nn.MSELoss()
        else:
            losses['task'] = nn.CrossEntropyLoss()  # default
        
        # Causal consistency loss
        losses['causal'] = CausalConsistencyLoss(self.causal_rules)
        
        # Regularization losses
        losses['sparsity'] = SparsityLoss()
        losses['smoothness'] = SmoothnessLoss()
        
        return losses
    
    @contextmanager
    def _error_handling(self, context: str):
        """Context manager for robust error handling."""
        try:
            yield
        except Exception as e:
            self.logger.error(f"Error in {context}: {str(e)}")
            # Save emergency checkpoint
            self._save_checkpoint(f"emergency_{context}")
            raise
    
    def train_epoch(self, train_loader: DataLoader) -> Dict[str, float]:
        """Train for one epoch with comprehensive monitoring."""
        self.model.train()
        epoch_metrics = {
            'loss': 0.0,
            'task_loss': 0.0,
            'causal_loss': 0.0,
            'causal_fidelity': 0.0
        }
        
        num_batches = len(train_loader)
        
        with self._error_handling("train_epoch"):
            for batch_idx, batch in enumerate(train_loader):
                # Move to device
                if torch.cuda.is_available():
                    batch = {k: v.cuda() if torch.is_tensor(v) else v 
                            for k, v in batch.items()}
                
                # Forward pass
                self.optimizer.zero_grad()
                outputs = self.model(batch['input'])
                
                # Compute losses
                losses = self._compute_losses(outputs, batch)
                total_loss = self._combine_losses(losses)
                
                # Backward pass
                total_loss.backward()
                
                # Gradient clipping
                if self.config.get('grad_clip'):
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(), 
                        self.config['grad_clip']
                    )
                
                self.optimizer.step()
                
                # Update metrics
                batch_size = batch['input'].size(0)
                for key, value in losses.items():
                    epoch_metrics[key] += value.item() * batch_size
                
                # Compute causal fidelity
                if batch_idx % 10 == 0:  # Sample every 10 batches
                    cfs = self._compute_causal_fidelity(outputs, batch)
                    epoch_metrics['causal_fidelity'] += cfs * batch_size
                
                self.global_step += 1
                
                # Log batch metrics
                if batch_idx % self.config.get('log_interval', 100) == 0:
                    self.logger.info(
                        f"Epoch {self.current_epoch}, Batch {batch_idx}/{num_batches}, "
                        f"Loss: {total_loss.item():.4f}"
                    )
        
        # Average metrics
        total_samples = len(train_loader.dataset)
        for key in epoch_metrics:
            epoch_metrics[key] /= total_samples
        
        return epoch_metrics
    
    def validate(self, val_loader: DataLoader) -> Dict[str, float]:
        """Validate model with causal metrics."""
        self.model.eval()
        val_metrics = {
            'loss': 0.0,
            'task_loss': 0.0,
            'causal_loss': 0.0,
            'causal_fidelity': 0.0,
            'temporal_consistency': 0.0
        }
        
        with torch.no_grad():
            with self._error_handling("validation"):
                for batch in val_loader:
                    if torch.cuda.is_available():
                        batch = {k: v.cuda() if torch.is_tensor(v) else v 
                                for k, v in batch.items()}
                    
                    outputs = self.model(batch['input'])
                    losses = self._compute_losses(outputs, batch)
                    
                    batch_size = batch['input'].size(0)
                    for key, value in losses.items():
                        val_metrics[key] += value.item() * batch_size
                    
                    # Causal metrics
                    cfs = self._compute_causal_fidelity(outputs, batch)
                    val_metrics['causal_fidelity'] += cfs * batch_size
                    
                    if 'temporal' in self.config.get('task_type', ''):
                        tc = temporal_consistency(outputs)
                        val_metrics['temporal_consistency'] += tc * batch_size
        
        # Average metrics
        total_samples = len(val_loader.dataset)
        for key in val_metrics:
            val_metrics[key] /= total_samples
        
        return val_metrics
    
    def fit(
        self, 
        train_loader: DataLoader, 
        val_loader: Optional[DataLoader] = None,
        num_epochs: Optional[int] = None
    ) -> Dict[str, List[float]]:
        """Complete training loop with validation and checkpointing."""
        num_epochs = num_epochs or self.config.get('max_epochs', 100)
        
        self.logger.info(f"Starting training for {num_epochs} epochs")
        
        for epoch in range(num_epochs):
            self.current_epoch = epoch
            
            # Training
            train_metrics = self.train_epoch(train_loader)
            self.train_metrics.append(train_metrics)
            
            # Validation
            if val_loader:
                val_metrics = self.validate(val_loader)
                self.val_metrics.append(val_metrics)
                
                # Early stopping
                if self._should_early_stop(val_metrics):
                    self.logger.info(f"Early stopping at epoch {epoch}")
                    break
            
            # Learning rate scheduling
            if self.scheduler:
                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_metrics.get('loss', train_metrics['loss']))
                else:
                    self.scheduler.step()
            
            # Checkpointing
            if epoch % self.config.get('checkpoint_interval', 10) == 0:
                self._save_checkpoint(f"epoch_{epoch}")
            
            # Logging
            self._log_epoch_metrics(epoch, train_metrics, val_metrics)
        
        # Save final model
        self._save_checkpoint("final")
        
        return {
            'train_metrics': self.train_metrics,
            'val_metrics': self.val_metrics
        }
    
    def _compute_losses(self, outputs: torch.Tensor, batch: Dict) -> Dict[str, torch.Tensor]:
        """Compute all loss components."""
        losses = {}
        
        # Task loss
        losses['task_loss'] = self.criterion['task'](outputs, batch['target'])
        
        # Causal consistency loss
        losses['causal_loss'] = self.criterion['causal'](
            outputs, batch.get('causal_context')
        )
        
        # Regularization losses
        if self.config.get('use_sparsity_loss', True):
            losses['sparsity'] = self.criterion['sparsity'](self.model)
        
        if self.config.get('use_smoothness_loss', True):
            losses['smoothness'] = self.criterion['smoothness'](outputs)
        
        return losses
    
    def _combine_losses(self, losses: Dict[str, torch.Tensor]) -> torch.Tensor:
        """Combine multiple loss components with weights."""
        weights = self.config.get('loss_weights', {
            'task_loss': 1.0,
            'causal_loss': 0.5,
            'sparsity': 0.1,
            'smoothness': 0.1
        })
        
        total_loss = 0.0
        for name, loss in losses.items():
            if name in weights:
                total_loss += weights[name] * loss
        
        return total_loss
    
    def _compute_causal_fidelity(self, outputs: torch.Tensor, batch: Dict) -> float:
        """Compute causal fidelity score for the batch."""
        # Convert outputs to appropriate format for CFS calculation
        test_cases = self._create_test_cases(outputs, batch)
        return calculate_cfs(self.model, test_cases)
    
    def _save_checkpoint(self, name: str):
        """Save model checkpoint with metadata."""
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'epoch': self.current_epoch,
            'global_step': self.global_step,
            'config': self.config,
            'train_metrics': self.train_metrics,
            'val_metrics': self.val_metrics,
            'causal_rules': self.causal_rules.to_dict()
        }
        
        checkpoint_path = self.checkpoint_dir / f"{name}.pt"
        torch.save(checkpoint, checkpoint_path)
        self.logger.info(f"Checkpoint saved: {checkpoint_path}")
    
    def _should_early_stop(self, val_metrics: Dict[str, float]) -> bool:
        """Check early stopping criteria."""
        if not self.config.get('early_stopping', True):
            return False
        
        patience = self.config.get('patience', 10)
        metric_name = self.config.get('early_stop_metric', 'loss')
        
        current_metric = val_metrics.get(metric_name, float('inf'))
        
        if current_metric < self.best_metric:
            self.best_metric = current_metric
            self.patience_counter = 0
        else:
            self.patience_counter += 1
        
        return self.patience_counter >= patience
    
    def _log_epoch_metrics(self, epoch: int, train_metrics: Dict, val_metrics: Dict):
        """Log metrics for the epoch."""
        log_msg = f"Epoch {epoch}: "
        log_msg += f"Train Loss: {train_metrics['loss']:.4f}, "
        if val_metrics:
            log_msg += f"Val Loss: {val_metrics['loss']:.4f}, "
            log_msg += f"Causal Fidelity: {val_metrics['causal_fidelity']:.4f}"
        
        self.logger.info(log_msg)
        
        # Weights & Biases logging
        if hasattr(self, 'wandb_run') and self.wandb_run is not None:
            wandb.log({
                'epoch': epoch,
                **{f'train_{k}': v for k, v in train_metrics.items()},
                **{f'val_{k}': v for k, v in val_metrics.items()}
            })
    
    def _init_wandb(self):
        """Initialize Weights & Biases tracking."""
        if not WANDB_AVAILABLE:
            return
        
        self.wandb_run = wandb.init(
            project=self.config.get('wandb_project', 'causaltorch'),
            name=self.experiment_name,
            config=self.config,
            tags=self.config.get('wandb_tags', [])
        )


class CausalConsistencyLoss(nn.Module):
    """Loss function to enforce causal consistency."""
    
    def __init__(self, causal_rules: CausalRuleSet):
        super().__init__()
        self.causal_rules = causal_rules
    
    def forward(self, outputs: torch.Tensor, causal_context: Optional[Dict] = None) -> torch.Tensor:
        """Compute causal consistency loss."""
        if causal_context is None:
            return torch.tensor(0.0, device=outputs.device)
        
        # Implement causal consistency checking
        loss = 0.0
        for rule in self.causal_rules.rules.values():
            # Check if cause-effect relationship is satisfied
            cause_present = causal_context.get(rule.cause, False)
            effect_strength = outputs[..., causal_context.get(f"{rule.effect}_idx", 0)]
            
            if cause_present:
                # If cause is present, effect should be strong
                target_strength = rule.strength
                loss += torch.mean((effect_strength - target_strength) ** 2)
        
        return torch.tensor(loss, device=outputs.device)


class SparsityLoss(nn.Module):
    """L1 regularization for sparsity."""
    
    def forward(self, model: nn.Module) -> torch.Tensor:
        l1_loss = 0.0
        for param in model.parameters():
            l1_loss += torch.sum(torch.abs(param))
        return l1_loss


class SmoothnessLoss(nn.Module):
    """Smoothness regularization for temporal consistency."""
    
    def forward(self, outputs: torch.Tensor) -> torch.Tensor:
        if outputs.dim() < 3:  # No temporal dimension
            return torch.tensor(0.0, device=outputs.device)
        
        # Compute differences between consecutive time steps
        diff = outputs[:, 1:] - outputs[:, :-1]
        return torch.mean(diff ** 2)
