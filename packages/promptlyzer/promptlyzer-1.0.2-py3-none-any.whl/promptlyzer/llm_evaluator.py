"""
LLM-based Dataset Evaluator

LLM kullanarak dataset kalitesini deÄŸerlendirme.
Maliyet optimizasyonu iÃ§in batch processing kullanÄ±r.

Author: Promptlyzer Team
"""

import json
from typing import List, Dict, Tuple


class LLMEvaluator:
    """
    LLM ile dataset kalite deÄŸerlendirmesi
    """
    
    def __init__(self, inference_manager):
        self.inference = inference_manager
        self.evaluation_cache = {}
        
    def evaluate_single_pair(self, input_text: str, output_text: str) -> Tuple[bool, str]:
        """
        TEK bir Q&A pair'i deÄŸerlendir - PAHALI YÃ–NTEM!
        
        Ã–rnek:
        Input: "Python'da liste nasÄ±l oluÅŸturulur?"
        Output: "Python'da liste ÅŸÃ¶yle oluÅŸturulur: my_list = [1, 2, 3]"
        
        LLM'e sorar: Bu iyi bir cevap mÄ±?
        """
        
        prompt = f"""
        DeÄŸerlendir: Bu soru-cevap Ã§ifti kaliteli mi?
        
        SORU: {input_text}
        CEVAP: {output_text}
        
        Kriterler:
        - Cevap soruyu doÄŸru yanÄ±tlÄ±yor mu?
        - Cevap yeterince aÃ§Ä±klayÄ±cÄ± mÄ±?
        - Hata veya yanÄ±ltÄ±cÄ± bilgi var mÄ±?
        
        Sadece "GOOD" veya "BAD" yaz. Tek kelime.
        """
        
        # Bu her seferinde $0.01 civarÄ± maliyet!
        response = self.inference.infer(
            prompt=prompt,
            model="gpt-3.5-turbo",
            temperature=0
        )
        
        evaluation = response.content.strip().upper()
        return evaluation == "GOOD", evaluation
    
    def evaluate_batch(self, qa_pairs: List[Dict]) -> List[Dict]:
        """
        BATCH deÄŸerlendirme - EKONOMÄ°K YÃ–NTEM!
        
        100 pair'i tek seferde deÄŸerlendirir.
        Maliyet: 100 pair iÃ§in $0.02 (tek tek $1.00 yerine!)
        """
        
        # Ã–rnek batch data:
        # [
        #   {"id": 1, "q": "Python nedir?", "a": "Python bir programlama dilidir..."},
        #   {"id": 2, "q": "For loop nasÄ±l?", "a": "For loop ÅŸÃ¶yle: for i in range(10):..."},
        #   ... 98 tane daha
        # ]
        
        batch_prompt = f"""
        AÅŸaÄŸÄ±daki soru-cevap Ã§iftlerini deÄŸerlendir.
        Sadece GOOD olanlarÄ±n ID'lerini virgÃ¼lle ayÄ±rarak yaz.
        
        Kriterler:
        - Cevap soruya uygun
        - Yeterli detay var
        - Hata yok
        
        DATA:
        """
        
        # Her pair'i formatla
        for pair in qa_pairs:
            batch_prompt += f"\nID:{pair['id']} Q:{pair['q'][:100]} A:{pair['a'][:200]}"
        
        batch_prompt += "\n\nGOOD IDs:"
        
        # Tek API call - 100 evaluation!
        response = self.inference.infer(
            prompt=batch_prompt,
            model="gpt-3.5-turbo",
            temperature=0,
            max_tokens=500
        )
        
        # Parse response: "1,5,12,45,67,89"
        try:
            good_ids = [int(id.strip()) for id in response.content.split(",")]
        except:
            good_ids = []
        
        # Mark good ones
        for pair in qa_pairs:
            pair['is_good'] = pair['id'] in good_ids
            pair['evaluated_by'] = 'llm_batch'
        
        return qa_pairs


class SmartCollector:
    """
    AkÄ±llÄ± collector - staged approach ile maliyet optimizasyonu
    """
    
    def __init__(self, inference_manager):
        self.inference = inference_manager
        self.evaluator = LLMEvaluator(inference_manager)
        
        # AÅŸamalÄ± toplama
        self.stage = 1
        self.collected_count = 0
        self.learned_patterns = {
            "good_indicators": [],
            "bad_indicators": []
        }
        
        # Buffer
        self.pending_evaluation = []
        self.validated_dataset = []
    
    def collect(self, session_id: str, input_text: str, output_text: str, model: str, latency: float):
        """
        Ã–RNEK SENARYO:
        
        KullanÄ±cÄ±: "Python'da dosya nasÄ±l okunur?"
        Bot: "Python'da dosya okumak iÃ§in: with open('file.txt', 'r') as f: content = f.read()"
        """
        
        data_point = {
            "id": self.collected_count,
            "session_id": session_id,
            "q": input_text,
            "a": output_text,
            "model": model,
            "latency": latency
        }
        
        self.collected_count += 1
        
        # STAGE 1: Ä°lk 1000 veri - Sadece basit kontrol
        if self.stage == 1:
            if self._basic_quality_check(output_text, latency):
                self.pending_evaluation.append(data_point)
                
            # 1000 veri toplandÄ± mÄ±?
            if self.collected_count >= 1000:
                print("STAGE 1 bitti! 1000 veri toplandÄ±. STAGE 2'ye geÃ§iyoruz...")
                self.stage = 2
                # Ä°lk batch'i deÄŸerlendir
                self._evaluate_pending_batch()
        
        # STAGE 2: 1000-5000 arasÄ± - %10 sampling ile LLM
        elif self.stage == 2:
            # Ã–nce basit kontrol
            if self._basic_quality_check(output_text, latency):
                # %10 ÅŸansla LLM'e gÃ¶nder
                import random
                if random.random() < 0.1:  # %10
                    self.pending_evaluation.append(data_point)
                else:
                    # Ã–ÄŸrendiÄŸimiz pattern'lere gÃ¶re deÄŸerlendir
                    if self._check_learned_patterns(input_text, output_text):
                        data_point['is_good'] = True
                        data_point['evaluated_by'] = 'learned_pattern'
                        self.validated_dataset.append(data_point)
            
            # Her 100 veri'de batch evaluation
            if len(self.pending_evaluation) >= 100:
                self._evaluate_pending_batch()
            
            # 5000'e ulaÅŸtÄ±k mÄ±?
            if self.collected_count >= 5000:
                print("STAGE 2 bitti! Pattern'ler Ã¶ÄŸrenildi. STAGE 3'e geÃ§iyoruz...")
                self.stage = 3
                self._learn_patterns_from_evaluated()
        
        # STAGE 3: 5000+ - Sadece Ã¶ÄŸrenilmiÅŸ pattern'ler
        else:
            # ArtÄ±k LLM'e gerek yok!
            if self._basic_quality_check(output_text, latency):
                if self._check_learned_patterns(input_text, output_text):
                    data_point['is_good'] = True
                    data_point['evaluated_by'] = 'learned_pattern_only'
                    self.validated_dataset.append(data_point)
                    print(f"âœ“ Veri #{self.collected_count} otomatik onaylandÄ± (LLM'siz!)")
    
    def _basic_quality_check(self, output: str, latency: float) -> bool:
        """Basit kalite kontrolÃ¼"""
        if len(output) < 50:
            return False
        if latency > 2000:
            return False
        if any(err in output.lower() for err in ["error", "exception", "failed"]):
            return False
        return True
    
    def _evaluate_pending_batch(self):
        """Bekleyen veriyi batch olarak deÄŸerlendir"""
        if not self.pending_evaluation:
            return
            
        print(f"ðŸ¤– LLM'e {len(self.pending_evaluation)} veri gÃ¶nderiliyor...")
        
        # LLM batch evaluation
        evaluated = self.evaluator.evaluate_batch(self.pending_evaluation)
        
        # Good olanlarÄ± kaydet
        good_count = 0
        for item in evaluated:
            if item.get('is_good'):
                self.validated_dataset.append(item)
                good_count += 1
        
        print(f"âœ“ {good_count}/{len(evaluated)} veri onaylandÄ±")
        
        # Maliyet hesabÄ±
        cost = len(self.pending_evaluation) * 0.0002  # ~$0.02 per 100
        print(f"ðŸ’° Tahmini maliyet: ${cost:.4f}")
        
        self.pending_evaluation.clear()
    
    def _check_learned_patterns(self, input_text: str, output_text: str) -> bool:
        """
        Ã–ÄŸrenilmiÅŸ pattern'lere gÃ¶re kontrol
        
        Ã–rnek Ã¶ÄŸrenilmiÅŸ pattern'ler:
        - "nasÄ±l" sorularÄ± -> kod Ã¶rneÄŸi iÃ§ermeli
        - "nedir" sorularÄ± -> aÃ§Ä±klama iÃ§ermeli
        - Kod iÃ§eren cevaplar -> ``` olmalÄ±
        """
        
        # Good indicators
        if "nasÄ±l" in input_text.lower() and "```" in output_text:
            return True
        if "nedir" in input_text.lower() and len(output_text) > 100:
            return True
        if "Ã¶rnek" in input_text.lower() and ("```" in output_text or "Ã¶rneÄŸin" in output_text.lower()):
            return True
        
        # Bad indicators
        if "sorry" in output_text.lower():
            return False
        if len(output_text.split()) < 10:
            return False
        
        # Default
        return len(output_text) > 100
    
    def _learn_patterns_from_evaluated(self):
        """LLM'den onaylanan verilerden pattern Ã¶ÄŸren"""
        print("ðŸ§  Pattern'ler Ã¶ÄŸreniliyor...")
        
        good_data = [d for d in self.validated_dataset if d.get('is_good')]
        
        # Basit pattern extraction
        for data in good_data[:100]:  # Ä°lk 100 good data
            q = data['q'].lower()
            a = data['a']
            
            # Question patterns
            if "nasÄ±l" in q and "```" in a:
                self.learned_patterns['good_indicators'].append("how_to_with_code")
            if "nedir" in q and len(a) > 150:
                self.learned_patterns['good_indicators'].append("what_is_detailed")
            if "Ã¶rnek" in q and "```" in a:
                self.learned_patterns['good_indicators'].append("example_with_code")
        
        print(f"âœ“ {len(self.learned_patterns['good_indicators'])} pattern Ã¶ÄŸrenildi!")
    
    def get_stats(self):
        """Ä°statistikleri gÃ¶ster"""
        return {
            "stage": self.stage,
            "total_collected": self.collected_count,
            "validated": len(self.validated_dataset),
            "pending": len(self.pending_evaluation),
            "estimated_cost": {
                "stage1": 0,
                "stage2": (self.collected_count - 1000) * 0.1 * 0.0002 if self.stage >= 2 else 0,
                "stage3": 0
            }
        }


# KULLANIM Ã–RNEÄžÄ°:
"""
collector = SmartCollector(inference_manager)

# Ä°lk 1000 veri - MALÄ°YET: $0
for i in range(1000):
    collector.collect(
        session_id="sess_1",
        input_text="Python'da liste nasÄ±l oluÅŸturulur?",
        output_text="Python'da liste ÅŸÃ¶yle oluÅŸturulur: my_list = [1, 2, 3]",
        model="gpt-3.5-turbo",
        latency=245
    )

# 1000-5000 arasÄ± - MALÄ°YET: ~$0.80 (400 veri Ã— $0.002)
# Sadece %10'u LLM'e gider

# 5000+ sonrasÄ± - MALÄ°YET: $0
# ArtÄ±k tamamen pattern-based, LLM'e gerek yok!

print(collector.get_stats())
# Output:
# {
#   "stage": 3,
#   "total_collected": 10000,
#   "validated": 7500,
#   "estimated_cost": {"total": "$0.80"}
# }
"""