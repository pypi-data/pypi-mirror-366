from collections.abc import Callable

import numpy as np
import torch
from torch import nn

from cusrl.module import Module, ModuleFactoryLike
from cusrl.template import ActorCritic, Hook
from cusrl.utils.helper import get_or
from cusrl.utils.normalizer import RunningMeanStd
from cusrl.utils.typing import Array, Slice

__all__ = ["AdversarialMotionPrior"]


class AdversarialMotionPrior(Hook[ActorCritic]):
    """Implements the Adversarial Motion Priors.

    Described in: "AMP: adversarial motion priors for stylized physics-based
    character control",
    https://dl.acm.org/doi/10.1145/3450626.3459670

    This hook modifies the agent's reward and provides a loss objective to train
    a discriminator network. The discriminator is trained to distinguish between
    transitions generated by the agent and transitions from an expert dataset.
    The agent is then rewarded for generating transitions that the discriminator
    classifies as expert-like. This encourages the agent to imitate the style of
    motion from the expert dataset.

    Args:
        discriminator_factory (ModuleFactoryLike):
            A factory for creating the discriminator model. The discriminator
            may not be recurrent.
        dataset_source (str | Array | Callable[[], Array]):
            The source of the expert motion dataset. Can be a file path to a
            `.npy` or `.pt` file, a `numpy.ndarray` / `torch.Tensor`, or a
            callable that returns a `numpy.ndarray` / `torch.Tensor`.
        state_indices (Slice | None, optional):
            A slice object to extract the relevant parts of the state for AMP.
            If `None`, it's assumed that the environment provides an "amp_obs"
            key in the transition dictionary. Defaults to ``None``.
        batch_size (int, optional):
            The batch size for training the discriminator. Defaults to 512.
        reward_scale (float, optional):
            A scaling factor for the calculated style reward. Defaults to 1.0.
        loss_weight (float, optional):
            A weight for the total discriminator loss in the optimization
            objective. Defaults to 1.0.
        grad_penalty_weight (float, optional):
            The weight for the gradient penalty term in the discriminator loss.
            Defaults to 5.0.
    """

    dataset: torch.Tensor
    discriminator: Module
    transition_rms: RunningMeanStd

    # Mutable attributes
    batch_size: int | None
    reward_scale: float
    loss_weight: float
    grad_penalty_weight: float

    def __init__(
        self,
        discriminator_factory: ModuleFactoryLike,
        dataset_source: str | Array | Callable[[], Array],
        state_indices: Slice | None = None,
        batch_size: int | None = 512,
        reward_scale: float = 1.0,
        loss_weight: float = 1.0,
        grad_penalty_weight: float = 5.0,
    ):
        super().__init__()
        self.discriminator_factory = discriminator_factory
        self.dataset_source = dataset_source
        self.state_indices = state_indices
        self.register_mutable("batch_size", batch_size)
        self.register_mutable("reward_scale", reward_scale)
        self.register_mutable("loss_weight", loss_weight)
        self.register_mutable("grad_penalty_weight", grad_penalty_weight)
        self.bce_with_logits_loss = nn.BCEWithLogitsLoss()

    def init(self):
        if isinstance(self.dataset_source, str):
            if self.dataset_source.endswith(".npy"):
                self.dataset = torch.as_tensor(np.load(self.dataset_source), device=self.agent.device)
            elif self.dataset_source.endswith(".pt"):
                self.dataset = torch.load(self.dataset_source, map_location=self.agent.device)
            else:
                raise ValueError(f"Unsupported dataset file format: {self.dataset_source}")
        elif isinstance(self.dataset_source, (torch.Tensor, np.ndarray)):
            self.dataset = self.agent.to_tensor(self.dataset_source)
        elif callable(self.dataset_source):
            self.dataset = self.agent.to_tensor(self.dataset_source())
        else:
            raise ValueError(f"Unsupported dataset_path type: {type(self.dataset_source)}.")

        self.register_module("discriminator", self.discriminator_factory(self.dataset.size(-1), 1))
        self.register_module("transition_rms", RunningMeanStd(self.dataset.size(-1)))

    @torch.no_grad()
    def post_step(self, transition):
        if (amp_transition := transition.pop("amp_obs", None)) is None:
            if self.state_indices is None:
                raise ValueError("AMP observation is not provided and indices are not specified.")
            amp_state = get_or(transition, "state", "observation")[..., self.state_indices]
            amp_next_state = get_or(transition, "next_state", "next_observation")[..., self.state_indices]
            amp_transition = torch.cat([amp_state, amp_next_state], dim=-1)
        if amp_transition.size(-1) != self.dataset.size(-1):
            raise ValueError(
                f"AMP transition size ({amp_transition.size(-1)}) does not match that of dataset"
                f" ({self.dataset.size(-1)})."
            )

        self.transition_rms.update(amp_transition)
        self.transition_rms.update(self.dataset[torch.randint(self.dataset.size(0), (amp_transition.size(0),))])
        with self.agent.autocast():
            logit = self.discriminator(self.transition_rms.normalize(amp_transition))
        style_reward = -torch.log(torch.clamp(1 - 1 / (1 + torch.exp(-logit)), min=1e-4))

        transition["reward"].add_(style_reward)
        transition["amp_transition"] = amp_transition
        self.agent.record(amp_reward=style_reward)

    def objective(self, batch: dict) -> torch.Tensor:
        agent_transition = batch["amp_transition"].flatten(0, -2)
        if self.batch_size is not None:
            batch_size = self.batch_size
            amp_transition_indices = torch.randint(agent_transition.size(0), (batch_size,), device=self.agent.device)
            agent_transition = agent_transition[amp_transition_indices]
        else:
            batch_size = agent_transition.size(0)
        dataset_indices = torch.randint(self.dataset.size(0), (batch_size,), device=self.agent.device)
        expert_transition = self.dataset[dataset_indices]

        agent_transition = self.transition_rms.normalize(agent_transition)
        expert_transition = self.transition_rms.normalize(expert_transition)
        with self.agent.autocast():
            expert_transition.requires_grad_(True)
            agent_logit = self.discriminator(agent_transition)
            agent_disc_loss = self.bce_with_logits_loss(agent_logit, torch.zeros_like(agent_logit))
            expert_logit = self.discriminator(expert_transition)
            expert_disc_loss = self.bce_with_logits_loss(expert_logit, torch.ones_like(expert_logit))
            discrimination_loss = (agent_disc_loss + expert_disc_loss) / 2
            grad_penalty_loss = self._compute_grad_penalty_loss(expert_logit, expert_transition)

        self.agent.record(
            amp_discrimination_loss=discrimination_loss,
            amp_grad_penalty_loss=grad_penalty_loss,
        )
        return (discrimination_loss + grad_penalty_loss * self.grad_penalty_weight) * self.loss_weight

    @staticmethod
    def _compute_grad_penalty_loss(
        outputs: torch.Tensor,
        inputs: torch.Tensor,
    ) -> torch.Tensor:
        gradients = torch.autograd.grad(
            outputs=outputs,
            inputs=inputs,
            grad_outputs=torch.ones_like(outputs),
            create_graph=True,
            retain_graph=True,
            only_inputs=True,
        )[0]
        gradient_penalty = gradients.square().sum(dim=-1).mean()
        return gradient_penalty
