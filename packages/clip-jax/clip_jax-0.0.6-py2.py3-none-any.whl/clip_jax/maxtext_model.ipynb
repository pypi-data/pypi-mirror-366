{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Port model from maxtext to clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import orbax\n",
    "from flax.training import orbax_utils\n",
    "from flax.traverse_util import flatten_dict, unflatten_dict\n",
    "from jax.experimental.mesh_utils import create_device_mesh\n",
    "from jax.experimental.pjit import pjit\n",
    "from jax.sharding import Mesh, NamedSharding, PartitionSpec\n",
    "from partitions import logical_axis_rules\n",
    "from maxtext.layers.models import Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: port checkpoint to maxtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, use custom `_save_checkpoint` of `ckpt = {\"params\": jax_weights}` when converting the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: create a config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from max_utils import unbox_logicallypartioned\n",
    "import pyconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxtext config\n",
    "\n",
    "jax.config.update(\"jax_default_prng_impl\", \"unsafe_rbg\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"0\"\n",
    "\n",
    "pyconfig.initialize(\n",
    "    [\n",
    "        \"/home/boris/maxtext/MaxText/decode.py\",\n",
    "        \"/home/boris/maxtext/MaxText/configs/base.yml\",\n",
    "        \"load_parameters_path=/home/boris/maxtext/test/2024-03-18-16-53/decode-ckpt-maxtext/0/items\",\n",
    "        \"run_name=runner_direct_2024-03-18-16-53\",\n",
    "        \"per_device_batch_size=1\",\n",
    "        \"model_name=mistral-7b\",\n",
    "        \"tokenizer_path=/home/boris/maxtext/input/mistral-7B-v0.1/tokenizer.model\",\n",
    "        \"ici_tensor_parallelism=4\",\n",
    "        \"max_prefill_predict_length=4\",\n",
    "        \"max_target_length=16\",\n",
    "        \"prompt=I love to\",\n",
    "        \"autoregressive_decode_assert=read. I love to read about the Bible. I love\",\n",
    "        \"attention=dot_product\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "pconfig = pyconfig.config\n",
    "#pconfig.get_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mesh\n",
    "mp_devices = 8\n",
    "\n",
    "assert jax.device_count() % mp_devices == 0\n",
    "dp_devices = jax.local_device_count() // mp_devices\n",
    "dev_mesh = create_device_mesh((dp_devices, mp_devices))\n",
    "mesh = Mesh(dev_mesh, (\"data\", \"model\"))\n",
    "\n",
    "# input\n",
    "rng = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for updating config\n",
    "def show(k):\n",
    "    val = f\"{k}={getattr(pconfig, k)},\"\n",
    "    #pyperclip.copy(val)\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_try(config, do_raise=False):\n",
    "    input_shape = (1, 16)\n",
    "\n",
    "    #model = Transformer(pconfig, mesh, quant=None)\n",
    "    model = Transformer(config, mesh, quant=None)\n",
    "\n",
    "    def init_llm(key):\n",
    "        return model.init(\n",
    "            {\"params\": key, \"dropout\": key, \"aqt\": key},\n",
    "            jnp.ones(input_shape, dtype=jnp.int32),\n",
    "            jnp.ones(input_shape, dtype=jnp.int32),\n",
    "        )[\"params\"]\n",
    "\n",
    "    try:\n",
    "        llm_shape = jax.eval_shape(init_llm, rng)\n",
    "        return llm_shape, model\n",
    "    except Exception as e:\n",
    "        if do_raise:\n",
    "            raise(e)\n",
    "        k = str(e).split(\"no attribute \")[-1].split(\"'\")[1]\n",
    "        show(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace(\n",
    "    decoder_block=\"mistral\",\n",
    "    num_experts=1,\n",
    "    vocab_size=32_000,\n",
    "    emb_dim=4096,\n",
    "    mlp_dim=14336,\n",
    "    num_decoder_layers=32,\n",
    "    num_query_heads=32,\n",
    "    normalization_layer_epsilon=1e-05,\n",
    "    head_dim=128,\n",
    "    num_kv_heads=8,\n",
    "    mlp_activations=['silu', 'linear'],\n",
    "    logits_dot_in_fp32=True,\n",
    "    use_iota_embed=False,\n",
    "    use_untrainable_positional_embedding=False,\n",
    "    trainable_position_size=-1,\n",
    "    enable_dropout=False,\n",
    "    dropout_rate=0,\n",
    "    scan_layers=True,\n",
    "    attention=\"dot_product\",\n",
    "    quantize_kvcache=False,\n",
    "    fused_qkv=False,\n",
    "    fused_mlp=False,\n",
    "    record_internal_nn_metrics=0,\n",
    "    logits_via_embedding=False,\n",
    "    # TODO: change\n",
    "    param_scan_axis=1,\n",
    "    # customizable\n",
    "    remat_policy=\"full\",\n",
    "    dtype=\"bfloat16\",\n",
    "    weight_dtype=\"float32\",\n",
    "    max_target_length=16,\n",
    "    # unused\n",
    "    max_prefill_predict_length=4,\n",
    ")\n",
    "\n",
    "do_try(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_shape, model = do_try(config, do_raise=True)\n",
    "llm_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logical_spec = nn.get_partition_spec(llm_shape)\n",
    "logical_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(x for v in flatten_dict(logical_spec).values() for x in v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = logical_axis_rules(\n",
    "        activation_partitioning_dims=1,\n",
    "        parameter_partitioning_dims=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_params_spec = nn.logical_to_mesh(logical_spec, rules)\n",
    "llm_params_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(pjit, in_shardings=None, out_shardings=llm_params_spec)\n",
    "def init_params(logical_params):\n",
    "    return jax.tree_map(lambda x: jnp.zeros(x.shape, dtype=x.dtype), logical_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mesh:\n",
    "    llm_params = init_params(llm_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the maxtext dir\n",
    "model_dir = f\"/home/boris/maxtext/mistral_7b/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "\n",
    "def _restore_checkpoint(ckpt, dir, step):\n",
    "    print(f\"Restoring checkpoint from {dir} at step {step}\")\n",
    "    restore_args = orbax_utils.restore_args_from_target(ckpt, mesh)\n",
    "    orbax_options = orbax.checkpoint.CheckpointManagerOptions()\n",
    "    checkpoint_manager = orbax.checkpoint.CheckpointManager(dir, orbax_checkpointer, orbax_options)\n",
    "    transforms = {}\n",
    "    transforms = None\n",
    "    return checkpoint_manager.restore(\n",
    "        step, ckpt, restore_kwargs={\"restore_args\": restore_args, \"transforms\": transforms}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to unbox for it to work\n",
    "ckpt = _restore_checkpoint({\"params\":unbox_logicallypartioned(llm_params)}, model_dir, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in flatten_dict(ckpt).items():\n",
    "    if jnp.sum(jnp.abs(v)) == 0:\n",
    "        print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unboxed_params = ckpt[\"params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(flatten_dict(unboxed_params).items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(flatten_dict(llm_params).items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_ckpt = flatten_dict(unboxed_params)\n",
    "flattened_llm = flatten_dict(llm_params)\n",
    "len(flattened_ckpt), len(flattened_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(flattened_ckpt.keys()) - set(flattened_llm.keys()), set(flattened_llm.keys()) - set(flattened_ckpt.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in flattened_llm.keys():\n",
    "    print(k, flattened_llm[k].value.shape, flattened_ckpt[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in flattened_llm.keys():\n",
    "    print(k, flattened_llm[k].value.dtype, flattened_ckpt[k].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in flattened_llm.items():\n",
    "    v = v.value\n",
    "    if jnp.sum(jnp.abs(v)) == 0:\n",
    "        print(\"*** zero ***\")\n",
    "    else:\n",
    "        print(\"** non-zero **\")\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in flattened_llm.keys():\n",
    "    flattened_llm[k] = flattened_llm[k].replace_boxed(flattened_ckpt[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in flattened_llm.items():\n",
    "    v = v.value\n",
    "    if jnp.sum(jnp.abs(v)) == 0:\n",
    "        print(\"*** zero ***\")\n",
    "    else:\n",
    "        print(\"** non-zero **\")\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = unflatten_dict(flattened_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the version converted for clip\n",
    "model_dir = \"/home/boris/maxtext/mistral_7b_pretrain\"\n",
    "\n",
    "orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "\n",
    "def _save_checkpoint(ckpt, dir, step):\n",
    "    orbax_options = orbax.checkpoint.CheckpointManagerOptions(create=True)\n",
    "    save_checkpoint_manager = orbax.checkpoint.CheckpointManager(\n",
    "        dir, orbax_checkpointer, orbax_options\n",
    "    )\n",
    "    save_args = orbax_utils.save_args_from_target(ckpt)\n",
    "    save_checkpoint_manager.save(step, ckpt, save_kwargs={\"save_args\": save_args})\n",
    "\n",
    "ckpt = {\"params\": llm}\n",
    "_save_checkpoint(ckpt, model_dir, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we can restore\n",
    "ckpt = _restore_checkpoint({\"params\":llm_params}, model_dir, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace(\n",
    "    decoder_block=\"mistral\",\n",
    "    num_experts=1,\n",
    "    vocab_size=32_000,\n",
    "    emb_dim=4096,\n",
    "    mlp_dim=14336,\n",
    "    num_decoder_layers=32,\n",
    "    num_query_heads=32,\n",
    "    normalization_layer_epsilon=1e-05,\n",
    "    head_dim=128,\n",
    "    num_kv_heads=8,\n",
    "    mlp_activations=['silu', 'linear'],\n",
    "    logits_dot_in_fp32=True,\n",
    "    use_iota_embed=False,\n",
    "    use_untrainable_positional_embedding=False,\n",
    "    trainable_position_size=-1,\n",
    "    enable_dropout=False,\n",
    "    dropout_rate=0,\n",
    "    scan_layers=True,\n",
    "    attention=\"dot_product\",\n",
    "    quantize_kvcache=False,\n",
    "    fused_qkv=False,\n",
    "    fused_mlp=False,\n",
    "    record_internal_nn_metrics=0,\n",
    "    logits_via_embedding=False,\n",
    "    # TODO: change\n",
    "    param_scan_axis=1,\n",
    "    # customizable\n",
    "    remat_policy=\"full\",\n",
    "    dtype=\"bfloat16\",\n",
    "    weight_dtype=\"float32\",\n",
    "    max_target_length=16,\n",
    "    # unused\n",
    "    max_prefill_predict_length=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mesh\n",
    "mp_devices = 8\n",
    "\n",
    "assert jax.device_count() % mp_devices == 0\n",
    "dp_devices = jax.local_device_count() // mp_devices\n",
    "dev_mesh = create_device_mesh((dp_devices, mp_devices))\n",
    "mesh = Mesh(dev_mesh, (\"data\", \"model\"))\n",
    "\n",
    "# input\n",
    "rng = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (1, 16)\n",
    "\n",
    "#model = Transformer(pconfig, mesh, quant=None)\n",
    "model = Transformer(config, mesh, quant=None)\n",
    "\n",
    "def init_llm(key):\n",
    "    return model.init(\n",
    "        {\"params\": key, \"dropout\": key, \"aqt\": key},\n",
    "        jnp.ones(input_shape, dtype=jnp.int32),\n",
    "        jnp.ones(input_shape, dtype=jnp.int32),\n",
    "    )[\"params\"]\n",
    "\n",
    "llm_shape = jax.eval_shape(init_llm, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maxtext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
