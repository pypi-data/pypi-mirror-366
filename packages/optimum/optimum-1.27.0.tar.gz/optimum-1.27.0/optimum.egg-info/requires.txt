transformers>=4.29
torch>=1.11
packaging
numpy
huggingface_hub>=0.8.0

[amd]
optimum-amd

[benchmark]
optuna
tqdm
scikit-learn
seqeval
torchvision
evaluate>=0.2.0

[dev]
accelerate
pytest<=8.0.0
requests
parameterized
pytest-xdist
Pillow
sacremoses
torchvision
torchaudio
einops
timm
scikit-learn
sentencepiece
rjieba
hf_xet
onnxslim>=0.1.53
black~=23.1
ruff==0.1.5

[doc-build]
accelerate

[exporters]
onnx
onnxruntime
protobuf>=3.20.1
transformers<4.54.0,>=4.36

[exporters-gpu]
onnx
onnxruntime-gpu
protobuf>=3.20.1
transformers<4.54.0,>=4.36

[exporters-tf]
onnx
h5py
tf2onnx
onnxruntime
numpy<1.24.0
datasets<=2.16
tensorflow<=2.12.1,>=2.4
transformers<4.38,>=4.36

[furiosa]
optimum-furiosa

[graphcore]
optimum-graphcore

[habana]
optimum-habana>=1.17.0

[intel]
optimum-intel>=1.23.0

[ipex]
optimum-intel[ipex]>=1.23.0

[neural-compressor]
optimum-intel[neural-compressor]>=1.23.0

[neuronx]
optimum-neuron[neuronx]>=0.0.28

[nncf]
optimum-intel[nncf]>=1.23.0

[onnxruntime]
onnx
datasets>=1.2.1
protobuf>=3.20.1
onnxruntime>=1.11.0
transformers<4.54.0,>=4.36

[onnxruntime-gpu]
onnx
datasets>=1.2.1
protobuf>=3.20.1
onnxruntime-gpu>=1.11.0
transformers<4.54.0,>=4.36

[onnxruntime-training]
evaluate
torch-ort
accelerate
datasets>=1.2.1
protobuf>=3.20.1
transformers<4.54.0,>=4.36
onnxruntime-training>=1.11.0

[openvino]
optimum-intel[openvino]>=1.23.0

[quality]
black~=23.1
ruff==0.1.5

[quanto]
optimum-quanto>=0.2.4

[tests]
accelerate
pytest<=8.0.0
requests
parameterized
pytest-xdist
Pillow
sacremoses
torchvision
torchaudio
einops
timm
scikit-learn
sentencepiece
rjieba
hf_xet
onnxslim>=0.1.53
