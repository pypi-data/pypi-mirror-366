
![Shekar](https://amirivojdan.io/wp-content/uploads/2025/01/shekar-lib.png)
![PyPI - Version](https://img.shields.io/pypi/v/shekar?color=00A693)
![GitHub Actions Workflow Status](https://img.shields.io/github/actions/workflow/status/amirivojdan/shekar/test.yml?color=00A693)
![Codecov](https://img.shields.io/codecov/c/github/amirivojdan/shekar?color=00A693)
![PyPI - Downloads](https://img.shields.io/pypi/dm/shekar?color=00A693)
![PyPI - License](https://img.shields.io/pypi/l/shekar?color=00A693)
![PyPI - Python Version](https://img.shields.io/pypi/pyversions/shekar?color=00A693)


<p align="center">
    <em>Simplifying Persian NLP for Modern Applications</em>
</p>

**Shekar** (meaning 'sugar' in Persian) is a Python library for Persian natural language processing, named after the influential satirical story *"ÙØ§Ø±Ø³ÛŒ Ø´Ú©Ø± Ø§Ø³Øª"* (Persian is Sugar) published in 1921 by Mohammad Ali Jamalzadeh.
The story became a cornerstone of Iran's literary renaissance, advocating for accessible yet eloquent expression.

---

### Table of Contents

- [Installation](#installation)
- [Preprocessing](#preprocessing)
  - [Component Overview](#component-overview)
  - [Using Pipelines](#using-pipelines)
  - [Normalizer](#normalizer)
  - [Batch Processing](#batch-processing)
  - [Decorator Support](#decorator-support)
- [Tokenization](#tokenization)
  - [WordTokenizer](#wordtokenizer)
  - [SentenceTokenizer](#sentencetokenizer)
- [Embeddings](#embeddings)
  - [Word Embeddings](#word-embeddings)
  - [Sentence Embeddings](#sentence-embeddings)
- [Part-of-Speech Tagging](#part-of-speech-tagging)
- [Named Entity Recognition (NER)](#named-entity-recognition-ner)
- [Keyword Extraction](#keyword-extraction)
- [WordCloud](#wordcloud)

---

## Installation

To install the package, you can use **`pip`**. Run the following command:

<!-- termynal -->
```bash
$ pip install shekar
```

## Preprocessing

[![Notebook](https://img.shields.io/badge/Notebook-Jupyter-00A693.svg)](examples/preprocessing.ipynb)  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/amirivojdan/shekar/blob/main/examples/preprocessing.ipynb)

Shekar provides a modular, composable system for Persian text preprocessing through `filters`, `normalizers`, `standardizers`, and `maskers`. You can use these independently or combine them using the `Pipeline` class and the `|` operator.

---

### Component Overview

<details>
<summary>Filters / Removers</summary>

| Component | Aliases | Description |
|----------|---------|-------------|
| `DiacriticFilter` | `DiacriticRemover`, `RemoveDiacritics` | Removes Persian/Arabic diacritics |
| `EmojiFilter` | `EmojiRemover`, `RemoveEmojis` | Removes emojis |
| `NonPersianLetterFilter` | `NonPersianRemover`, `RemoveNonPersianLetters` | Removes all non-Persian content (optionally keeps English) |
| `PunctuationFilter` | `PunctuationRemover`, `RemovePunctuations` | Removes all punctuation characters |
| `StopWordFilter` | `StopWordRemover`, `RemoveStopWords` | Removes frequent Persian stopwords |
| `DigitFilter` | `DigitRemover`, `RemoveDigits` | Removes all digit characters |
| `RepeatedLetterFilter` | `RepeatedLetterRemover`, `RemoveRepeatedLetters` | Shrinks repeated letters (e.g. "Ø³Ø³Ø³Ù„Ø§Ù…") |
| `HTMLTagFilter` | `HTMLRemover`, `RemoveHTMLTags` | Removes HTML tags but retains content |
| `HashtagFilter` | `HashtagRemover`, `RemoveHashtags` | Removes hashtags |
| `MentionFilter` | `MentionRemover`, `RemoveMentions` | Removes @mentions |

</details>

<details>
<summary>Normalizers</summary>

| Component | Aliases | Description |
|----------|---------|-------------|
| `DigitNormalizer` | `NormalizeDigits` | Converts English/Arabic digits to Persian |
| `PunctuationNormalizer` | `NormalizePunctuations` | Standardizes punctuation symbols |
| `AlphabetNormalizer` | `NormalizeAlphabets` | Converts Arabic characters to Persian equivalents |
| `ArabicUnicodeNormalizer` | `NormalizeArabicUnicodes` | Replaces Arabic presentation forms (e.g. ï·½) with Persian equivalents |

</details>

<details>
<summary>Standardizers</summary>

| Component | Aliases | Description |
|----------|---------|-------------|
| `SpacingStandardizer` | `StandardizeSpacings` | Removes extra spaces and fixes spacing around words |
| `PunctuationSpacingStandardizer` | `StandardizePunctuationSpacings` | Adjusts spacing around punctuation marks |

</details>

<details>
<summary>Maskers</summary>

| Component | Aliases | Description |
|----------|---------|-------------|
| `EmailMasker` | `MaskEmails` | Masks or removes email addresses |
| `URLMasker` | `MaskURLs` | Masks or removes URLs |

</details>

---

### Using Pipelines

You can combine any of the preprocessing components using the `|` operator:

```python
from shekar.preprocessing import EmojiRemover, PunctuationRemover

text = "Ø² Ø§ÛŒØ±Ø§Ù† Ø¯Ù„Ø´ ÛŒØ§Ø¯ Ú©Ø±Ø¯ Ùˆ Ø¨Ø³ÙˆØ®Øª! ğŸŒğŸ‡®ğŸ‡·"
pipeline = EmojiRemover() | PunctuationRemover()
output = pipeline(text)
print(output)
```

```shell
Ø² Ø§ÛŒØ±Ø§Ù† Ø¯Ù„Ø´ ÛŒØ§Ø¯ Ú©Ø±Ø¯ Ùˆ Ø¨Ø³ÙˆØ®Øª
```

---

### Normalizer

The built-in `Normalizer` class wraps the most common filters and normalizers:

```python
from shekar import Normalizer

normalizer = Normalizer()
text = "Û¿Ø¯Ù Ù…Ø§ Ø»Ù…Ú« Ø¨Û€ ÛÚªÚ‰ÙŠÚ±Ú• Ø£ÚšÙ¼"
print(normalizer(text))
```

```shell
Ù‡Ø¯Ù Ù…Ø§ Ú©Ù…Ú© Ø¨Ù‡ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ø§Ø³Øª
```

---

### Batch Processing

Both `Normalizer` and `Pipeline` support memory-efficient batch processing:

```python
texts = [
    "Ù¾Ø±Ù†Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ğŸ” Ù‚ÙØ³ÛŒØŒ Ø¹Ø§Ø¯Øª Ø¯Ø§Ø±Ù† Ø¨Ù‡ Ø¨ÛŒâ€ŒÚ©Ø³ÛŒ!",
    "ØªÙˆ Ø±Ø§ Ù…Ù† Ú†Ø´Ù…ğŸ‘€ Ø¯Ø± Ø±Ø§Ù‡Ù…!"
]
outputs = normalizer.fit_transform(texts)
print(list(outputs))
```

```shell
["Ù¾Ø±Ù†Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ  Ù‚ÙØ³ÛŒ Ø¹Ø§Ø¯Øª Ø¯Ø§Ø±Ù† Ø¨Ù‡ Ø¨ÛŒâ€ŒÚ©Ø³ÛŒ", "ØªÙˆ Ø±Ø§ Ù…Ù† Ú†Ø´Ù… Ø¯Ø± Ø±Ø§Ù‡Ù…"]
```

---

### Decorator Support

Use `.on_args(...)` to apply the pipeline to specific function arguments:

```python
@normalizer.on_args(["text"])
def process_text(text):
    return text

print(process_text("ØªÙˆ Ø±Ø§ Ù…Ù† Ú†Ø´Ù…ğŸ‘€ Ø¯Ø± Ø±Ø§Ù‡Ù…!"))
```

```shell
ØªÙˆ Ø±Ø§ Ù…Ù† Ú†Ø´Ù… Ø¯Ø± Ø±Ø§Ù‡Ù…
```

## Tokenization

### WordTokenizer
The WordTokenizer class in Shekar is a simple, rule-based tokenizer for Persian that splits text based on punctuation and whitespace using Unicode-aware regular expressions.

```python
from shekar import WordTokenizer

tokenizer = WordTokenizer()

text = "Ú†Ù‡ Ø³ÛŒØ¨â€ŒÙ‡Ø§ÛŒ Ù‚Ø´Ù†Ú¯ÛŒ! Ø­ÛŒØ§Øª Ù†Ø´Ø¦Ù‡Ù” ØªÙ†Ù‡Ø§ÛŒÛŒ Ø§Ø³Øª."
tokens = list(tokenizer(text))
print(tokens)
```

```shell
["Ú†Ù‡", "Ø³ÛŒØ¨â€ŒÙ‡Ø§ÛŒ", "Ù‚Ø´Ù†Ú¯ÛŒ", "!", "Ø­ÛŒØ§Øª", "Ù†Ø´Ø¦Ù‡Ù”", "ØªÙ†Ù‡Ø§ÛŒÛŒ", "Ø§Ø³Øª", "."]
```

### SentenceTokenizer

The `SentenceTokenizer` class is designed to split a given text into individual sentences. This class is particularly useful in natural language processing tasks where understanding the structure and meaning of sentences is important. The `SentenceTokenizer` class can handle various punctuation marks and language-specific rules to accurately identify sentence boundaries.

Below is an example of how to use the `SentenceTokenizer`:

```python
from shekar.tokenizers import SentenceTokenizer

text = "Ù‡Ø¯Ù Ù…Ø§ Ú©Ù…Ú© Ø¨Ù‡ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ø§Ø³Øª! Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø¨Ø§ Ù‡Ù… Ú©Ø§Ø± Ú©Ù†ÛŒÙ…."
tokenizer = SentenceTokenizer()
sentences = tokenizer(text)

for sentence in sentences:
    print(sentence)
```

```output
Ù‡Ø¯Ù Ù…Ø§ Ú©Ù…Ú© Ø¨Ù‡ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ø§Ø³Øª!
Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø¨Ø§ Ù‡Ù… Ú©Ø§Ø± Ú©Ù†ÛŒÙ….
```

## Embeddings

**Shekar** offers two main embedding classes:

- **`WordEmbedder`**: Provides static word embeddings using pre-trained FastText models.
- **`SentenceEmbedder`**: Provides contextual embeddings using a fine-tuned ALBERT model.

Both classes share a consistent interface:

- `embed(text)` returns a NumPy vector.
- `transform(text)` is an alias for `embed(text)` to integrate with pipelines.

---

### Word Embeddings

`WordEmbedder` supports two static FastText models:

- **`fasttext-d100`**: A 100-dimensional CBOW model trained on [Persian Wikipedia](https://huggingface.co/datasets/codersan/Persian-Wikipedia-Corpus).
- **`fasttext-d300`**: A 300-dimensional CBOW model trained on the large-scale [Naab dataset](https://huggingface.co/datasets/SLPL/naab).

> **Note:** The word embeddings are static due to Gensimâ€™s outdated dependencies, which can lead to compatibility issues. To ensure stability, the embeddings are stored as pre-computed vectors.

```python
from shekar.embeddings import WordEmbedder

embedder = WordEmbedder(model="fasttext-d100")

embedding = embedder("Ú©ØªØ§Ø¨")
print(embedding.shape)

similar_words = embedder.most_similar("Ú©ØªØ§Ø¨", top_n=5)
print(similar_words)
```

### Sentence Embeddings
SentenceEmbedder uses an ALBERT model trained with Masked Language Modeling (MLM) on the Naab dataset to generate high-quality contextual embeddings.
The resulting embeddings are 768-dimensional vectors representing the semantic meaning of entire phrases or sentences.

```python
from shekar.embeddings import SentenceEmbedder

embedder = SentenceEmbedder(model="albert")

sentence = "Ú©ØªØ§Ø¨â€ŒÙ‡Ø§ Ø¯Ø±ÛŒÚ†Ù‡â€ŒØ§ÛŒ Ø¨Ù‡ Ø¬Ù‡Ø§Ù† Ø¯Ø§Ù†Ø´ Ù‡Ø³ØªÙ†Ø¯."
embedding = embedder(sentence)
print(embedding.shape)  # (768,)
```

## Part-of-Speech Tagging

The POSTagger class provides part-of-speech tagging for Persian text using a transformer-based model (default: ALBERT). It returns one tag per word based on Universal POS tags (following the Universal Dependencies standard).

Example usage:

```python
from shekar.pos import POSTagger

pos_tagger = POSTagger()
text = "Ù†ÙˆØ±ÙˆØ²ØŒ Ø¬Ø´Ù† Ø³Ø§Ù„ Ù†Ùˆ Ø§ÛŒØ±Ø§Ù†ÛŒØŒ Ø¨ÛŒØ´ Ø§Ø² Ø³Ù‡ Ù‡Ø²Ø§Ø± Ø³Ø§Ù„ Ù‚Ø¯Ù…Øª Ø¯Ø§Ø±Ø¯ Ùˆ Ø¯Ø± Ú©Ø´ÙˆØ±Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¬Ø´Ù† Ú¯Ø±ÙØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯."

result = pos_tagger(text)
for word, tag in result:
    print(f"{word}: {tag}")
```

```output
Ù†ÙˆØ±ÙˆØ²: PROPN
ØŒ: PUNCT
Ø¬Ø´Ù†: NOUN
Ø³Ø§Ù„: NOUN
Ù†Ùˆ: ADJ
Ø§ÛŒØ±Ø§Ù†ÛŒ: ADJ
ØŒ: PUNCT
Ø¨ÛŒØ´: ADJ
Ø§Ø²: ADP
Ø³Ù‡: NUM
Ù‡Ø²Ø§Ø±: NUM
Ø³Ø§Ù„: NOUN
Ù‚Ø¯Ù…Øª: NOUN
Ø¯Ø§Ø±Ø¯: VERB
Ùˆ: CCONJ
Ø¯Ø±: ADP
Ú©Ø´ÙˆØ±Ù‡Ø§ÛŒ: NOUN
Ù…Ø®ØªÙ„Ù: ADJ
Ø¬Ø´Ù†: NOUN
Ú¯Ø±ÙØªÙ‡: VERB
Ù…ÛŒâ€ŒØ´ÙˆØ¯: VERB
.: PUNCT
```

## Named Entity Recognition (NER)

[![Notebook](https://img.shields.io/badge/Notebook-Jupyter-00A693.svg)](examples/ner.ipynb)  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/amirivojdan/shekar/blob/main/examples/ner.ipynb)

The `NER` module in **Shekar** offers a fast, quantized Named Entity Recognition pipeline using a fine-tuned ALBERT model in ONNX format. It detects common Persian entities such as persons, locations, organizations, and dates. This model is designed for efficient inference and can be easily combined with other preprocessing steps.

---

Example usage:

```python
from shekar import NER
from shekar import Normalizer

input_text = (
    "Ø´Ø§Ù‡Ø±Ø® Ù…Ø³Ú©ÙˆØ¨ Ø¨Ù‡ Ø³Ø§Ù„Ù Û±Û³Û°Û´ Ø¯Ø± Ø¨Ø§Ø¨Ù„ Ø²Ø§Ø¯Ù‡ Ø´Ø¯ Ùˆ Ø¯ÙˆØ±Ù‡ Ø§Ø¨ØªØ¯Ø§ÛŒÛŒ Ø±Ø§ Ø¯Ø± ØªÙ‡Ø±Ø§Ù† Ùˆ Ø¯Ø± Ù…Ø¯Ø±Ø³Ù‡ Ø¹Ù„Ù…ÛŒÙ‡ Ù¾Ø´Øª "
    "Ù…Ø³Ø¬Ø¯ Ø³Ù¾Ù‡Ø³Ø§Ù„Ø§Ø± Ú¯Ø°Ø±Ø§Ù†Ø¯. Ø§Ø² Ú©Ù„Ø§Ø³ Ù¾Ù†Ø¬Ù… Ø§Ø¨ØªØ¯Ø§ÛŒÛŒ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ø±Ù…Ø§Ù† Ùˆ Ø¢Ø«Ø§Ø± Ø§Ø¯Ø¨ÛŒ Ø±Ø§ Ø´Ø±ÙˆØ¹ Ú©Ø±Ø¯. Ø§Ø² Ù‡Ù…Ø§Ù† Ø²Ù…Ø§Ù† "
    "Ø¯Ø± Ø¯Ø¨ÛŒØ±Ø³ØªØ§Ù† Ø§Ø¯Ø¨ Ø§ØµÙÙ‡Ø§Ù† Ø§Ø¯Ø§Ù…Ù‡ ØªØ­ØµÛŒÙ„ Ø¯Ø§Ø¯. Ù¾Ø³ Ø§Ø² Ù¾Ø§ÛŒØ§Ù† ØªØ­ØµÛŒÙ„Ø§Øª Ø¯Ø¨ÛŒØ±Ø³ØªØ§Ù† Ø¯Ø± Ø³Ø§Ù„ Û±Û³Û²Û´ Ø§Ø² Ø§ØµÙÙ‡Ø§Ù† Ø¨Ù‡ ØªÙ‡Ø±Ø§Ù† Ø±ÙØª Ùˆ "
    "Ø¯Ø± Ø±Ø´ØªÙ‡ Ø­Ù‚ÙˆÙ‚ Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØªÙ‡Ø±Ø§Ù† Ù…Ø´ØºÙˆÙ„ Ø¨Ù‡ ØªØ­ØµÛŒÙ„ Ø´Ø¯."
)

normalizer = Normalizer()
normalized_text = normalizer(input_text)

albert_ner = NER()
entities = albert_ner(normalized_text)

for text, label in entities:
    print(f"{text} â†’ {label}")
```

```output
Ø´Ø§Ù‡Ø±Ø® Ù…Ø³Ú©ÙˆØ¨ â†’ PER
Ø³Ø§Ù„ Û±Û³Û°Û´ â†’ DAT
Ø¨Ø§Ø¨Ù„ â†’ LOC
Ø¯ÙˆØ±Ù‡ Ø§Ø¨ØªØ¯Ø§ÛŒÛŒ â†’ DAT
ØªÙ‡Ø±Ø§Ù† â†’ LOC
Ù…Ø¯Ø±Ø³Ù‡ Ø¹Ù„Ù…ÛŒÙ‡ â†’ LOC
Ù…Ø³Ø¬Ø¯ Ø³Ù¾Ù‡Ø³Ø§Ù„Ø§Ø± â†’ LOC
Ø¯Ø¨ÛŒØ±Ø³ØªØ§Ù† Ø§Ø¯Ø¨ Ø§ØµÙÙ‡Ø§Ù† â†’ LOC
Ø¯Ø± Ø³Ø§Ù„ Û±Û³Û²Û´ â†’ DAT
Ø§ØµÙÙ‡Ø§Ù† â†’ LOC
ØªÙ‡Ø±Ø§Ù† â†’ LOC
Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØªÙ‡Ø±Ø§Ù† â†’ ORG
ÙØ±Ø§Ù†Ø³Ù‡ â†’ LOC
```

You can seamlessly chain `NER` with other components using the `|` operator:

```python
ner_pipeline = normalizer | albert_ner
entities = ner_pipeline(input_text)

for text, label in entities:
    print(f"{text} â†’ {label}")
```

This chaining enables clean and readable code, letting you build custom NLP flows with preprocessing and tagging in one pass.

## Keyword Extraction

The **shekar.keyword_extraction** module provides tools for automatically identifying and extracting key terms and phrases from Persian text. These algorithms help identify the most important concepts and topics within documents.

```python
from shekar.keyword_extraction import RAKE

input_text = (
    "Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ ÛŒÚ©ÛŒ Ø§Ø² Ø²Ø¨Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù… Ù…Ù†Ø·Ù‚Ù‡ Ùˆ Ø¬Ù‡Ø§Ù† Ø§Ø³Øª Ú©Ù‡ ØªØ§Ø±ÛŒØ®Ú†Ù‡â€ŒØ§ÛŒ Ú©Ù‡Ù† Ø¯Ø§Ø±Ø¯. "
    "Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø¨Ø§ Ø¯Ø§Ø´ØªÙ† Ø§Ø¯Ø¨ÛŒØ§ØªÛŒ ØºÙ†ÛŒ Ùˆ Ø´Ø§Ø¹Ø±Ø§Ù†ÛŒ Ø¨Ø±Ø¬Ø³ØªÙ‡ØŒ Ù†Ù‚Ø´ÛŒ Ø¨ÛŒâ€ŒØ¨Ø¯ÛŒÙ„ Ø¯Ø± Ú¯Ø³ØªØ±Ø´ ÙØ±Ù‡Ù†Ú¯ Ø§ÛŒØ±Ø§Ù†ÛŒ Ø§ÛŒÙØ§ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª. "
    "Ø§Ø² Ø¯ÙˆØ±Ø§Ù† ÙØ±Ø¯ÙˆØ³ÛŒ Ùˆ Ø´Ø§Ù‡Ù†Ø§Ù…Ù‡ ØªØ§ Ø¯ÙˆØ±Ø§Ù† Ù…Ø¹Ø§ØµØ±ØŒ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ù‡Ù…ÙˆØ§Ø±Ù‡ Ø§Ø¨Ø²Ø§Ø± Ø¨ÛŒØ§Ù† Ø§Ù†Ø¯ÛŒØ´Ù‡ØŒ Ø§Ø­Ø³Ø§Ø³ Ùˆ Ù‡Ù†Ø± Ø¨ÙˆØ¯Ù‡ Ø§Ø³Øª. "
)

extractor = RAKE(max_length=2, top_n=5)
keywords = extractor(input_text)

for kw in keywords:
    print(kw)
```
```output
ÙØ±Ù‡Ù†Ú¯ Ø§ÛŒØ±Ø§Ù†ÛŒ
Ú¯Ø³ØªØ±Ø´ ÙØ±Ù‡Ù†Ú¯
Ø§ÛŒØ±Ø§Ù†ÛŒ Ø§ÛŒÙØ§
Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ
ØªØ§Ø±ÛŒØ®Ú†Ù‡â€ŒØ§ÛŒ Ú©Ù‡Ù†
```
## WordCloud

[![Notebook](https://img.shields.io/badge/Notebook-Jupyter-00A693.svg)](examples/word_cloud.ipynb)  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/amirivojdan/shekar/blob/main/examples/word_cloud.ipynb)

The WordCloud class offers an easy way to create visually rich Persian word clouds. It supports reshaping and right-to-left rendering, Persian fonts, color maps, and custom shape masks for accurate and elegant visualization of word frequencies.

```python
import requests
from collections import Counter

from shekar import WordCloud
from shekar import WordTokenizer
from shekar.preprocessing import (
  HTMLTagRemover,
  PunctuationRemover,
  StopWordRemover,
  NonPersianRemover,
)
preprocessing_pipeline = HTMLTagRemover() | PunctuationRemover() | StopWordRemover() | NonPersianRemover()


url = f"https://ganjoor.net/ferdousi/shahname/siavosh/sh9"
response = requests.get(url)
html_content = response.text
clean_text = preprocessing_pipeline(html_content)

word_tokenizer = WordTokenizer()
tokens = word_tokenizer(clean_text)

word_freqs = Counter(tokens)

wordCloud = WordCloud(
        mask="Iran",
        width=1000,
        height=500,
        max_font_size=220,
        min_font_size=5,
        bg_color="white",
        contour_color="black",
        contour_width=3,
        color_map="Set2",
    )

# if shows disconnect words, try again with bidi_reshape=True
image = wordCloud.generate(word_freqs, bidi_reshape=False)
image.show()
```

![](https://raw.githubusercontent.com/amirivojdan/shekar/main/assets/wordcloud_example.png)
