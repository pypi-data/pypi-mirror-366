"""
Core data structures and schemas for the RAG Evaluation Suite.

This module defines the fundamental data models using Pydantic, which provides:
- Automatic data validation and type checking
- JSON serialization/deserialization
- Clear schema documentation
- Runtime type enforcement

These models form the backbone of the evaluation pipeline, ensuring data
consistency and providing a clear contract for how data flows through the system.
"""

from pydantic import BaseModel, Field, validator
from typing import List, Dict, Any, Optional


class TestCase(BaseModel):
    """
    Represents a single test case for evaluating a RAG pipeline.
    
    This is the "ground truth" data that we use to evaluate how well
    a RAG system performs. Each test case contains:
    1. A question that users might ask
    2. The correct context chunks that contain the answer
    3. The ideal answer we expect the system to generate
    
    Why Pydantic BaseModel?
    - Automatic validation ensures data integrity
    - Type hints are enforced at runtime
    - Easy serialization to/from JSON for data persistence
    - Self-documenting through Field descriptions
    """
    question: str = Field(
        description="The user's question that will be posed to the RAG system.",
        min_length=1  # Ensure we don't have empty questions
    )
    ground_truth_context: List[str] = Field(
        description="A list of the exact text chunks that contain the correct answer. "
                   "These represent what the retrieval system should ideally find.",
        min_items=1  # Ensure we have at least one ground truth chunk
    )
    ground_truth_answer: str = Field(
        description="The ideal, factually correct answer to the question. "
                   "This is what we expect a perfect RAG system to generate.",
        min_length=1  # Ensure we don't have empty answers
    )

    @validator('ground_truth_context')
    def validate_context_not_empty(cls, v):
        """Ensure no empty strings in the ground truth context list."""
        if any(not chunk.strip() for chunk in v):
            raise ValueError("Ground truth context chunks cannot be empty or whitespace-only")
        return v


class RAGResult(BaseModel):
    """
    Represents the actual output of a RAG pipeline for a given question.
    
    This captures what your RAG system actually produced when given a test case.
    It contains both the retrieval results (what documents were found) and
    the generation results (what answer was produced).
    
    This separation is crucial because it allows us to evaluate:
    - Retrieval quality: Did we find the right documents?
    - Generation quality: Given the documents, did we produce a good answer?
    """
    retrieved_context: List[str] = Field(
        description="The list of text chunks retrieved by the RAG system's retrieval component. "
                   "Order matters here - typically ranked by relevance score.",
        default_factory=list  # Allow empty list for cases where nothing was retrieved
    )
    final_answer: str = Field(
        description="The final answer generated by the LLM component of the RAG system.",
        default=""  # Allow empty string for cases where no answer was generated
    )


class EvaluationResult(BaseModel):
    """
    Represents the complete evaluation results for a single TestCase.
    
    This is the final output of the evaluation pipeline, containing:
    1. The original test case (ground truth)
    2. What the RAG system actually produced
    3. All the calculated metric scores comparing the two
    
    This structure makes it easy to:
    - Store evaluation results for later analysis
    - Generate reports and visualizations
    - Debug specific test cases that performed poorly
    """
    test_case: TestCase = Field(
        description="The original test case containing ground truth data"
    )
    rag_result: RAGResult = Field(
        description="The actual output produced by the RAG system"
    )
    scores: Dict[str, Any] = Field(
        description="A dictionary containing all calculated metric scores. "
                   "Keys are metric names (e.g., 'hit_rate', 'mrr'), "
                   "values are the computed scores.",
        default_factory=dict  # Start with empty dict, metrics will be added
    )