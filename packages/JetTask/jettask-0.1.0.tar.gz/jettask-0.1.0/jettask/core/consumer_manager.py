import os
import time
import uuid
import json
import logging
import threading
import asyncio
import multiprocessing
from typing import Dict, Any, List, Set
from enum import Enum
from collections import defaultdict

import redis

logger = logging.getLogger('app')


class ConsumerStrategy(Enum):
    """æ¶ˆè´¹è€…åç§°ç­–ç•¥
    
    ç­–ç•¥é€‰æ‹©æŒ‡å—ï¼š
    
    âš ï¸  POD (ä»…æ¨èå•è¿›ç¨‹ä½¿ç”¨):
       - åŸºäºK8s Podåç§°çš„å›ºå®šconsumer
       - é€‚ç”¨åœºæ™¯: å•è¿›ç¨‹åº”ç”¨ (asyncio/threadæ‰§è¡Œå™¨)
       - ä¼˜ç‚¹: è¯­ä¹‰æ¸…æ™°ï¼Œä¾¿äºç›‘æ§
       - ç¼ºç‚¹: å¤šè¿›ç¨‹ä¸‹ä¼šäº§ç”Ÿå†²çª
       
    ğŸ”§ FIXED (é«˜çº§ç”¨æˆ·):
       - å®Œå…¨è‡ªå®šä¹‰çš„consumeråç§°
       - é€‚ç”¨åœºæ™¯: æœ‰ç‰¹æ®Šå‘½åéœ€æ±‚çš„åœºæ™¯ 
       - ä¼˜ç‚¹: å®Œå…¨å¯æ§
       - ç¼ºç‚¹: éœ€è¦ç”¨æˆ·ç¡®ä¿å”¯ä¸€æ€§
    
    ğŸ”¥ HEARTBEAT (æ¨èç”¨äºç”Ÿäº§ç¯å¢ƒ):
       - åŸºäºå¿ƒè·³çš„ç®€åŒ–ç­–ç•¥
       - é€‚ç”¨åœºæ™¯: æ— çŠ¶æ€æœåŠ¡å¹³å°ï¼ˆCloud Runã€Serverlessã€K8sï¼‰
       - ä¼˜ç‚¹: é€»è¾‘ç®€å•ï¼Œç¨³å®šå¯é ï¼Œè‡ªåŠ¨æ•…éšœæ¢å¤
       - ç‰¹ç‚¹: ä½¿ç”¨éšæœºconsumer nameï¼Œé€šè¿‡æœ‰åºé›†åˆç»´æŠ¤å¿ƒè·³
    """
    FIXED = "fixed"      # å›ºå®šåç§°
    POD = "pod"          # K8s Podåç§° (âš ï¸ å¤šè¿›ç¨‹ä¸‹ä¸æ¨è)
    HEARTBEAT = "heartbeat"  # å¿ƒè·³ç­–ç•¥ (æ¨èç”¨äºç”Ÿäº§ç¯å¢ƒ)


class ConsumerManager:
    """æ¶ˆè´¹è€…åç§°ç®¡ç†å™¨"""
    
    def __init__(
        self, 
        redis_client: redis.StrictRedis,
        strategy: ConsumerStrategy = ConsumerStrategy.HEARTBEAT,
        config: Dict[str, Any] = None
    ):
        self.redis_client = redis_client
        self.strategy = strategy
        self.config = config or {}
        self._consumer_name = None
        
        # Redis prefix configuration
        self.redis_prefix = config.get('redis_prefix', 'jettask')
        
        # éªŒè¯ç­–ç•¥é…ç½®çš„åˆç†æ€§
        self._validate_strategy_configuration()
        
        # å¿ƒè·³ç­–ç•¥å®ä¾‹ - å¦‚æœæ˜¯HEARTBEATç­–ç•¥ï¼Œç«‹å³åˆå§‹åŒ–
        if self.strategy == ConsumerStrategy.HEARTBEAT:
            # ä¼ é€’é˜Ÿåˆ—ä¿¡æ¯åˆ°å¿ƒè·³ç­–ç•¥
            heartbeat_config = self.config.copy()
            heartbeat_config['queues'] = self.config.get('queues', [])
            self._heartbeat_strategy = HeartbeatConsumerStrategy(
                self.redis_client,
                heartbeat_config
            )
        else:
            self._heartbeat_strategy = None
    
    def get_prefixed_queue_name(self, queue: str) -> str:
        """ä¸ºé˜Ÿåˆ—åç§°æ·»åŠ å‰ç¼€"""
        return f"{self.redis_prefix}:{queue}"
    
    def _validate_strategy_configuration(self):
        """éªŒè¯æ¶ˆè´¹è€…ç­–ç•¥é…ç½®çš„åˆç†æ€§"""
        # æ£€æŸ¥æ˜¯å¦åœ¨å¤šè¿›ç¨‹ç¯å¢ƒä¸­
        current_process = multiprocessing.current_process()
        is_multiprocess = current_process.name != 'MainProcess'
        
        if self.strategy == ConsumerStrategy.POD and is_multiprocess:
            # PODç­–ç•¥åœ¨å¤šè¿›ç¨‹ç¯å¢ƒä¸‹æ˜¯ä¸å…è®¸çš„ï¼Œç›´æ¥é€€å‡º
            error_msg = (
                "\n"
                "âŒ é”™è¯¯: PODç­–ç•¥ä¸èƒ½åœ¨å¤šè¿›ç¨‹ç¯å¢ƒä¸­ä½¿ç”¨ï¼\n"
                "\n"
                "åŸå› : PODç­–ç•¥ä½¿ç”¨å›ºå®šçš„consumeråç§°ï¼Œå¤šè¿›ç¨‹ä¼šå¯¼è‡´æ¶ˆæ¯é‡å¤æ¶ˆè´¹ã€‚\n"
                "\n"
                "è§£å†³æ–¹æ¡ˆ:\n"
                "  1. ä½¿ç”¨ ConsumerStrategy.HEARTBEAT - å¿ƒè·³ç­–ç•¥ (æ¨è)\n"
                "  2. ä½¿ç”¨ ConsumerStrategy.FIXED - è‡ªå®šä¹‰å›ºå®šåç§°\n"
                "  3. ä½¿ç”¨å•è¿›ç¨‹æ‰§è¡Œå™¨ (asyncio/thread)\n"
                "\n"
                f"å½“å‰ç¯å¢ƒ: {current_process.name} (PID: {os.getpid()})\n"
            )
            logger.error(error_msg)
            # ç«‹å³é€€å‡ºç¨‹åº
            import sys
            sys.exit(1)
        
        # è®°å½•ç­–ç•¥é€‰æ‹©ç”¨äºè°ƒè¯•
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(f"Consumer strategy: {self.strategy.value}, Process: {current_process.name}")
        
    def get_consumer_name(self, queue: str) -> str:
        """è·å–æ¶ˆè´¹è€…åç§°"""
        if self.strategy == ConsumerStrategy.FIXED:
            return self._get_fixed_name(queue)
        elif self.strategy == ConsumerStrategy.POD:
            return self._get_pod_name(queue)
        elif self.strategy == ConsumerStrategy.HEARTBEAT:
            return self._get_heartbeat_name(queue)
        else:
            raise ValueError(f"Unknown consumer strategy: {self.strategy}")
    
    def _get_fixed_name(self, queue: str) -> str:
        """è·å–å›ºå®šçš„æ¶ˆè´¹è€…åç§°"""
        if not self._consumer_name:
            # å¯ä»¥ä»é…ç½®ã€ç¯å¢ƒå˜é‡æˆ–æ–‡ä»¶ä¸­è¯»å–
            self._consumer_name = self.config.get('consumer_name') or \
                                  os.environ.get('EASYTASK_CONSUMER_NAME') or \
                                  f"worker-{os.getpid()}"
        return f"{self._consumer_name}-{queue}"
    
    def _get_pod_name(self, queue: str) -> str:
        """è·å–åŸºäºK8s Podçš„æ¶ˆè´¹è€…åç§°
        
        æ³¨æ„ï¼šPODç­–ç•¥åªèƒ½åœ¨å•è¿›ç¨‹ç¯å¢ƒä¸‹ä½¿ç”¨
        """
        if not self._consumer_name:
            # åœ¨K8sä¸­ï¼Œé€šå¸¸é€šè¿‡ç¯å¢ƒå˜é‡è·å–Podåç§°
            pod_name = os.environ.get('HOSTNAME') or \
                       os.environ.get('POD_NAME') or \
                       os.environ.get('K8S_POD_NAME')
            
            if not pod_name:
                logger.warning("Pod name not found, falling back to hostname")
                import socket
                pod_name = socket.gethostname()
            
            # ç”±äºå·²ç»åœ¨_validate_strategy_configurationä¸­éªŒè¯è¿‡ï¼Œ
            # è¿™é‡Œåº”è¯¥åªä¼šåœ¨MainProcessä¸­æ‰§è¡Œ
            self._consumer_name = pod_name
            logger.info(f"ä½¿ç”¨Podç­–ç•¥çš„consumeråç§°: {self._consumer_name}")
                
        return f"{self._consumer_name}-{queue}"
    
    
    def _get_heartbeat_name(self, queue: str) -> str:
        """åŸºäºå¿ƒè·³ç­–ç•¥è·å–æ¶ˆè´¹è€…åç§°"""
        if not self._heartbeat_strategy:
            raise RuntimeError("Heartbeat strategy not initialized properly")
        
        return self._heartbeat_strategy.get_consumer_name(queue)
    
    def cleanup(self):
        """æ¸…ç†èµ„æºï¼ˆä¼˜é›…å…³é—­æ—¶è°ƒç”¨ï¼‰"""
        # å¤„ç†å¿ƒè·³ç­–ç•¥çš„æ¸…ç†
        if self.strategy == ConsumerStrategy.HEARTBEAT and self._heartbeat_strategy:
            self._heartbeat_strategy.cleanup()
    
    def update_stats(self, queue: str, success: bool = True, processing_time: float = 0.0):
        """æ›´æ–°æ¶ˆè´¹è€…çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆä»…å¯¹HEARTBEATç­–ç•¥æœ‰æ•ˆï¼‰"""
        if self.strategy == ConsumerStrategy.HEARTBEAT and self._heartbeat_strategy:
            self._heartbeat_strategy.update_stats(queue, success, processing_time)
    
    def task_started(self, queue: str):
        """ä»»åŠ¡å¼€å§‹æ‰§è¡Œæ—¶è°ƒç”¨ï¼ˆä»…å¯¹HEARTBEATç­–ç•¥æœ‰æ•ˆï¼‰"""
        if self.strategy == ConsumerStrategy.HEARTBEAT and self._heartbeat_strategy:
            self._heartbeat_strategy.task_started(queue)
    
    def task_finished(self, queue: str):
        """ä»»åŠ¡å®Œæˆæ—¶è°ƒç”¨ï¼ˆä»…å¯¹HEARTBEATç­–ç•¥æœ‰æ•ˆï¼‰"""
        if self.strategy == ConsumerStrategy.HEARTBEAT and self._heartbeat_strategy:
            self._heartbeat_strategy.task_finished(queue)
    
    def cleanup_expired_consumers(self, queue: str):
        """æ¸…ç†è¿‡æœŸçš„æ¶ˆè´¹è€…ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰"""
        try:
            # è·å–æ¶ˆè´¹è€…ç»„çš„pendingæ¶ˆæ¯ä¿¡æ¯
            prefixed_queue = self.get_prefixed_queue_name(queue)
            pending_info = self.redis_client.xpending(prefixed_queue, prefixed_queue)
            if not pending_info:
                return
                
            # è·å–è¯¦ç»†çš„pendingæ¶ˆæ¯
            consumers = self.redis_client.xpending_range(
                prefixed_queue, prefixed_queue, min='-', max='+', count=100
            )
            
            for consumer_info in consumers:
                consumer_name = consumer_info['consumer']
                idle_time = consumer_info['time_since_delivered']
                
                # å¦‚æœæ¶ˆæ¯ç©ºé—²æ—¶é—´è¶…è¿‡é˜ˆå€¼ï¼Œå¯èƒ½æ¶ˆè´¹è€…å·²ç»æ­»äº¡
                # ä½¿ç”¨120ç§’ä½œä¸ºé»˜è®¤çš„æ­»äº¡æ£€æµ‹é˜ˆå€¼
                if idle_time > 120 * 1000:  # 120ç§’
                    logger.warning(
                        f"Consumer {consumer_name} has pending messages "
                        f"idle for {idle_time/1000}s, may be dead"
                    )
                    # è¿™é‡Œå¯ä»¥å®ç°æ¶ˆæ¯é‡æ–°åˆ†é…é€»è¾‘
                    
        except Exception as e:
            logger.error(f"Error cleaning up expired consumers: {e}")

class HeartbeatConsumerStrategy:
    """åŸºäºå¿ƒè·³çš„ç®€åŒ–æ¶ˆè´¹è€…ç­–ç•¥
    
    ç‰¹ç‚¹ï¼š
    1. ä½¿ç”¨éšæœºconsumer name
    2. æ¯ä¸ªé˜Ÿåˆ—ç»´æŠ¤ç‹¬ç«‹çš„å¿ƒè·³æœ‰åºé›†åˆ
    3. å¿ƒè·³æ•°æ®åŒ…å«workerçš„è¯¦ç»†ä¿¡æ¯
    4. è‡ªåŠ¨é‡ç½®æ­»äº¡workerçš„pendingä»»åŠ¡
    """
    
    def __init__(self, redis_client: redis.StrictRedis, config: Dict = None):
        self.redis = redis_client
        self.config = config or {}
        
        # é…ç½®å‚æ•°
        self.heartbeat_interval = self.config.get('heartbeat_interval', 5)  # 5ç§’å¿ƒè·³
        self.heartbeat_timeout = self.config.get('heartbeat_timeout', 30)  # 30ç§’è¶…æ—¶
        self.scan_interval = self.config.get('scan_interval', 10)  # 10ç§’æ‰«æä¸€æ¬¡
        
        # ç”Ÿæˆconsumer IDï¼Œä½¿ç”¨hostnameæˆ–IPä½œä¸ºå‰ç¼€
        import socket
        try:
            # é¦–å…ˆå°è¯•è·å–hostname
            hostname = socket.gethostname()
            # å°è¯•è·å–IPåœ°å€
            ip = socket.gethostbyname(hostname)
            # ä¼˜å…ˆä½¿ç”¨hostnameï¼Œå¦‚æœhostnameæ˜¯localhoståˆ™ä½¿ç”¨IP
            prefix = hostname if hostname != 'localhost' else ip
        except:
            # å¦‚æœè·å–å¤±è´¥ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é»˜è®¤å€¼
            prefix = os.environ.get('HOSTNAME', 'unknown')
        
        self.consumer_id = f"{prefix}-{uuid.uuid4().hex[:8]}-{os.getpid()}"
        
        # æ–°çš„æ•°æ®ç»“æ„è®¾è®¡
        # è·å–Rediså‰ç¼€ï¼ˆä»é…ç½®ä¸­ï¼‰
        self.redis_prefix = config.get('redis_prefix', 'jettask').lower()
        self.worker_key = f'{self.redis_prefix}:worker:{self.consumer_id}'  # workerçš„hashé”®
        self.global_queue_workers_key = f'{self.redis_prefix}:global:queue_workers'  # å…¨å±€é˜Ÿåˆ—workerç®¡ç†
        
        self.consumer_names = {}  # queue -> consumer_name mapping
        self.active_queues = set()  # è®°å½•å½“å‰æ´»è·ƒçš„é˜Ÿåˆ—
        
        # åå°çº¿ç¨‹æ§åˆ¶
        self._heartbeat_threads = {}  # queue -> thread mapping
        self._heartbeat_stops = {}   # queue -> stop_event mapping
        self._scanner_thread = None
        self._scanner_stop = threading.Event()
        
        # ç»Ÿè®¡ç¼“å†²åŒº
        self.stats_buffer = {
            'running_tasks': defaultdict(int),
            'success_count': defaultdict(int),
            'failed_count': defaultdict(int),
            'total_time': defaultdict(float),
            'total_count': defaultdict(int)
        }
        self.stats_buffer_lock = threading.Lock()  # å› ä¸ºæœ‰åå°çº¿ç¨‹ï¼Œéœ€è¦é”
        self.stats_flush_interval = self.config.get('stats_flush_interval', 1.0)
        self.last_stats_flush = time.time()
        
        # å¯åŠ¨æ‰«æçº¿ç¨‹
        self._start_scanner_thread()
        
        # æ³¨å†Œé€€å‡ºå¤„ç†
        import atexit
        atexit.register(self.cleanup)
    
    def get_prefixed_queue_name(self, queue: str) -> str:
        """ä¸ºé˜Ÿåˆ—åç§°æ·»åŠ å‰ç¼€"""
        return f"{self.redis_prefix}:{queue}"
    
    def update_stats(self, queue: str, success: bool = True, processing_time: float = 0.0):
        """æ›´æ–°workerçš„ç»Ÿè®¡ä¿¡æ¯ - ç¼“å†²åˆ°å†…å­˜ä¸­
        
        Args:
            queue: é˜Ÿåˆ—åç§°
            success: æ˜¯å¦æ‰§è¡ŒæˆåŠŸ
            processing_time: å¤„ç†æ—¶é—´ï¼ˆç§’ï¼‰
        """
        with self.stats_buffer_lock:
            if success:
                self.stats_buffer['success_count'][queue] += 1
            else:
                self.stats_buffer['failed_count'][queue] += 1
            
            self.stats_buffer['total_count'][queue] += 1
            self.stats_buffer['total_time'][queue] += processing_time
    
    def task_started(self, queue: str):
        """ä»»åŠ¡å¼€å§‹æ‰§è¡Œæ—¶è°ƒç”¨ - ç¼“å†²åˆ°å†…å­˜ä¸­"""
        with self.stats_buffer_lock:
            self.stats_buffer['running_tasks'][queue] += 1
    
    def task_finished(self, queue: str):
        """ä»»åŠ¡å®Œæˆæ—¶è°ƒç”¨ - ç¼“å†²åˆ°å†…å­˜ä¸­"""
        with self.stats_buffer_lock:
            if self.stats_buffer['running_tasks'][queue] > 0:
                self.stats_buffer['running_tasks'][queue] -= 1
    
    def flush_stats_buffer(self):
        """åˆ·æ–°ç»Ÿè®¡ç¼“å†²åˆ° Redis"""
        with self.stats_buffer_lock:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®éœ€è¦åˆ·æ–°
            has_data = False
            for buffer in self.stats_buffer.values():
                if buffer:
                    has_data = True
                    break
            
            if not has_data:
                return
            
            try:
                pipeline = self.redis.pipeline()
                
                # æ›´æ–°è¿è¡Œä¸­ä»»åŠ¡æ•°
                for queue, delta in self.stats_buffer['running_tasks'].items():
                    if delta != 0:
                        pipeline.hincrby(self.worker_key, f'{queue}:running_tasks', delta)
                
                # æ›´æ–°ç»Ÿè®¡æ•°æ®
                processed_queues = set()
                for queue in set().union(
                    self.stats_buffer['success_count'].keys(),
                    self.stats_buffer['failed_count'].keys(),
                    self.stats_buffer['total_count'].keys()
                ):
                    processed_queues.add(queue)
                    
                    if queue in self.stats_buffer['success_count']:
                        pipeline.hincrby(self.worker_key, f'{queue}:success_count', 
                                       self.stats_buffer['success_count'][queue])
                    
                    if queue in self.stats_buffer['failed_count']:
                        pipeline.hincrby(self.worker_key, f'{queue}:failed_count', 
                                       self.stats_buffer['failed_count'][queue])
                    
                    if queue in self.stats_buffer['total_count']:
                        pipeline.hincrby(self.worker_key, f'{queue}:total_count', 
                                       self.stats_buffer['total_count'][queue])
                        pipeline.hincrbyfloat(self.worker_key, f'{queue}:total_processing_time', 
                                            self.stats_buffer['total_time'][queue])
                
                # æ‰§è¡Œæ‰¹é‡æ›´æ–°
                pipeline.execute()
                
                # æ›´æ–°å¹³å‡å¤„ç†æ—¶é—´
                for queue in processed_queues:
                    if queue in self.stats_buffer['total_count'] and self.stats_buffer['total_count'][queue] > 0:
                        # è·å–å½“å‰æ€»æ•°
                        current_total = self.redis.hget(self.worker_key, f'{queue}:total_count')
                        current_time = self.redis.hget(self.worker_key, f'{queue}:total_processing_time')
                        
                        if current_total and current_time:
                            total_count = int(current_total)
                            total_time = float(current_time)
                            avg_time = total_time / total_count
                            self.redis.hset(self.worker_key, f'{queue}:avg_processing_time', f'{avg_time:.3f}')
                
                # æ¸…ç©ºç¼“å†²åŒº
                for buffer in self.stats_buffer.values():
                    buffer.clear()
                    
            except Exception as e:
                logger.error(f"Failed to flush stats buffer: {e}")
    
    def get_stats(self, queue: str) -> dict:
        """è·å–é˜Ÿåˆ—çš„ç»Ÿè®¡ä¿¡æ¯ - ä»Redis Hashè¯»å–"""
        try:
            # æ‰¹é‡è·å–è¯¥é˜Ÿåˆ—çš„æ‰€æœ‰ç»Ÿè®¡å­—æ®µ
            fields = [
                f'{queue}:success_count',
                f'{queue}:failed_count', 
                f'{queue}:total_count',
                f'{queue}:running_tasks',
                f'{queue}:avg_processing_time'
            ]
            
            values = self.redis.hmget(self.worker_key, fields)
            
            return {
                'success_count': int(values[0] or 0),
                'failed_count': int(values[1] or 0),
                'total_count': int(values[2] or 0),
                'running_tasks': int(values[3] or 0),
                'avg_processing_time': float(values[4] or 0.0)
            }
        except Exception as e:
            logger.error(f"Failed to get stats for queue {queue}: {e}")
            return {
                'success_count': 0,
                'failed_count': 0,
                'total_count': 0,
                'running_tasks': 0,
                'avg_processing_time': 0.0
            }
    
    def get_consumer_name(self, queue: str) -> str:
        """è·å–æ¶ˆè´¹è€…åç§°"""
        if queue not in self.consumer_names:
            # ä¸ºæ¯ä¸ªé˜Ÿåˆ—ç”Ÿæˆå”¯ä¸€çš„consumer name
            self.consumer_names[queue] = f"{self.consumer_id}-{queue}"
            self.active_queues.add(queue)
            
            # ä¸ºè¿™ä¸ªé˜Ÿåˆ—å¯åŠ¨å¿ƒè·³çº¿ç¨‹
            if queue not in self._heartbeat_threads:
                self._start_heartbeat_thread_for_queue(queue)
            
            logger.info(f"Created consumer name for queue {queue}: {self.consumer_names[queue]}")
        return self.consumer_names[queue]
    
    def _start_heartbeat_thread_for_queue(self, queue: str):
        """ä¸ºç‰¹å®šé˜Ÿåˆ—å¯åŠ¨å¿ƒè·³çº¿ç¨‹"""
        if queue in self._heartbeat_threads:
            return
        
        stop_event = threading.Event()
        self._heartbeat_stops[queue] = stop_event
        
        thread = threading.Thread(
            target=self._heartbeat_loop_for_queue,
            args=(queue, stop_event),
            daemon=True,
            name=f"heartbeat-{queue}"
        )
        thread.start()
        self._heartbeat_threads[queue] = thread
        
        logger.info(f"Started heartbeat thread for queue {queue}")
    
    def _start_scanner_thread(self):
        """å¯åŠ¨æ‰«æçº¿ç¨‹"""
        self._scanner_thread = threading.Thread(
            target=self._scanner_loop,
            daemon=True,
            name="heartbeat-scanner"
        )
        self._scanner_thread.start()
        
        # ç«‹å³æ‰§è¡Œä¸€æ¬¡æ‰«æï¼Œæ¸…ç†å¯èƒ½å­˜åœ¨çš„æ­»äº¡worker
        threading.Thread(
            target=self._immediate_scan,
            daemon=True,
            name="immediate-scanner"
        ).start()
        
        logger.info(f"Started heartbeat scanner for consumer {self.consumer_id}")
    
    def _immediate_scan(self):
        """å¯åŠ¨æ—¶ç«‹å³æ‰§è¡Œä¸€æ¬¡æ‰«æ"""
        try:
            logger.info("Performing immediate scan for dead workers...")
            self._perform_scan()
            logger.info("Immediate scan completed")
        except Exception as e:
            logger.error(f"Error in immediate scan: {e}")
    
    def _heartbeat_loop_for_queue(self, queue: str, stop_event: threading.Event):
        """ç‰¹å®šé˜Ÿåˆ—çš„å¿ƒè·³å¾ªç¯ - ç®€åŒ–ç‰ˆæœ¬ï¼Œåªç»´æŠ¤worker hash"""
        consumer_name = self.consumer_names[queue]
        
        # è·å–ä¸»æœºåæˆ–IPï¼ˆä¸€æ¬¡æ€§è·å–ï¼‰
        import socket
        try:
            hostname = socket.gethostname()
            if not hostname or hostname == 'localhost':
                hostname = socket.gethostbyname(socket.gethostname())
        except:
            hostname = os.environ.get('HOSTNAME', 'unknown')
        
        heartbeat_update_count = 0
        
        while not stop_event.is_set():
            try:
                current_time = time.time()
                
                # ä½¿ç”¨pipelineæ‰¹é‡æ›´æ–°workerä¿¡æ¯
                pipeline = self.redis.pipeline()
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦è®¾ç½®created_at
                if not self.redis.hexists(self.worker_key, 'created_at'):
                    pipeline.hset(self.worker_key, 'created_at', str(current_time))
                
                # æ›´æ–°workerçš„åŸºæœ¬ä¿¡æ¯
                pipeline.hset(self.worker_key, mapping={
                    'consumer_id': self.consumer_id,
                    'host': hostname,
                    'pid': str(os.getpid()),
                    'last_heartbeat': str(current_time),
                    'is_alive': 'true'
                })
                
                # æ›´æ–°é˜Ÿåˆ—åˆ—è¡¨
                current_queues = self.redis.hget(self.worker_key, 'queues') or ''
                queue_list = set(current_queues.split(',')) if current_queues else set()
                queue_list.add(queue)
                pipeline.hset(self.worker_key, 'queues', ','.join(sorted(queue_list)))
                
                # åœ¨å…¨å±€hashä¸­ç»´æŠ¤é˜Ÿåˆ—çš„workeråˆ—è¡¨
                current_workers = self.redis.hget(self.global_queue_workers_key, queue) or ''
                worker_set = set(current_workers.split(',')) if current_workers else set()
                worker_set.add(self.consumer_id)
                pipeline.hset(self.global_queue_workers_key, queue, ','.join(sorted(worker_set)))
                
                # æ‰§è¡Œæ‰¹é‡æ›´æ–°
                pipeline.execute()
                
                heartbeat_update_count += 1
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°ç»Ÿè®¡ç¼“å†²
                if current_time - self.last_stats_flush >= self.stats_flush_interval:
                    self.flush_stats_buffer()
                    self.last_stats_flush = current_time
                
                # å®šæœŸæ—¥å¿—ï¼ˆæ¯100æ¬¡æ›´æ–°è®°å½•ä¸€æ¬¡ï¼‰
                if heartbeat_update_count % 100 == 0:
                    logger.debug(f"Heartbeat updated {heartbeat_update_count} times for {consumer_name}")
                
                stop_event.wait(self.heartbeat_interval)
                
            except Exception as e:
                logger.error(f"Error in heartbeat loop for queue {queue}: {e}")
                stop_event.wait(1)
    
    def _perform_scan(self):
        """æ‰§è¡Œä¸€æ¬¡æ‰«ææ“ä½œ - æ–°çš„Hashç»“æ„ç‰ˆæœ¬"""
        current_time = time.time()
        timeout_threshold = current_time - self.heartbeat_timeout
        
        try:
            # æ‰«ææ‰€æœ‰worker hashé”®
            pattern = f"{self.redis_prefix}:worker:*"
            worker_keys = []
            cursor = 0
            
            # ä½¿ç”¨SCANè¿­ä»£è·å–æ‰€æœ‰workeré”®
            while True:
                cursor, keys = self.redis.scan(cursor, match=pattern, count=100)
                worker_keys.extend(keys)
                if cursor == 0:
                    break
            
            if not worker_keys:
                logger.debug("No worker keys found")
                return
            
            timeout_workers = []
            
            # æ£€æŸ¥æ¯ä¸ªworkerçš„å¿ƒè·³æ—¶é—´
            for worker_key in worker_keys:
                try:
                    # è·³è¿‡historyç›¸å…³çš„é”®
                    if ':history:' in worker_key:
                        continue
                    
                    # å…ˆæ£€æŸ¥keyçš„ç±»å‹
                    key_type = self.redis.type(worker_key)
                    if key_type != 'hash':
                        logger.warning(f"Worker key {worker_key} is not a hash, type: {key_type}, skipping")
                        continue
                    
                    worker_data = self.redis.hgetall(worker_key)
                    if not worker_data:
                        continue
                        
                    last_heartbeat = float(worker_data.get('last_heartbeat', 0))
                    consumer_id = worker_data.get('consumer_id')
                    is_alive = worker_data.get('is_alive', 'true').lower() == 'true'
                    
                    # è·³è¿‡è‡ªå·±
                    if consumer_id == self.consumer_id:
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦è¶…æ—¶ä¸”å½“å‰æ ‡è®°ä¸ºåœ¨çº¿
                    if is_alive and last_heartbeat < timeout_threshold:
                        timeout_workers.append((worker_key, worker_data))
                        
                except (ValueError, TypeError) as e:
                    logger.error(f"Error parsing worker data from {worker_key}: {e}")
                    continue
            
            if timeout_workers:
                logger.info(f"Found {len(timeout_workers)} timeout workers")
                
                for worker_key, worker_data in timeout_workers:
                    consumer_id = worker_data.get('consumer_id')
                    queues = worker_data.get('queues', '').split(',') if worker_data.get('queues') else []
                    
                    # ä½¿ç”¨åˆ†å¸ƒå¼é”æ¥é¿å…å¤šä¸ªscanneråŒæ—¶å¤„ç†åŒä¸€ä¸ªworker
                    lock_key = f"{self.redis_prefix}:scanner:lock:{consumer_id}"
                    lock_ttl = self.scan_interval * 2
                    
                    if not self.redis.set(lock_key, self.consumer_id, nx=True, ex=lock_ttl):
                        logger.debug(f"Another scanner is processing worker {consumer_id}, skipping")
                        continue
                    
                    try:
                        # å†æ¬¡æ£€æŸ¥workeræ˜¯å¦çœŸçš„è¶…æ—¶ï¼ˆé¿å…ç«æ€æ¡ä»¶ï¼‰
                        current_heartbeat = self.redis.hget(worker_key, 'last_heartbeat')
                        if current_heartbeat and float(current_heartbeat) >= timeout_threshold:
                            logger.info(f"Worker {consumer_id} is now alive, skipping")
                            continue
                        
                        logger.info(f"Processing timeout worker: {consumer_id}")
                        # æ ‡è®°workerä¸ºç¦»çº¿å¹¶æ¸…ç†æ¶ˆè´¹è€…
                        self._mark_worker_offline_and_cleanup_v2(worker_key, worker_data, queues)
                        
                    except Exception as e:
                        logger.error(f"Error processing timeout worker {consumer_id}: {e}")
                    finally:
                        # é‡Šæ”¾é”
                        self.redis.delete(lock_key)
                        
        except Exception as e:
            logger.error(f"Error in scanner: {e}")
    
    def _mark_worker_offline_and_cleanup_v2(self, worker_key: str, worker_data: dict, queues: list):
        """æ ‡è®°workerä¸ºç¦»çº¿çŠ¶æ€å¹¶æ¸…ç†æ¶ˆè´¹è€… - ç®€åŒ–ç‰ˆæœ¬"""
        consumer_id = worker_data.get('consumer_id')
        
        try:
            current_time = time.time()
            
            # ä¿å­˜workerä¸‹çº¿å†å²è®°å½•
            self._save_worker_offline_history(consumer_id, worker_data, current_time)
            
            # æ ‡è®°workerä¸ºç¦»çº¿çŠ¶æ€
            pipeline = self.redis.pipeline()
            pipeline.hset(worker_key, mapping={
                'is_alive': 'false',
                'offline_time': str(current_time)
            })
            
            # å°†æ‰§è¡Œä¸­ä»»åŠ¡å½’é›¶ï¼Œå¹¶ä»å…¨å±€é˜Ÿåˆ—workeråˆ—è¡¨ä¸­ç§»é™¤
            for queue in queues:
                if queue.strip():  # ç¡®ä¿é˜Ÿåˆ—åä¸ä¸ºç©º
                    pipeline.hset(worker_key, f'{queue}:running_tasks', '0')
                    
                    # ä»å…¨å±€é˜Ÿåˆ—workeråˆ—è¡¨ä¸­ç§»é™¤å½“å‰worker
                    current_workers = self.redis.hget(self.global_queue_workers_key, queue.strip()) or ''
                    if current_workers:
                        worker_set = set(current_workers.split(','))
                        worker_set.discard(consumer_id)
                        if worker_set:
                            pipeline.hset(self.global_queue_workers_key, queue.strip(), ','.join(sorted(worker_set)))
                        else:
                            pipeline.hdel(self.global_queue_workers_key, queue.strip())
            
            # æ‰§è¡Œæ‰¹é‡æ›´æ–°
            pipeline.execute()
            
            logger.info(f"Marked worker {consumer_id} as offline")
            
            # æ¸…ç†Redis Streamæ¶ˆè´¹è€…ç»„ä¸­çš„consumer
            for queue in queues:
                if queue.strip():
                    # è·å–consumer nameï¼ˆéœ€è¦é‡æ„è¿™éƒ¨åˆ†é€»è¾‘ï¼‰
                    consumer_name = f"{consumer_id}-{queue}"
                    self._cleanup_stream_consumer(queue, consumer_name)
                    self._reset_consumer_pending_messages(queue, consumer_name)
                    
        except Exception as e:
            logger.error(f"Error marking worker {consumer_id} offline: {e}")
    
    def _scanner_loop(self):
        """æ‰«æè¶…æ—¶workerçš„å¾ªç¯"""
        while not self._scanner_stop.is_set():
            try:
                self._perform_scan()
                self._scanner_stop.wait(self.scan_interval)
            except Exception as e:
                logger.error(f"Error in scanner loop: {e}")
                self._scanner_stop.wait(5)  # é”™è¯¯æ—¶ç­‰å¾…5ç§’åé‡è¯•
    
    
    def _cleanup_stream_consumer(self, queue: str, consumer_name: str):
        """ä»Redis Streamæ¶ˆè´¹è€…ç»„ä¸­åˆ é™¤consumer"""
        try:
            # åˆ é™¤æ¶ˆè´¹è€…ï¼ˆè¿™ä¼šé˜»æ­¢å®ƒé‡æ–°åŠ å…¥åç»§ç»­æ¶ˆè´¹æ¶ˆæ¯ï¼‰
            prefixed_queue = self.get_prefixed_queue_name(queue)
            result = self.redis.execute_command('XGROUP', 'DELCONSUMER', prefixed_queue, prefixed_queue, consumer_name)
            if result > 0:
                logger.info(f"Deleted stream consumer {consumer_name} from group {queue}")
            else:
                logger.debug(f"Stream consumer {consumer_name} was not found in group {queue}")
        except Exception as e:
            logger.error(f"Error deleting stream consumer {consumer_name}: {e}")

    def _handle_dead_worker(self, queue: str, worker_info: dict, worker_data: bytes):
        """å¤„ç†æ­»äº¡çš„worker"""
        consumer_name = worker_info.get('consumer_name', 'unknown')
        
        # ä½¿ç”¨åˆ†å¸ƒå¼é”æ¥é¿å…å¤šä¸ªscanneråŒæ—¶å¤„ç†åŒä¸€ä¸ªconsumer
        consumer_lock_key = f"{self.redis_prefix}:consumer:lock:{consumer_name}"
        consumer_lock_ttl = 30  # 30ç§’é”è¶…æ—¶
        
        # å°è¯•è·å–consumerçº§åˆ«çš„é”
        if not self.redis.set(consumer_lock_key, self.consumer_id, nx=True, ex=consumer_lock_ttl):
            logger.debug(f"Another scanner is handling consumer {consumer_name}, skipping")
            return
        
        try:
            heartbeat_key = f"{self.heartbeat_key_prefix}{queue}"
            
            # å†æ¬¡æ£€æŸ¥workeræ˜¯å¦çœŸçš„è¶…æ—¶ï¼ˆé¿å…ç«æ€æ¡ä»¶ï¼‰
            current_score = self.redis.zscore(heartbeat_key, worker_data)
            if current_score and time.time() - current_score < self.heartbeat_timeout:
                logger.info(f"Worker {consumer_name} is now alive, skipping")
                return
            
            # ä»æœ‰åºé›†åˆä¸­åˆ é™¤æ­»äº¡çš„workerï¼ˆä½¿ç”¨åŸå§‹çš„worker_dataï¼‰
            removed = self.redis.zrem(heartbeat_key, worker_data)
            if removed:
                logger.info(f"Removed dead worker {consumer_name} from heartbeat set for queue {queue}")
                
                # é‡ç½®è¯¥consumerçš„pendingæ¶ˆæ¯
                self._reset_consumer_pending_messages(queue, consumer_name)
            else:
                logger.debug(f"Worker {consumer_name} already removed by another scanner")
            
        except Exception as e:
            logger.error(f"Error handling dead worker {consumer_name}: {e}")
        finally:
            # é‡Šæ”¾consumeré”
            self.redis.delete(consumer_lock_key)
    
    def _reset_consumer_pending_messages(self, queue: str, consumer_name: str):
        """é‡ç½®æŒ‡å®šconsumerçš„pendingæ¶ˆæ¯"""
        try:
            # é¦–å…ˆè·å–è¯¥consumerçš„æ‰€æœ‰pendingæ¶ˆæ¯
            consumer_messages = []
            try:
                # åˆ†æ‰¹è·å–è¯¥consumerçš„æ‰€æœ‰pendingæ¶ˆæ¯
                batch_size = 1000
                last_id = '-'
                
                while True:
                    # è·å–ä¸€æ‰¹pendingæ¶ˆæ¯
                    prefixed_queue = self.get_prefixed_queue_name(queue)
                    pending_batch = self.redis.xpending_range(
                        prefixed_queue, prefixed_queue,
                        min=last_id, max='+',
                        count=batch_size
                    )
                    
                    if not pending_batch:
                        break
                    
                    # è¿‡æ»¤å‡ºå±äºè¯¥consumerçš„æ¶ˆæ¯
                    for msg in pending_batch:
                        msg_consumer = msg['consumer']
                        # å¤„ç†bytesç±»å‹
                        if isinstance(msg_consumer, bytes):
                            msg_consumer = msg_consumer.decode('utf-8')
                        if msg_consumer == consumer_name:
                            consumer_messages.append(msg)
                    
                    # å¦‚æœè·å–çš„æ¶ˆæ¯æ•°å°äºbatch_sizeï¼Œè¯´æ˜å·²ç»è·å–å®Œæ‰€æœ‰æ¶ˆæ¯
                    if len(pending_batch) < batch_size:
                        break
                    
                    # æ›´æ–°last_idä¸ºæœ€åä¸€æ¡æ¶ˆæ¯çš„IDï¼Œç”¨äºä¸‹ä¸€æ‰¹æŸ¥è¯¢
                    last_id = pending_batch[-1]['message_id']
                
                if not consumer_messages:
                    logger.debug(f"No pending messages for consumer {consumer_name}")
                    # ä»ç„¶å°è¯•åˆ é™¤consumer
                    try:
                        prefixed_queue = self.get_prefixed_queue_name(queue)
                        self.redis.execute_command('XGROUP', 'DELCONSUMER', prefixed_queue, prefixed_queue, consumer_name)
                    except:
                        pass
                    return
                
                logger.info(f"Found {len(consumer_messages)} pending messages for dead consumer {consumer_name}")
                
                # è·å–æ¶ˆæ¯IDåˆ—è¡¨
                message_ids = [msg['message_id'] for msg in consumer_messages]
                
                # ä½¿ç”¨ä¸€ä¸ªç‰¹æ®Šçš„consumeræ¥claimè¿™äº›æ¶ˆæ¯ï¼Œç„¶åç«‹å³ACKå¹¶é‡æ–°æ·»åŠ 
                temp_consumer = f"recovery-{uuid.uuid4().hex[:8]}"
                
                # åˆ†æ‰¹å¤„ç†æ¶ˆæ¯
                recovered_count = 0
                for i in range(0, len(message_ids), 100):
                    batch = message_ids[i:i+100]
                    try:
                        # 1. Claimæ¶ˆæ¯åˆ°ä¸´æ—¶consumer
                        claimed = self.redis.xclaim(
                            queue, queue,
                            temp_consumer,
                            min_idle_time=0,
                            message_ids=batch,
                            force=True
                        )
                        
                        if claimed:
                            # 2. å°†æ¶ˆæ¯å†…å®¹é‡æ–°æ·»åŠ åˆ°stream
                            for msg_id, msg_data in claimed:
                                try:
                                    # é‡æ–°æ·»åŠ æ¶ˆæ¯åˆ°streamæœ«å°¾
                                    self.redis.xadd(queue, msg_data)
                                    recovered_count += 1
                                except Exception as e:
                                    logger.error(f"Failed to re-add message {msg_id}: {e}")
                            
                            # 3. ACKåŸå§‹æ¶ˆæ¯
                            self.redis.xack(queue, queue, *[msg[0] for msg in claimed])
                            
                    except Exception as e:
                        logger.error(f"Error recovering batch of messages: {e}")
                
                logger.info(f"Successfully recovered {recovered_count} pending messages from {consumer_name}")
                
            except Exception as e:
                logger.error(f"Error getting pending messages: {e}")
            
            # æœ€ååˆ é™¤æ­»äº¡çš„consumer
            try:
                prefixed_queue = self.get_prefixed_queue_name(queue)
                self.redis.execute_command('XGROUP', 'DELCONSUMER', prefixed_queue, prefixed_queue, consumer_name)
                logger.debug(f"Deleted consumer {consumer_name}")
            except:
                pass
                        
        except Exception as e:
            logger.error(f"Error resetting pending messages for {consumer_name}: {e}")
    
    def _save_worker_offline_history(self, consumer_id: str, worker_data: dict, offline_time: float):
        """ä¿å­˜workerä¸‹çº¿å†å²è®°å½•"""
        try:
            # å†å²è®°å½•key
            history_key = f"{self.redis_prefix}:worker:history:{consumer_id}"
            
            # è·å–workerçš„è¿è¡Œç»Ÿè®¡
            online_time = float(worker_data.get('created_at', offline_time))
            duration = offline_time - online_time
            
            # è·å–æ‰€æœ‰é˜Ÿåˆ—çš„ç»Ÿè®¡ä¿¡æ¯
            queues = worker_data.get('queues', '').split(',') if worker_data.get('queues') else []
            total_success = 0
            total_failed = 0
            total_tasks = 0
            total_running = 0
            total_processing_time = 0.0
            
            # èšåˆæ‰€æœ‰é˜Ÿåˆ—çš„ç»Ÿè®¡
            for queue in queues:
                if queue.strip():
                    queue = queue.strip()
                    success_count = int(worker_data.get(f'{queue}:success_count', 0))
                    failed_count = int(worker_data.get(f'{queue}:failed_count', 0))
                    task_count = int(worker_data.get(f'{queue}:total_count', 0))
                    running_tasks = int(worker_data.get(f'{queue}:running_tasks', 0))
                    processing_time = float(worker_data.get(f'{queue}:total_processing_time', 0))
                    
                    total_success += success_count
                    total_failed += failed_count
                    total_tasks += task_count
                    total_running += running_tasks
                    total_processing_time += processing_time
            
            # è®¡ç®—å¹³å‡å¤„ç†æ—¶é—´
            avg_processing_time = total_processing_time / total_tasks if total_tasks > 0 else 0.0
            
            # æ„å»ºå†å²è®°å½•
            history_data = {
                'consumer_id': consumer_id,
                'host': worker_data.get('host', 'unknown'),
                'pid': worker_data.get('pid', '0'),
                'queues': worker_data.get('queues', ''),
                'online_time': str(online_time),
                'offline_time': str(offline_time),
                'duration_seconds': str(int(duration)),
                'last_heartbeat': worker_data.get('last_heartbeat', '0'),
                'shutdown_reason': 'heartbeat_timeout',
                'final_status': 'offline',
                # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
                'total_success_count': str(total_success),
                'total_failed_count': str(total_failed),
                'total_count': str(total_tasks),
                'total_running_tasks': str(total_running),
                'total_processing_time': str(total_processing_time),
                'avg_processing_time': str(avg_processing_time)
            }
            
            # ä¿å­˜å†å²è®°å½•ï¼ˆä½¿ç”¨hashå­˜å‚¨ï¼‰
            self.redis.hset(history_key, mapping=history_data)
            
            # è®¾ç½®è¿‡æœŸæ—¶é—´ï¼ˆä¿ç•™7å¤©çš„å†å²è®°å½•ï¼‰
            self.redis.expire(history_key, 7 * 24 * 3600)
            
            # å°†å†å²è®°å½•IDæ·»åŠ åˆ°æœ‰åºé›†åˆä¸­ï¼ˆæŒ‰ä¸‹çº¿æ—¶é—´æ’åºï¼‰
            history_index_key = f"{self.redis_prefix}:worker:history:index"
            self.redis.zadd(history_index_key, {consumer_id: offline_time})
            
            # æ¸…ç†è¿‡æœŸçš„å†å²è®°å½•ç´¢å¼•ï¼ˆä¿ç•™æœ€è¿‘7å¤©çš„ï¼‰
            expire_threshold = offline_time - (7 * 24 * 3600)
            self.redis.zremrangebyscore(history_index_key, '-inf', expire_threshold)
            
            logger.info(f"Saved offline history for worker {consumer_id}, duration: {int(duration)}s")
            
        except Exception as e:
            logger.error(f"Error saving worker offline history: {e}")
    
    def get_worker_offline_history(self, limit: int = 100, start_time: float = None, end_time: float = None):
        """è·å–workerä¸‹çº¿å†å²è®°å½•"""
        try:
            history_index_key = f"{self.redis_prefix}:worker:history:index"
            
            # è®¾ç½®æ—¶é—´èŒƒå›´
            if start_time is None:
                start_time = '-inf'
            if end_time is None:
                end_time = '+inf'
                
            # ä»ç´¢å¼•ä¸­è·å–consumer_idåˆ—è¡¨
            consumer_ids = self.redis.zrevrangebyscore(
                history_index_key, 
                end_time, 
                start_time,
                start=0,
                num=limit
            )
            
            # è·å–è¯¦ç»†å†å²è®°å½•
            history_records = []
            for consumer_id in consumer_ids:
                history_key = f"{self.redis_prefix}:worker:history:{consumer_id}"
                record = self.redis.hgetall(history_key)
                if record:
                    # è½¬æ¢æ•°å€¼ç±»å‹
                    record['online_time'] = float(record.get('online_time', 0))
                    record['offline_time'] = float(record.get('offline_time', 0))
                    record['duration_seconds'] = int(record.get('duration_seconds', 0))
                    
                    # è½¬æ¢ç»Ÿè®¡ä¿¡æ¯
                    record['total_success_count'] = int(record.get('total_success_count', 0))
                    record['total_failed_count'] = int(record.get('total_failed_count', 0))
                    record['total_count'] = int(record.get('total_count', 0))
                    record['total_running_tasks'] = int(record.get('total_running_tasks', 0))
                    record['total_processing_time'] = float(record.get('total_processing_time', 0))
                    record['avg_processing_time'] = float(record.get('avg_processing_time', 0))
                    
                    history_records.append(record)
            
            return history_records
            
        except Exception as e:
            logger.error(f"Error getting worker offline history: {e}")
            return []
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info(f"Cleaning up heartbeat consumer {self.consumer_id}")
        
        # æœ€ååˆ·æ–°ä¸€æ¬¡ç»Ÿè®¡ç¼“å†²
        try:
            self.flush_stats_buffer()
        except Exception as e:
            logger.error(f"Failed to flush stats buffer during cleanup: {e}")
        
        # åœæ­¢æ‰€æœ‰å¿ƒè·³çº¿ç¨‹
        for queue, stop_event in self._heartbeat_stops.items():
            stop_event.set()
        
        # åœæ­¢æ‰«æçº¿ç¨‹
        self._scanner_stop.set()
        
        # ç­‰å¾…æ‰€æœ‰å¿ƒè·³çº¿ç¨‹ç»“æŸ
        for queue, thread in self._heartbeat_threads.items():
            if thread and thread.is_alive():
                thread.join(timeout=2)
        
        # ç­‰å¾…æ‰«æçº¿ç¨‹ç»“æŸ
        if self._scanner_thread and self._scanner_thread.is_alive():
            self._scanner_thread.join(timeout=2)
        
        # é‡è¦ï¼šä¸åˆ é™¤å¿ƒè·³è®°å½•ï¼
        # å¿ƒè·³è®°å½•å¿…é¡»ä¿ç•™ï¼Œè®©scannerèƒ½å¤Ÿæ£€æµ‹åˆ°workerç¦»çº¿å¹¶æ¢å¤pendingæ¶ˆæ¯
        # å¿ƒè·³ä¼šå› ä¸ºè¶…æ—¶è‡ªåŠ¨è¢«scanneræ¸…ç†
        logger.info(f"Heartbeat consumer {self.consumer_id} stopped (heartbeat will timeout naturally)")