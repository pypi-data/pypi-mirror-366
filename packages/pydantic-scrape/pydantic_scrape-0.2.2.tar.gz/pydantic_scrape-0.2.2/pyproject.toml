[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "pydantic-scrape"
version = "0.2.2"
description = "Advanced web automation framework with AI-powered agents, Chawan terminal browser integration, and geographic search targeting"
readme = "README.md"
license = {text = "MIT"}
authors = [
    {name = "Pydantic Scrape Contributors"}
]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Internet :: WWW/HTTP :: Browsers",
    "Topic :: Software Development :: Libraries :: Python Modules",
    "Topic :: Text Processing :: Markup :: HTML",
]
keywords = ["scraping", "web-scraping", "pydantic", "chawan", "automation", "browser-automation", "ai-agents", "geographic-search"]
requires-python = ">=3.10"

dependencies = [
    # Core framework dependencies
    "pydantic-ai>=0.2.11",
    "pydantic-graph>=0.2.11",
    "loguru>=0.7.3",
    "python-dotenv",
    "httpx>=0.24.0",
    "platformdirs>=3.0.0",
    # Core scraping dependencies (lightweight)
    "beautifulsoup4>=4.13.0",
    "lxml>=4.9.0",
    "lxml_html_clean>=0.1.0",
    "rapidfuzz",
]

[project.optional-dependencies]
# YouTube video downloading and processing
youtube = [
    "yt-dlp>=2023.12.30",
]

# AI services (OpenAI, Google AI)
ai = [
    "openai>=1.0.0",
    "google-generativeai>=0.3.0",
]

# Academic research (papers, CrossRef)
academic = [
    "pyalex>=0.17",
    "habanero>=1.2.6",
]

# Document processing (PDFs, Word, eBooks)
documents = [
    "PyMuPDF>=1.25.0",
    "python-docx>=1.1.0",
    "EbookLib>=0.19",
]

# Content extraction (advanced article extraction)
content = [
    "goose3>=3.1.19",
    "newspaper3k>=0.2.8",
]

# Legacy Camoufox browser (now replaced by Chawan)
camoufox = [
    "camoufox>=0.4.11",
]

# All optional extras combined
all = [
    "yt-dlp>=2023.12.30",
    "openai>=1.0.0", 
    "google-generativeai>=0.3.0",
    "pyalex>=0.17",
    "habanero>=1.2.6",
    "PyMuPDF>=1.25.0",
    "python-docx>=1.1.0",
    "EbookLib>=0.19",
    "goose3>=3.1.19",
    "newspaper3k>=0.2.8",
    "camoufox>=0.4.11",
]

# Development dependencies
dev = [
    "pytest>=7.4.0",
    "pytest-asyncio>=0.21.0",
    "black>=23.0.0",
    "ruff>=0.1.0",
    "build>=1.0.0",
    "twine>=4.0.0",
]

[project.urls]
Homepage = "https://github.com/philmade/pydantic_scrape"
Repository = "https://github.com/philmade/pydantic_scrape"
Issues = "https://github.com/philmade/pydantic_scrape/issues"

[tool.setuptools.packages.find]
include = ["pydantic_scrape*"]

[tool.black]
line-length = 88
target-version = ['py310']

[tool.ruff]
target-version = "py310"
line-length = 88
select = [
    "E",  # pycodestyle errors
    "W",  # pycodestyle warnings
    "F",  # pyflakes
    "I",  # isort
    "B",  # flake8-bugbear
    "C4", # flake8-comprehensions
]
ignore = [
    "E501",  # line too long, handled by black
    "B008",  # do not perform function calls in argument defaults
]

[tool.pytest.ini_options]
asyncio_mode = "auto"
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
