# This file was auto-generated by Fern from our API Definition.

import datetime as dt
import typing

from ..core.datetime_utils import serialize_datetime
from .create_model_request_training_config_openpipe_hyperparameters import (
    CreateModelRequestTrainingConfigOpenpipeHyperparameters,
)
from .create_model_request_training_config_openpipe_preference_hyperparameters import (
    CreateModelRequestTrainingConfigOpenpipePreferenceHyperparameters,
)
from .create_model_request_training_config_openpipe_sft_hyperparameters import (
    CreateModelRequestTrainingConfigOpenpipeSftHyperparameters,
)

try:
    import pydantic.v1 as pydantic  # type: ignore
except ImportError:
    import pydantic  # type: ignore


class CreateModelRequestTrainingConfigOpenpipe(pydantic.BaseModel):
    base_model: str = pydantic.Field(
        alias="baseModel",
        description="The base model to train from. This could be a base model name or the slug of a previously trained model. Supported base models include: `meta-llama/Meta-Llama-3.1-8B-Instruct`, `meta-llama/Meta-Llama-3.1-70B-Instruct`, `meta-llama/Llama-3.3-70B-Instruct`, `meta-llama/Llama-3.1-8B`, `meta-llama/Llama-3.1-70B`, `Qwen/Qwen2.5-72B-Instruct`, `Qwen/Qwen2.5-Coder-7B-Instruct`, `Qwen/Qwen2.5-Coder-32B-Instruct`, `Qwen/Qwen2.5-1.5B-Instruct`, `Qwen/Qwen2.5-7B-Instruct`, `Qwen/Qwen2-VL-7B-Instruct`, `Qwen/Qwen2.5-14B-Instruct`, `mistralai/Mistral-Nemo-Base-2407`, `mistralai/Mistral-Small-24B-Base-2501`, `meta-llama/Llama-3.2-1B-Instruct`, `meta-llama/Llama-3.2-3B-Instruct`, `google/gemma-3-1b-it`, `google/gemma-3-4b-it`, `google/gemma-3-12b-it`, `google/gemma-3-27b-it`",
    )
    enable_sft: typing.Optional[bool] = pydantic.Field(
        description="Whether to enable SFT training. If true, the model will be trained using SFT. Can be used in conjunction with DPO training."
    )
    enable_preference_tuning: typing.Optional[bool] = pydantic.Field(
        description="Whether to enable DPO training. If true, the model will be trained using DPO. Can be used in conjunction with SFT training."
    )
    sft_hyperparameters: typing.Optional[CreateModelRequestTrainingConfigOpenpipeSftHyperparameters] = pydantic.Field(
        description="Hyperparameters for SFT training job. Ensure `enable_sft` is true. If no SFT hyperparameters are provided, default values will be used."
    )
    preference_hyperparameters: typing.Optional[
        CreateModelRequestTrainingConfigOpenpipePreferenceHyperparameters
    ] = pydantic.Field(
        description="Hyperparameters for DPO training job. Ensure `enable_preference_tuning` is true. If no preference hyperparameters are provided, default values will be used."
    )
    hyperparameters: typing.Optional[CreateModelRequestTrainingConfigOpenpipeHyperparameters] = pydantic.Field(
        description="DEPRECATED: Use the `sft_hyperparameters` and `preference_hyperparameters` fields instead."
    )

    def json(self, **kwargs: typing.Any) -> str:
        kwargs_with_defaults: typing.Any = {"by_alias": True, "exclude_unset": True, **kwargs}
        return super().json(**kwargs_with_defaults)

    def dict(self, **kwargs: typing.Any) -> typing.Dict[str, typing.Any]:
        kwargs_with_defaults: typing.Any = {"by_alias": True, "exclude_unset": True, **kwargs}
        return super().dict(**kwargs_with_defaults)

    class Config:
        frozen = True
        smart_union = True
        allow_population_by_field_name = True
        json_encoders = {dt.datetime: serialize_datetime}
