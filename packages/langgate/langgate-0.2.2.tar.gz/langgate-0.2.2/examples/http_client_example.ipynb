{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTTP Client Example\n",
    "\n",
    "This notebook demonstrates how to use the `HTTPRegistryClient` to connect to a remote LangGate service for model information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint as pp\n",
    "from pydantic import SecretStr\n",
    "\n",
    "# Configure server URL\n",
    "LANGGATE_URL = \"http://localhost:4000/api/v1\"\n",
    "\n",
    "# If running the langgate server behind Envoy proxy in docker:\n",
    "# LANGGATE_URL = \"http://localhost:10000/api/v1\"\n",
    "\n",
    "# If running and accessing langgate from within a kubernetes cluster:\n",
    "# LANGGATE_URL = \"http://langgate.ns.svc.cluster.local:10000/api/v1\"\n",
    "\n",
    "# Optional: Set API key (if your LangGate service requires authentication)\n",
    "api_key = SecretStr(\"your_api_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the HTTP Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-07-09 23:42:37\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mcreating_http_registry_client_singleton\u001b[0m\n",
      "\u001b[2m2025-07-09 23:42:37\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1minitialized_base_http_registry_client\u001b[0m \u001b[36mapi_key\u001b[0m=\u001b[35mSecretStr('**********')\u001b[0m \u001b[36mbase_url\u001b[0m=\u001b[35mhttp://localhost:4000/api/v1\u001b[0m \u001b[36mimage_info_cls\u001b[0m=\u001b[35m<class 'langgate.core.models.ImageModelInfo'>\u001b[0m \u001b[36mllm_info_cls\u001b[0m=\u001b[35m<class 'langgate.core.models.LLMInfo'>\u001b[0m\n",
      "\u001b[2m2025-07-09 23:42:37\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1minitialized_http_registry_client_singleton\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langgate.client.http import HTTPRegistryClient\n",
    "\n",
    "# Initialize the HTTP client\n",
    "# HTTPRegistryClient is a singleton\n",
    "client = HTTPRegistryClient(base_url=LANGGATE_URL, api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Available Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-07-09 23:42:41\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mrefreshing_model_caches       \u001b[0m\n",
      "\u001b[2m2025-07-09 23:42:41\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mrefreshed_model_caches        \u001b[0m \u001b[36mimage_count\u001b[0m=\u001b[35m33\u001b[0m \u001b[36mllm_count\u001b[0m=\u001b[35m61\u001b[0m\n",
      "Available LLMs: 61\n",
      "- openai/gpt-4.1: GPT-4.1\n",
      "- openai/gpt-4.1-mini: GPT-4.1 mini\n",
      "- openai/gpt-4.1-nano: GPT-4.1 nano\n",
      "- openai/gpt-4o: GPT-4o\n",
      "- openai/gpt-4o-mini: GPT-4o mini\n",
      "\n",
      "==================================================\n",
      "\n",
      "Available Image Models: 33\n",
      "- openai/gpt-image-1: GPT Image 1\n",
      "- openai/dall-e-3: DALL-E 3\n",
      "- xai/grok-2-image: Grok 2 Image\n",
      "- google/imagen-4-ultra: Imagen 4 Ultra\n",
      "- google/imagen-4: Imagen 4\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # List available LLMs\n",
    "    llms = await client.list_llms()\n",
    "    print(f\"Available LLMs: {len(llms)}\")\n",
    "    for model in llms[:5]:  # Show first 5 models\n",
    "        print(f\"- {model.id}: {model.name}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "    # List available image models\n",
    "    image_models = await client.list_image_models()\n",
    "    print(f\"Available Image Models: {len(image_models)}\")\n",
    "    for model in image_models[:5]:  # Show first 5 models\n",
    "        print(f\"- {model.id}: {model.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to the LangGate service: {str(e)}\")\n",
    "    print(\"Note: This example requires a running LangGate service.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Detailed Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: GPT-4.1\n",
      "Provider: OpenAI\n",
      "\n",
      "Model metadata:\n",
      "{'capabilities': {'supports_parallel_tool_calls': True,\n",
      "                  'supports_prompt_caching': True,\n",
      "                  'supports_response_schema': True,\n",
      "                  'supports_system_messages': True,\n",
      "                  'supports_tool_choice': True,\n",
      "                  'supports_tools': True,\n",
      "                  'supports_vision': True},\n",
      " 'context_window': {'max_input_tokens': 1047576, 'max_output_tokens': 32768},\n",
      " 'costs': {'cache_read_input_token_cost': Decimal('5E-7'),\n",
      "           'input_cost_per_token': Decimal('0.000002'),\n",
      "           'output_cost_per_token': Decimal('0.000008')},\n",
      " 'description': \"GPT-4.1 is the latest iteration of OpenAI's flagship model \"\n",
      "                'with improved capabilities across all domains.',\n",
      " 'id': 'openai/gpt-4.1',\n",
      " 'name': 'GPT-4.1',\n",
      " 'provider': {'id': 'openai', 'name': 'OpenAI'},\n",
      " 'provider_id': 'openai',\n",
      " 'updated_dt': datetime.datetime(2025, 7, 9, 22, 42, 31, 394249, tzinfo=TzInfo(UTC))}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Get detailed information about a specific language model\n",
    "    if llms and len(llms) > 0:\n",
    "        model_id = llms[0].id\n",
    "        model_info = await client.get_llm_info(model_id)\n",
    "\n",
    "        print(f\"\\nModel: {model_info.name}\")\n",
    "        print(f\"Provider: {model_info.provider.name}\")\n",
    "\n",
    "        print(\"\\nModel metadata:\")\n",
    "        pp(model_info.model_dump(exclude_none=True))\n",
    "except Exception as exc:\n",
    "    print(f\"Error fetching model information: {str(exc)}\")\n",
    "    print(\"Note: This example requires a running LangGate service.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: GPT Image 1\n",
      "Provider: OpenAI\n",
      "\n",
      "Model metadata:\n",
      "{'costs': {'image_generation': {'quality_tiers': {'high': {'1024x1024': Decimal('0.167'),\n",
      "                                                           '1024x1536': Decimal('0.25'),\n",
      "                                                           '1536x1024': Decimal('0.25')},\n",
      "                                                  'low': {'1024x1024': Decimal('0.011'),\n",
      "                                                          '1024x1536': Decimal('0.016'),\n",
      "                                                          '1536x1024': Decimal('0.016')},\n",
      "                                                  'medium': {'1024x1024': Decimal('0.042'),\n",
      "                                                             '1024x1536': Decimal('0.063'),\n",
      "                                                             '1536x1024': Decimal('0.063')}}},\n",
      "           'token_costs': {'input_cached_cost_per_token': Decimal('0.0000025'),\n",
      "                           'input_cost_per_token': Decimal('0.00001'),\n",
      "                           'output_cost_per_token': Decimal('0.00004')}},\n",
      " 'description': 'GPT-Image-1 is a multimodal model that accepts both text and '\n",
      "                'image inputs, and produces image outputs.',\n",
      " 'id': 'openai/gpt-image-1',\n",
      " 'name': 'GPT Image 1',\n",
      " 'provider': {'id': 'openai', 'name': 'OpenAI'},\n",
      " 'provider_id': 'openai',\n",
      " 'updated_dt': datetime.datetime(2025, 7, 9, 22, 42, 31, 394911, tzinfo=TzInfo(UTC))}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Get detailed information about a specific image model\n",
    "    if image_models and len(image_models) > 0:\n",
    "        model_id = image_models[0].id\n",
    "        model_info = await client.get_image_model_info(model_id)\n",
    "\n",
    "        print(f\"\\nModel: {model_info.name}\")\n",
    "        print(f\"Provider: {model_info.provider.name}\")\n",
    "\n",
    "        print(\"\\nModel metadata:\")\n",
    "        pp(model_info.model_dump(exclude_none=True))\n",
    "except Exception as exc:\n",
    "    print(f\"Error fetching model information: {str(exc)}\")\n",
    "    print(\"Note: This example requires a running LangGate service.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgate (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
