# LLMBrick

ä¸€å€‹æ¨¡çµ„åŒ–çš„ LLM æ‡‰ç”¨é–‹ç™¼æ¡†æ¶ï¼Œæ”¯æ´å¤šç¨®é€šä¿¡å”è­°å’Œå¯æ’æ‹”çš„çµ„ä»¶æ¶æ§‹ã€‚

## ç‰¹è‰²

- ğŸ§± **æ¨¡çµ„åŒ–è¨­è¨ˆ**: åŸºæ–¼ Brick çµ„ä»¶çš„å¯æ’æ‹”æ¶æ§‹
- ğŸ”„ **å¤šå”è­°æ”¯æ´**: SSEã€gRPCã€(~~WebSocket~~ã€~~WebRTC~~)
- ğŸ¤– **å¤š LLM æ”¯æ´**: OpenAIã€Anthropicã€æœ¬åœ°æ¨¡å‹
- ğŸ¤ **èªéŸ³è™•ç†**: ASR èªéŸ³è­˜åˆ¥æ•´åˆ
- ğŸ“š **RAG æ”¯æ´**: å…§å»ºæª¢ç´¢å¢å¼·ç”Ÿæˆ
- ğŸ”§ **æ˜“æ–¼æ“´å±•**: æ’ä»¶ç³»çµ±å’Œè‡ªå®šç¾©çµ„ä»¶

## å¿«é€Ÿé–‹å§‹

### å®‰è£

```bash
pip install llmbrick
```

### åŸºæœ¬ä½¿ç”¨

```python
from llmbrick import Pipeline, OpenAILLM
from llmbrick.servers.sse import SSEServer

# å»ºç«‹ Pipeline
pipeline = Pipeline()
pipeline.add_brick(OpenAILLM(api_key="your-api-key"))

# å•Ÿå‹• SSE æœå‹™
server = SSEServer(pipeline)
server.run(host="0.0.0.0", port=8000)
```

## ç¯„ä¾‹

#### é‹ä½œå–®å…ƒ Brick (ä½¿ç”¨å…§å»ºBrickï¼ŒDecoratorç›´æ¥æ›¿æ›è‡ªå·±çš„func)

```python
from llmbrick.core.brick import BaseBrick
import nest_asyncio

class LLMBrick(BaseBrick[str, str]):
    pass


llm = LLMBrick()

@llm.unary()
async def input(prompt: str) -> str:
    return f"user input: {prompt}"

result = await llm.run_unary("What is your name?") #ç›´æ¥èª¿ç”¨å°±æœ¬æ©Ÿé‹ç®—
```

#### é‹ä½œå–®å…ƒ Brick (ç¹¼æ‰¿Brickï¼Œå®¢è£½è‡ªå·±éœ€è¦çš„Brick)

```python
from llmbrick.core.brick import BaseBrick, unary_handler
import nest_asyncio


class MyNewBrick(BaseBrick[str, str]):
    def __init__(self, some_param: str, **kwargs):
        super().__init__(**kwargs)
        self.some_param = some_param
    
    @unary_handler
    async def process(self, input_data: str) -> str:
        return f"Processed: {input_data} with param {self.some_param}"
    
nest_asyncio.apply()

brick = MyNewBrick(some_param="example")

result = await brick.run_unary("What is your name? ") #ç›´æ¥èª¿ç”¨å°±æœ¬æ©Ÿé‹ç®—
```

#### Brickè½‰æ›ç‚ºç•°æ­¥ gRPC Server

```python
import asyncio
from llmbrick.servers.grpc.server import GrpcServer
from llmbrick.bricks.llm.base_llm import LLMBrick

async def main():
    # å»ºç«‹ LLM Brick
    brick = LLMBrick(default_prompt="ä½ æ˜¯ä¸€å€‹æœ‰ç”¨çš„åŠ©æ‰‹")
    
    # å»ºç«‹ç•°æ­¥ gRPC ä¼ºæœå™¨
    server = GrpcServer(port=50051)
    server.register_service(brick)
    
    # å•Ÿå‹•ç•°æ­¥ä¼ºæœå™¨
    await server.start()

# é‹è¡Œä¼ºæœå™¨
asyncio.run(main())
```

#### Brickè½‰æ›ç‚ºç•°æ­¥ gRPC Client

```python
import asyncio
from llmbrick.bricks.llm.base_llm import LLMBrick
from llmbrick.protocols.models.bricks.llm_types import LLMRequest

async def main():
    # å»ºç«‹ç•°æ­¥ gRPC å®¢æˆ¶ç«¯
    brick = LLMBrick.toGrpcClient(remote_address="127.0.0.1:50051")
    
    # å–®æ¬¡è«‹æ±‚ - è·Ÿæœ¬æ©Ÿèª¿ç”¨çš„å¯«æ³•ä¸€æ¨£
    request = LLMRequest(prompt="What is your name?")
    result = await brick.run_unary(request)
    print(result)
    
    # æµå¼è«‹æ±‚
    async for chunk in brick.run_output_streaming(request):
        print(chunk)
    
    # æ¸…ç†è³‡æº
    await brick._grpc_channel.close()

# é‹è¡Œå®¢æˆ¶ç«¯
asyncio.run(main())
```

## âš ï¸ æ³¨æ„äº‹é …ï¼šgRPC èˆ‡ Python çš„æ•´æ•¸å‹æ…‹å‚³è¼¸å•é¡Œ

åœ¨ä½¿ç”¨ gRPC æ–¼ Python é€²è¡Œé–‹ç™¼æ™‚ï¼Œéœ€ç‰¹åˆ¥æ³¨æ„æ•´æ•¸å‹æ…‹ï¼ˆå¦‚ int64ï¼‰åœ¨å‚³è¼¸éç¨‹ä¸­å¯èƒ½ç™¼ç”Ÿç²¾åº¦æå¤±çš„å•é¡Œã€‚ç”±æ–¼ Python çš„ gRPC å¯¦ä½œæœƒå°‡è¶…é JavaScript Number ç²¾åº¦ç¯„åœçš„ int64 è½‰ç‚º floatï¼Œå°è‡´å¤§æ•´æ•¸ç„¡æ³•æ­£ç¢ºé‚„åŸã€‚

**å»ºè­°ï¼š**
- è‹¥éœ€å‚³éå¤§æ•´æ•¸ï¼Œè«‹è€ƒæ…®å°‡å…¶ä»¥ string å‹æ…‹å‚³éï¼Œæˆ–æ–¼ proto è¨­è¨ˆæ™‚ç‰¹åˆ¥æ¨™è¨»èˆ‡è™•ç†ã€‚
- è«‹æ–¼å‰å¾Œç«¯å”å®šè¨­è¨ˆæ™‚ï¼Œæ˜ç¢ºç¢ºèªæ•¸å€¼ç¯„åœèˆ‡å‹æ…‹ï¼Œé¿å…è³‡æ–™éºå¤±æˆ–éŒ¯èª¤ã€‚


#### å»ºç«‹SSEæ¥å£

```python
from llmbrick.servers.sse.server import SSEServer
import asyncio

server = SSEServer() 
# æœƒè‡ªå‹•å»ºç«‹SSEçš„Router

fast_app = server.fastapi_app # é€™ç­‰åŒFastAPIçš„app
# ç­‰åƒ¹ app = FastAPI()

@server.handler # ä½¿ç”¨Decoratorè‡ªè¨‚éœ€è¦çš„é‚è¼¯ï¼Œé€™é‚Šå°±æ˜¯æ•´åˆæ‰€æœ‰LLM Brickçš„å€å¡Š
async def simple_flow(request_body):
    # æ¨¡æ“¬è¨Šæ¯è™•ç†èˆ‡å›æ‡‰
    yield {"id": "1", "type": "text", "text": "Hello, this is a streaming response.", "progress": "IN_PROGRESS"}
    await asyncio.sleep(0.5)
    yield {"id": "1", "type": "done", "progress": "DONE"}

if __name__ == "__main__":
    server.run(host="0.0.0.0", port=8000)
```

#### å»ºç«‹SSEæ¥å£ï¼Œæ­é…Brick

```python
from llmbrick.servers.sse.server import SSEServer
from llmbrick.bricks.llm.base_llm import LLMBrick
from llmbrick.bricks.intention.base_intention import IntentionBrick
import asyncio

server = SSEServer() 
# æœƒè‡ªå‹•å»ºç«‹SSEçš„Router

fast_app = server.fastapi_app # é€™ç­‰åŒFastAPIçš„app
# ç­‰åƒ¹ app = FastAPI()

intention_brick = IntentionBrick()
llm_brick = LLMBrick.toGrpcClient(remote_address="192.168.1.100:50051")

@server.handler #ä½¿ç”¨Decoratorè‡ªè¨‚éœ€è¦çš„é‚è¼¯ï¼Œé€™é‚Šå°±æ˜¯æ•´åˆæ‰€æœ‰LLM Brickçš„å€å¡Š
async def simple_flow(request_body):
    # æ¨¡æ“¬è¨Šæ¯è™•ç†èˆ‡å›æ‡‰
    text = request_body.text;
    
    intention_list = await intention_brick.run_unary(text)
    
    ... # è‡ªè¨‚ç´°ç¯€

    input_data = {
        "intention_list": intention_list,
        "model_id": request_body.modelId,
        "max_tokens": request_body.maxTokens,
        "context": [...]
    }
    try:
        for i in llm_brick.run_output_stream(input_data):
            output = {"event": "message", "data": i.text}
            yield output
    except:
        yield {"event": "done"}
    
    yield {"event": "done"}
        

if __name__ == "__main__":
    server.run(host="0.0.0.0", port=8000)
```

## æ–‡æª”

- [å¿«é€Ÿé–‹å§‹](docs/quickstart.md)
- [API åƒè€ƒ](docs/api_reference/)
- [æ•™å­¸ç¯„ä¾‹](docs/tutorials/)

## æˆæ¬Š

MIT License
## Metrics Utilities

The `llmbrick.utils.metrics` module provides decorators for monitoring function performance and resource usage. All decorators support both sync and async functions.

### Available Decorators

- **@measure_time**  
  Logs the execution time of the decorated function.

- **@measure_memory**  
  Logs the difference in process memory usage (RSS, MB) before and after the function runs. Requires `psutil`.

- **@measure_peak_memory**  
  Logs the peak memory usage (MB) during function execution using `tracemalloc`.

### Usage Example

```python
from llmbrick.utils.metrics import measure_time, measure_memory, measure_peak_memory

@measure_time
def sync_func(x):
    return x * 2

@measure_memory
async def async_func(x):
    a = [0] * 10000
    return x + 1

@measure_peak_memory
def another_sync_func(x):
    a = [0] * 10000
    return x - 1
```

All decorators will log performance metrics using the standard logging module.  
For async functions, simply decorate as usual.