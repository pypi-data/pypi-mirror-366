# MX-RMQ ä½¿ç”¨æŒ‡å—

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Redis](https://img.shields.io/badge/redis-5.0+-red.svg)](https://redis.io/)

MX-RMQ æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½ã€å¯é çš„åŸºäºRedisçš„åˆ†å¸ƒå¼æ¶ˆæ¯é˜Ÿåˆ—ç³»ç»Ÿï¼Œæ”¯æŒæ™®é€šæ¶ˆæ¯ã€å»¶æ—¶æ¶ˆæ¯ã€ä¼˜å…ˆçº§æ¶ˆæ¯ï¼Œå…·å¤‡å®Œå–„çš„ç›‘æ§å’Œé‡è¯•æœºåˆ¶ã€‚

## ç›®å½•

- [ç‰¹æ€§æ¦‚è§ˆ](#ç‰¹æ€§æ¦‚è§ˆ)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [å®‰è£…](#å®‰è£…)
- [åŸºæœ¬ä½¿ç”¨](#åŸºæœ¬ä½¿ç”¨)
- [é«˜çº§åŠŸèƒ½](#é«˜çº§åŠŸèƒ½)
- [æ—¥å¿—ç³»ç»Ÿ](#æ—¥å¿—ç³»ç»Ÿ)
- [é…ç½®å‚è€ƒ](#é…ç½®å‚è€ƒ)
- [API å‚è€ƒ](#api-å‚è€ƒ)
- [ç›‘æ§å’Œç®¡ç†](#ç›‘æ§å’Œç®¡ç†)
- [éƒ¨ç½²æŒ‡å—](#éƒ¨ç½²æŒ‡å—)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

## ç‰¹æ€§æ¦‚è§ˆ

- ğŸš€ **é«˜æ€§èƒ½**: åŸºäºRedisçš„å†…å­˜å­˜å‚¨ï¼Œæ”¯æŒ10,000+æ¶ˆæ¯/ç§’çš„ååé‡
- ğŸ”„ **å¯é æ€§**: åŸå­æ€§Luaè„šæœ¬æ“ä½œï¼Œä¿è¯æ¶ˆæ¯ä¸ä¸¢å¤±
- â° **å»¶æ—¶æ¶ˆæ¯**: æ”¯æŒä»»æ„æ—¶é—´å»¶è¿Ÿçš„æ¶ˆæ¯è°ƒåº¦
- ğŸ·ï¸ **ä¼˜å…ˆçº§**: æ”¯æŒé«˜ã€ä¸­ã€ä½ä¼˜å…ˆçº§æ¶ˆæ¯å¤„ç†
- ğŸ” **è‡ªåŠ¨é‡è¯•**: å¯é…ç½®çš„é‡è¯•æœºåˆ¶å’ŒæŒ‡æ•°é€€é¿
- ğŸ’€ **æ­»ä¿¡é˜Ÿåˆ—**: å¤±è´¥æ¶ˆæ¯è‡ªåŠ¨è¿›å…¥æ­»ä¿¡é˜Ÿåˆ—ï¼Œæ”¯æŒäººå·¥å¹²é¢„
- ğŸ“Š **ç›‘æ§æŒ‡æ ‡**: å®æ—¶ç›‘æ§é˜Ÿåˆ—çŠ¶æ€ã€å¤„ç†æ—¶é—´ã€ååç‡ç­‰
- ğŸ›‘ **ä¼˜é›…åœæœº**: æ”¯æŒä¼˜é›…åœæœºï¼Œç¡®ä¿æ¶ˆæ¯å¤„ç†å®Œæˆ
- ğŸ”§ **æ˜“äºä½¿ç”¨**: ç®€æ´çš„APIè®¾è®¡ï¼Œå¼€ç®±å³ç”¨

## è®¾è®¡é™åˆ¶

- âŒ **ä¸æ”¯æŒæ¶ˆè´¹è€…ç»„**: æ¯ä¸ªtopicåªèƒ½è¢«å•ä¸€æ¶ˆè´¹è€…ç»„è´Ÿè½½å‡è¡¡æ¶ˆè´¹ï¼Œå¦‚éœ€å¤šç»„æ¶ˆè´¹åŒä¸€æ¶ˆæ¯ï¼Œè¯·åˆ›å»ºå¤šä¸ªtopicå¹¶æŠ•é€’å¤šæ¬¡æ¶ˆæ¯

## å¿«é€Ÿå¼€å§‹

å¯åŠ¨ redis
```shell
docker run -d --name redis8 -p 6379:6379 redis:8 redis-server
```
### 30ç§’å¿«é€Ÿä½“éªŒ

```python
import asyncio
from mx_rmq import MQConfig, RedisMessageQueue

async def handle_order(payload: dict) -> None:
    """å¤„ç†è®¢å•æ¶ˆæ¯"""
    print(f"å¤„ç†è®¢å•: {payload['order_id']}")
    # ä½ çš„ä¸šåŠ¡é€»è¾‘
    await asyncio.sleep(1)

async def main():
    # åˆ›å»ºæ¶ˆæ¯é˜Ÿåˆ—
    mq = RedisMessageQueue()
    
    # æ³¨å†Œæ¶ˆæ¯å¤„ç†å™¨
    mq.register("order_created", handle_order)
    
    # ç”Ÿäº§æ¶ˆæ¯
    await mq.produce("order_created", {
        "order_id": "ORD_123",
        "user_id": 456,
        "amount": 99.99
    })
    
    # ä¸ä¼šé˜»å¡
    task = await mq.start_background()  
    #é˜»å¡ä¸‹
    
    await task
if __name__ == "__main__":
    asyncio.run(main())
```

## å®‰è£…

### ä½¿ç”¨ uv (æ¨è)

```bash
# æ·»åŠ åˆ°ç°æœ‰é¡¹ç›®
uv add mx-rmq

# æˆ–è€…ä»æºç å®‰è£…
git clone https://github.com/CodingOX/mx-rmq.git
cd mx-rmq
uv sync
```

### ä½¿ç”¨ pip

```bash
pip install mx-rmq

# æˆ–ä»æºç å®‰è£…
pip install git+https://github.com/CodingOX/mx-rmq.git
```

### ç³»ç»Ÿè¦æ±‚

- Python 3.12+
- Redis 5.0+

## åŸºæœ¬ä½¿ç”¨

### 1. åˆ›å»ºæ¶ˆæ¯é˜Ÿåˆ—

```python
from mx_rmq import MQConfig, RedisMessageQueue

# ä½¿ç”¨é»˜è®¤é…ç½®
mq = RedisMessageQueue()

# æˆ–è‡ªå®šä¹‰é…ç½®
config = MQConfig(
    redis_host="redis://localhost:6379",
    max_workers=10,
    task_queue_size=20
)
mq = RedisMessageQueue(config)
```

### 2. æ³¨å†Œæ¶ˆæ¯å¤„ç†å™¨

```python
# æ–¹å¼1: ä½¿ç”¨è£…é¥°å™¨
@mq.register("user_registration")
async def handle_user_registration(payload: dict) -> None:
    user_id = payload['user_id']
    email = payload['email']
    print(f"æ¬¢è¿æ–°ç”¨æˆ·: {user_id} ({email})")

# æ–¹å¼2: ç›´æ¥æ³¨å†Œ
async def handle_payment(payload: dict) -> None:
    print(f"å¤„ç†æ”¯ä»˜: {payload}")

mq.register("payment_completed", handle_payment)
```

### 3. ç”Ÿäº§æ¶ˆæ¯

```python
# ç”Ÿäº§æ™®é€šæ¶ˆæ¯
message_id = await mq.produce("user_registration", {
    "user_id": 12345,
    "email": "user@example.com",
    "timestamp": "2024-01-01T00:00:00Z"
})

print(f"æ¶ˆæ¯å·²å‘é€: {message_id}")
```

### 4. å¯åŠ¨æ¶ˆè´¹è€…

```python
# å¯åŠ¨æ¶ˆè´¹è€…ï¼ˆä¼šé˜»å¡ï¼Œç›´åˆ°æ”¶åˆ°åœæœºä¿¡å·ï¼‰
await mq.start_dispatch_consuming()
```

## é«˜çº§åŠŸèƒ½

### å»¶æ—¶æ¶ˆæ¯

```python
# 5åˆ†é’Ÿåå‘é€æé†’
await mq.produce(
    topic="send_reminder",
    payload={"user_id": 123, "type": "payment_due"},
    delay=300  # 300ç§’åæ‰§è¡Œ
)

# 1å°æ—¶åå‘é€é‚®ä»¶
await mq.produce(
    topic="send_email",
    payload={
        "to": "user@example.com",
        "subject": "è®¢å•ç¡®è®¤",
        "body": "æ„Ÿè°¢æ‚¨çš„è®¢å•..."
    },
    delay=3600  # 1å°æ—¶åæ‰§è¡Œ
)
```

### ä¼˜å…ˆçº§æ¶ˆæ¯

```python
from mx_rmq import MessagePriority

# é«˜ä¼˜å…ˆçº§æ¶ˆæ¯ï¼ˆä¼˜å…ˆå¤„ç†ï¼‰
await mq.produce(
    topic="system_alert",
    payload={"level": "critical", "message": "ç³»ç»Ÿå‘Šè­¦"},
    priority=MessagePriority.HIGH
)

# æ™®é€šä¼˜å…ˆçº§ï¼ˆé»˜è®¤ï¼‰
await mq.produce(
    topic="user_activity",
    payload={"user_id": 123, "action": "login"},
    priority=MessagePriority.NORMAL
)

# ä½ä¼˜å…ˆçº§æ¶ˆæ¯ï¼ˆæœ€åå¤„ç†ï¼‰
await mq.produce(
    topic="analytics_data",
    payload={"event": "page_view", "page": "/home"},
    priority=MessagePriority.LOW
)
```

### è‡ªå®šä¹‰é‡è¯•é…ç½®

```python
config = MQConfig(
    redis_url="redis://localhost:6379",
    max_retries=5,  # æœ€å¤§é‡è¯•5æ¬¡
    retry_delays=[30, 60, 300, 900, 1800],  # é‡è¯•é—´éš”ï¼š30s, 1m, 5m, 15m, 30m
    processing_timeout=300,  # 5åˆ†é’Ÿå¤„ç†è¶…æ—¶
)

mq = RedisMessageQueue(config)
```

### æ¶ˆæ¯ç”Ÿå­˜æ—¶é—´(TTL)

```python
# è®¾ç½®æ¶ˆæ¯1å°æ—¶åè¿‡æœŸ
await mq.produce(
    topic="temp_notification",
    payload={"message": "ä¸´æ—¶é€šçŸ¥"},
    ttl=3600  # 1å°æ—¶åè¿‡æœŸ
)
```

### æ‰¹é‡ç”Ÿäº§æ¶ˆæ¯

```python
# æ‰¹é‡å‘é€å¤šä¸ªæ¶ˆæ¯
messages = [
    {"topic": "order_created", "payload": {"order_id": f"ORD_{i}"}}
    for i in range(100)
]

for msg in messages:
    await mq.produce(msg["topic"], msg["payload"])
```

## æ—¥å¿—ç³»ç»Ÿ

MX-RMQ ä½¿ç”¨æ ‡å‡†çš„ Python logging ç³»ç»Ÿï¼Œå¹¶æä¾›äº†ä¾¿æ·çš„é…ç½®å‡½æ•°å’Œå½©è‰²æ—¥å¿—æ”¯æŒã€‚

### å¿«é€Ÿå¼€å§‹

#### 1. åŸºæœ¬æ—¥å¿—é…ç½®

```python
import logging
from mx_rmq import RedisMessageQueue

# æ–¹å¼1ï¼šä½¿ç”¨æ ‡å‡† logging é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# æ–¹å¼2ï¼šä½¿ç”¨ MX-RMQ æä¾›çš„ä¾¿æ·é…ç½®
from mx_rmq.logging import setup_basic_logging
setup_basic_logging("INFO")

# åˆ›å»ºæ¶ˆæ¯é˜Ÿåˆ—
mq = RedisMessageQueue()

# ç°åœ¨å¯ä»¥çœ‹åˆ°æ‰€æœ‰å†…éƒ¨æ—¥å¿—
await mq.start_dispatch_consuming()
```

#### 2. å½©è‰²æ—¥å¿—é…ç½®

```python
from mx_rmq.logging import setup_colored_logging

# é…ç½®å½©è‰²æ—¥å¿—è¾“å‡º
setup_colored_logging("INFO")

# æˆ–è€…ä½¿ç”¨ç®€æ´çš„å½©è‰²æ—¥å¿—ï¼ˆä¸æ˜¾ç¤ºæ—¶é—´æˆ³ï¼‰
from mx_rmq.logging import setup_simple_colored_logging
setup_simple_colored_logging("INFO")

# åˆ›å»ºæ¶ˆæ¯é˜Ÿåˆ—
mq = RedisMessageQueue()
```

### æ—¥å¿—é…ç½®å‡½æ•°

MX-RMQ æä¾›äº†ä¸‰ç§ä¾¿æ·çš„æ—¥å¿—é…ç½®å‡½æ•°ï¼š

#### `setup_basic_logging(level="INFO")`
åŸºæœ¬çš„æ—¥å¿—é…ç½®ï¼Œè¾“å‡ºåˆ°æ§åˆ¶å°ï¼š

```python
from mx_rmq.logging import setup_basic_logging

setup_basic_logging("DEBUG")  # è®¾ç½®ä¸º DEBUG çº§åˆ«
```

#### `setup_colored_logging(level="INFO")`
å½©è‰²æ—¥å¿—é…ç½®ï¼Œä¸åŒçº§åˆ«ä½¿ç”¨ä¸åŒé¢œè‰²ï¼š

```python
from mx_rmq.logging import setup_colored_logging

setup_colored_logging("INFO")
```

é¢œè‰²æ–¹æ¡ˆï¼š
- ğŸ”µ **DEBUG**: é’è‰²
- ğŸŸ¢ **INFO**: ç»¿è‰²  
- ğŸŸ¡ **WARNING**: é»„è‰²
- ğŸ”´ **ERROR**: çº¢è‰²
- ğŸŸ£ **CRITICAL**: ç´«è‰²

#### `setup_simple_colored_logging(level="INFO")`
ç®€æ´çš„å½©è‰²æ—¥å¿—ï¼Œä¸æ˜¾ç¤ºæ—¶é—´æˆ³ï¼š

```python
from mx_rmq.logging import setup_simple_colored_logging

setup_simple_colored_logging("INFO")
```

### åœ¨åº”ç”¨ä¸­ä½¿ç”¨æ—¥å¿—

#### 1. æ ‡å‡†æ–¹å¼ï¼ˆæ¨èï¼‰

```python
import logging
from mx_rmq import RedisMessageQueue
from mx_rmq.logging import setup_colored_logging

# é…ç½®å½©è‰²æ—¥å¿—
setup_colored_logging("INFO")

# åœ¨æ¯ä¸ªæ¨¡å—ä¸­ä½¿ç”¨æ ‡å‡†æ–¹å¼
logger = logging.getLogger(__name__)

async def handle_order(payload: dict) -> None:
    order_id = payload.get("order_id")
    
    # è®°å½•ä¸šåŠ¡æ—¥å¿—
    logger.info("å¼€å§‹å¤„ç†è®¢å•", extra={"order_id": order_id})
    
    try:
        # å¤„ç†è®¢å•é€»è¾‘
        await process_order(payload)
        logger.info("è®¢å•å¤„ç†æˆåŠŸ", extra={"order_id": order_id})
        
    except Exception as e:
        logger.error("è®¢å•å¤„ç†å¤±è´¥", extra={"order_id": order_id}, exc_info=e)
        raise

# åˆ›å»ºæ¶ˆæ¯é˜Ÿåˆ—
mq = RedisMessageQueue()
mq.register("order_created", handle_order)
```

#### 2. ä½¿ç”¨ LoggerServiceï¼ˆå‘åå…¼å®¹ï¼‰

```python
from mx_rmq import LoggerService
from mx_rmq.logging import setup_colored_logging

# é…ç½®å½©è‰²æ—¥å¿—
setup_colored_logging("INFO")

# åˆ›å»ºæ—¥å¿—æœåŠ¡
logger_service = LoggerService("PaymentService")

# ä½¿ç”¨æ ‡å‡†çš„æ—¥å¿—æ¥å£
logger_service.logger.info("æ”¯ä»˜æœåŠ¡å¯åŠ¨")

# ä½¿ç”¨ä¾¿æ·æ–¹æ³•
logger_service.log_message_event("æ”¯ä»˜å¼€å§‹", "msg_123", "payments", user_id=456)
logger_service.log_error("æ”¯ä»˜å¤±è´¥", Exception("ç½‘ç»œé”™è¯¯"), payment_id="pay_789")
logger_service.log_metric("å¤„ç†å»¶è¿Ÿ", 150, unit="ms")
```

### å®Œæ•´ç¤ºä¾‹

```python
import asyncio
import logging
from mx_rmq import MQConfig, RedisMessageQueue
from mx_rmq.logging import setup_colored_logging

# é…ç½®å½©è‰²æ—¥å¿—
setup_colored_logging("INFO")

# è·å–åº”ç”¨æ—¥å¿—å™¨
logger = logging.getLogger("OrderApp")

async def handle_order(payload: dict) -> None:
    """å¤„ç†è®¢å•æ¶ˆæ¯"""
    order_id = payload.get("order_id")
    
    logger.info(f"å¼€å§‹å¤„ç†è®¢å•: {order_id}")
    
    try:
        # æ¨¡æ‹Ÿè®¢å•å¤„ç†
        await asyncio.sleep(1)
        
        # æ¨¡æ‹Ÿä¸åŒçš„å¤„ç†ç»“æœ
        if order_id.endswith("error"):
            raise ValueError("è®¢å•æ•°æ®æ— æ•ˆ")
        elif order_id.endswith("warn"):
            logger.warning(f"è®¢å•å¤„ç†æœ‰è­¦å‘Š: {order_id}")
        
        logger.info(f"è®¢å•å¤„ç†æˆåŠŸ: {order_id}")
        
    except Exception as e:
        logger.error(f"è®¢å•å¤„ç†å¤±è´¥: {order_id}", exc_info=e)
        raise

async def main():
    # åˆ›å»ºæ¶ˆæ¯é˜Ÿåˆ—
    mq = RedisMessageQueue()
    
    # æ³¨å†Œå¤„ç†å™¨
    mq.register("order_created", handle_order)
    
    # å‘é€ä¸€äº›æµ‹è¯•æ¶ˆæ¯
    await mq.produce("order_created", {"order_id": "ORD_001"})
    await mq.produce("order_created", {"order_id": "ORD_002_warn"})
    await mq.produce("order_created", {"order_id": "ORD_003_error"})
    
    # å¯åŠ¨æ¶ˆè´¹è€…
    await mq.start_dispatch_consuming()

if __name__ == "__main__":
    asyncio.run(main())
```

### æ—¥å¿—çº§åˆ«è¯´æ˜

```python
import logging

logger = logging.getLogger("MyApp")

# DEBUG: è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
logger.debug("è®¡ç®—æŠ˜æ‰£", extra={"original_price": 100, "discount_rate": 0.1})

# INFO: é‡è¦çš„ä¸šåŠ¡äº‹ä»¶
logger.info("è®¢å•æ”¯ä»˜æˆåŠŸ", extra={"order_id": "ORD_123", "payment_id": "PAY_456"})

# WARNING: æ½œåœ¨é—®é¢˜ä½†ä¸å½±å“åŠŸèƒ½
logger.warning("åº“å­˜ä¸è¶³", extra={"product_id": "PROD_789", "requested": 10, "available": 5})

# ERROR: é”™è¯¯éœ€è¦å…³æ³¨
logger.error("æ”¯ä»˜ç½‘å…³é”™è¯¯", extra={"order_id": "ORD_123"}, exc_info=True)

# CRITICAL: ä¸¥é‡é”™è¯¯éœ€è¦ç«‹å³å¤„ç†
logger.critical("æ•°æ®åº“è¿æ¥å¤±è´¥", extra={"database": "order_db"})
```

### ç¯å¢ƒå˜é‡é…ç½®

å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡é…ç½®æ—¥å¿—çº§åˆ«ï¼š

```bash
# è®¾ç½®æ—¥å¿—çº§åˆ«
export LOG_LEVEL=DEBUG

# åœ¨åº”ç”¨ä¸­ä½¿ç”¨
python your_app.py
```

```python
import os
from mx_rmq.logging import setup_colored_logging

# ä»ç¯å¢ƒå˜é‡è¯»å–æ—¥å¿—çº§åˆ«
log_level = os.getenv("LOG_LEVEL", "INFO")
setup_colored_logging(log_level)
```

### æ—¥å¿—æœ€ä½³å®è·µ

#### 1. ä½¿ç”¨ç»“æ„åŒ–æ—¥å¿—

```python
# âœ… æ¨èï¼šä½¿ç”¨ extra å‚æ•°ä¼ é€’ç»“æ„åŒ–æ•°æ®
logger.info("è®¢å•åˆ›å»º", extra={
    "order_id": "ORD_123",
    "user_id": 456, 
    "amount": 99.99,
    "currency": "USD"
})

# âŒ é¿å…ï¼šå­—ç¬¦ä¸²æ‹¼æ¥
logger.info(f"ç”¨æˆ· {user_id} åˆ›å»ºäº†è®¢å• {order_id}ï¼Œé‡‘é¢ {amount}")
```

#### 2. é”™è¯¯å¤„ç†ä¸­çš„æ—¥å¿—

```python
async def handle_payment(payload: dict) -> None:
    order_id = payload.get("order_id")
    
    try:
        await process_payment(payload)
        
    except PaymentValidationError as e:
        # ä¸šåŠ¡éªŒè¯é”™è¯¯ï¼Œè®°å½•ä½†ä¸é‡è¯•
        logger.warning("æ”¯ä»˜éªŒè¯å¤±è´¥", extra={
            "order_id": order_id, 
            "error": str(e),
            "error_type": "validation"
        })
        raise  # é‡æ–°æŠ›å‡ºï¼Œè¿›å…¥æ­»ä¿¡é˜Ÿåˆ—
        
    except PaymentGatewayError as e:
        # å¤–éƒ¨æœåŠ¡é”™è¯¯ï¼Œå¯é‡è¯•
        logger.error("æ”¯ä»˜ç½‘å…³é”™è¯¯", extra={
            "order_id": order_id,
            "error": str(e),
            "error_type": "gateway",
            "retryable": True
        })
        raise  # é‡æ–°æŠ›å‡ºï¼Œè§¦å‘é‡è¯•
        
    except Exception as e:
        # æœªçŸ¥é”™è¯¯ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
        logger.error("æ”¯ä»˜å¤„ç†å¼‚å¸¸", extra={
            "order_id": order_id,
            "error": str(e),
            "error_type": type(e).__name__,
            "payload": payload
        }, exc_info=True)
        raise
```

#### 3. æ€§èƒ½ç›‘æ§æ—¥å¿—

```python
import time

async def timed_handler(payload: dict) -> None:
    start_time = time.time()
    order_id = payload.get("order_id")
    
    try:
        await process_order(payload)
        
    finally:
        processing_time = time.time() - start_time
        logger.info("è®¢å•å¤„ç†å®Œæˆ", extra={
            "order_id": order_id,
            "processing_time": f"{processing_time:.2f}s"
        })
        
                 # æ€§èƒ½å‘Šè­¦
         if processing_time > 5:
             logger.warning("è®¢å•å¤„ç†æ—¶é—´è¿‡é•¿", extra={
                 "order_id": order_id,
                 "processing_time": f"{processing_time:.2f}s"
             })
```

### æµ‹è¯•æ—¥å¿—åŠŸèƒ½

æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æµ‹è¯•ä¸åŒçš„æ—¥å¿—åŠŸèƒ½ï¼š

```bash
# æµ‹è¯•æ‰€æœ‰æ—¥å¿—åŠŸèƒ½ï¼ˆæ¨èï¼‰
uv run python examples/usage_sample.py test_all_logging

# æµ‹è¯•æ ‡å‡†æ—¥å¿—
uv run python examples/usage_sample.py logging

# æµ‹è¯•å½©è‰²æ—¥å¿—
uv run python examples/usage_sample.py colored

# æµ‹è¯•ç®€æ´å½©è‰²æ—¥å¿—
uv run python examples/usage_sample.py simple_colored

# æµ‹è¯• LoggerService å…¼å®¹æ€§
uv run python examples/usage_sample.py logger

# æµ‹è¯•å¸¦å½©è‰²æ—¥å¿—çš„ consumer
uv run python examples/usage_sample.py consumer
```

### å¸¸è§é—®é¢˜

#### Q: ä¸ºä»€ä¹ˆçœ‹ä¸åˆ°æ—¥å¿—è¾“å‡ºï¼Ÿ

**A:** MX-RMQ éµå¾ª Python åº“æœ€ä½³å®è·µï¼Œé»˜è®¤ä½¿ç”¨ `NullHandler`ï¼Œéœ€è¦ç”¨æˆ·é…ç½®æ—¥å¿—å¤„ç†å™¨ï¼š

```python
# è§£å†³æ–¹æ¡ˆ1ï¼šä½¿ç”¨ä¾¿æ·é…ç½®å‡½æ•°
from mx_rmq.logging import setup_colored_logging
setup_colored_logging("INFO")

# è§£å†³æ–¹æ¡ˆ2ï¼šä½¿ç”¨æ ‡å‡† logging é…ç½®
import logging
logging.basicConfig(level=logging.INFO)
```

#### Q: å¦‚ä½•åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨æ–‡ä»¶æ—¥å¿—ï¼Ÿ

**A:** ä½¿ç”¨æ ‡å‡†çš„ Python logging é…ç½®ï¼š

```python
import logging
from logging.handlers import RotatingFileHandler

# é…ç½®æ–‡ä»¶æ—¥å¿—
file_handler = RotatingFileHandler(
    'app.log', 
    maxBytes=10*1024*1024,  # 10MB
    backupCount=5
)
file_handler.setLevel(logging.INFO)

# é…ç½®æ§åˆ¶å°æ—¥å¿—
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)

# è®¾ç½®æ ¼å¼
formatter = logging.Formatter(
    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
file_handler.setFormatter(formatter)
console_handler.setFormatter(formatter)

# é…ç½®æ ¹æ—¥å¿—å™¨
logger = logging.getLogger('mx_rmq')
logger.setLevel(logging.INFO)
logger.addHandler(file_handler)
logger.addHandler(console_handler)
```

## é…ç½®å‚è€ƒ

### MQConfig å®Œæ•´å‚æ•°

```python
from mx_rmq import MQConfig

config = MQConfig(
    # Redis è¿æ¥é…ç½®
    redis_host="redis://localhost:6379",      # Redisè¿æ¥URL
    redis_db=0,                              # Redisæ•°æ®åº“ç¼–å· (0-15)
    redis_password=None,                     # Rediså¯†ç 
    queue_prefix="",                         # é˜Ÿåˆ—å‰ç¼€ï¼Œç”¨äºå¤šç¯å¢ƒéš”ç¦»
    connection_pool_size=20,                 # è¿æ¥æ± å¤§å°
    
    # æ¶ˆè´¹è€…é…ç½®
    max_workers=5,                           # æœ€å¤§å·¥ä½œåç¨‹æ•°
    task_queue_size=8,                       # æœ¬åœ°ä»»åŠ¡é˜Ÿåˆ—å¤§å°
    
    # æ¶ˆæ¯ç”Ÿå‘½å‘¨æœŸé…ç½®
    message_ttl=86400,                       # æ¶ˆæ¯TTLï¼ˆç§’ï¼‰ï¼Œé»˜è®¤24å°æ—¶
    processing_timeout=180,                  # æ¶ˆæ¯å¤„ç†è¶…æ—¶ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤3åˆ†é’Ÿ
    
    # é‡è¯•é…ç½®
    max_retries=3,                           # æœ€å¤§é‡è¯•æ¬¡æ•°
    retry_delays=[60, 300, 1800],           # é‡è¯•å»¶è¿Ÿé—´éš”ï¼ˆç§’ï¼‰
    
    # æ­»ä¿¡é˜Ÿåˆ—é…ç½®
    enable_dead_letter=True,                 # æ˜¯å¦å¯ç”¨æ­»ä¿¡é˜Ÿåˆ—
    
    # ç›‘æ§é…ç½®
    monitor_interval=30,                     # ç›‘æ§æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
    expired_check_interval=10,               # è¿‡æœŸæ¶ˆæ¯æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
    processing_monitor_interval=30,          # Processingé˜Ÿåˆ—ç›‘æ§é—´éš”ï¼ˆç§’ï¼‰
    batch_size=100,                          # æ‰¹å¤„ç†å¤§å°
)
```

### ç¯å¢ƒå˜é‡é…ç½®

æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡é…ç½®ï¼š

```bash
export REDIS_URL="redis://localhost:6379"
export REDIS_PASSWORD="your_password"
export MQ_MAX_WORKERS=10
export MQ_TASK_QUEUE_SIZE=20
export MQ_MESSAGE_TTL=86400
```

```python
import os
from mx_rmq import MQConfig

config = MQConfig(
    redis_host=os.getenv("REDIS_URL", "redis://localhost:6379"),
    redis_password=os.getenv("REDIS_PASSWORD"),
    max_workers=int(os.getenv("MQ_MAX_WORKERS", "5")),
    task_queue_size=int(os.getenv("MQ_TASK_QUEUE_SIZE", "8")),
    message_ttl=int(os.getenv("MQ_MESSAGE_TTL", "86400")),
)
```

## API å‚è€ƒ

### RedisMessageQueue ç±»

#### åˆå§‹åŒ–

```python
def __init__(self, config: MQConfig | None = None) -> None:
    """
    åˆå§‹åŒ–æ¶ˆæ¯é˜Ÿåˆ—
    
    Args:
        config: æ¶ˆæ¯é˜Ÿåˆ—é…ç½®ï¼Œå¦‚ä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
    """
```

#### æ ¸å¿ƒæ–¹æ³•

```python
async def produce(
    self,
    topic: str,
    payload: dict[str, Any],
    delay: int = 0,
    priority: MessagePriority = MessagePriority.NORMAL,
    ttl: int | None = None,
    message_id: str | None = None,
) -> str:
    """
    ç”Ÿäº§æ¶ˆæ¯
    
    Args:
        topic: ä¸»é¢˜åç§°
        payload: æ¶ˆæ¯è´Ÿè½½ï¼ˆå¿…é¡»æ˜¯å¯JSONåºåˆ—åŒ–çš„å­—å…¸ï¼‰
        delay: å»¶è¿Ÿæ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œ0è¡¨ç¤ºç«‹å³æ‰§è¡Œ
        priority: æ¶ˆæ¯ä¼˜å…ˆçº§
        ttl: æ¶ˆæ¯ç”Ÿå­˜æ—¶é—´ï¼ˆç§’ï¼‰ï¼ŒNoneä½¿ç”¨é…ç½®é»˜è®¤å€¼
        message_id: æ¶ˆæ¯IDï¼ŒNoneåˆ™è‡ªåŠ¨ç”ŸæˆUUID
        
    Returns:
        æ¶ˆæ¯IDï¼ˆå­—ç¬¦ä¸²ï¼‰
        
    Raises:
        ValueError: å‚æ•°éªŒè¯å¤±è´¥
        RedisError: Redisæ“ä½œå¤±è´¥
    """

def register(self, topic: str, handler: Callable) -> None:
    """
    æ³¨å†Œæ¶ˆæ¯å¤„ç†å™¨
    
    Args:
        topic: ä¸»é¢˜åç§°
        handler: å¤„ç†å‡½æ•°ï¼Œå¿…é¡»æ˜¯asyncå‡½æ•°ï¼Œæ¥å—ä¸€ä¸ªdictå‚æ•°
        
    Raises:
        ValueError: å¤„ç†å™¨ä¸æ˜¯å¯è°ƒç”¨å¯¹è±¡
    """

async def start_dispatch_consuming(self) -> None:
    """
    å¯åŠ¨æ¶ˆæ¯åˆ†å‘å’Œæ¶ˆè´¹
    
    æ­¤æ–¹æ³•ä¼šé˜»å¡ï¼Œç›´åˆ°æ”¶åˆ°åœæœºä¿¡å·(SIGINT/SIGTERM)
    
    Raises:
        RuntimeError: ç³»ç»Ÿæœªæ­£ç¡®åˆå§‹åŒ–
        RedisError: Redisè¿æ¥é”™è¯¯
    """

async def cleanup(self) -> None:
    """
    æ¸…ç†èµ„æºï¼Œå…³é—­Redisè¿æ¥æ± 
    """
```

### Message ç±»

```python
@dataclass
class Message:
    """æ¶ˆæ¯æ•°æ®ç±»"""
    id: str                    # æ¶ˆæ¯å”¯ä¸€ID
    version: str               # æ¶ˆæ¯æ ¼å¼ç‰ˆæœ¬
    topic: str                 # ä¸»é¢˜åç§°  
    payload: dict[str, Any]    # æ¶ˆæ¯è´Ÿè½½
    priority: MessagePriority  # æ¶ˆæ¯ä¼˜å…ˆçº§
    created_at: int           # åˆ›å»ºæ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
    meta: MessageMeta         # æ¶ˆæ¯å…ƒæ•°æ®

@dataclass  
class MessageMeta:
    """æ¶ˆæ¯å…ƒæ•°æ®"""
    status: MessageStatus      # æ¶ˆæ¯çŠ¶æ€
    retry_count: int          # é‡è¯•æ¬¡æ•°
    max_retries: int          # æœ€å¤§é‡è¯•æ¬¡æ•°
    retry_delays: list[int]   # é‡è¯•å»¶è¿Ÿé…ç½®
    last_error: str | None    # æœ€åä¸€æ¬¡é”™è¯¯ä¿¡æ¯
    expire_at: int            # è¿‡æœŸæ—¶é—´æˆ³
    # ... å…¶ä»–å…ƒæ•°æ®å­—æ®µ
```

### æšä¸¾ç±»å‹

```python
class MessagePriority(str, Enum):
    """æ¶ˆæ¯ä¼˜å…ˆçº§"""
    HIGH = "high"      # é«˜ä¼˜å…ˆçº§
    NORMAL = "normal"  # æ™®é€šä¼˜å…ˆçº§
    LOW = "low"        # ä½ä¼˜å…ˆçº§

class MessageStatus(str, Enum):
    """æ¶ˆæ¯çŠ¶æ€"""
    PENDING = "pending"        # å¾…å¤„ç†
    PROCESSING = "processing"  # å¤„ç†ä¸­
    COMPLETED = "completed"    # å·²å®Œæˆ
    RETRYING = "retrying"      # é‡è¯•ä¸­
    DEAD_LETTER = "dead_letter" # æ­»ä¿¡
```

## ç›‘æ§å’Œç®¡ç†

### æŒ‡æ ‡æ”¶é›†

```python
from mx_rmq import MetricsCollector

# åˆ›å»ºæŒ‡æ ‡æ”¶é›†å™¨
collector = MetricsCollector(redis=mq.redis, queue_prefix=config.queue_prefix)

# æ”¶é›†æ‰€æœ‰æŒ‡æ ‡
metrics = await collector.collect_all_metrics(["order_created", "user_registration"])

# æ‰“å°å…³é”®æŒ‡æ ‡
print(f"å¾…å¤„ç†æ¶ˆæ¯: {metrics['queue.order_created.pending']}")
print(f"å¤„ç†ä¸­æ¶ˆæ¯: {metrics['queue.order_created.processing']}")
print(f"æ€»ååé‡: {metrics['throughput.messages_per_minute']}")
print(f"æ­»ä¿¡é˜Ÿåˆ—: {metrics['queue.dlq.count']}")
```

### é˜Ÿåˆ—ç›‘æ§

```python
# ç›‘æ§å•ä¸ªé˜Ÿåˆ—
queue_metrics = await collector.collect_queue_metrics(["order_created"])
print(f"è®¢å•é˜Ÿåˆ—çŠ¶æ€: {queue_metrics}")

# ç›‘æ§å¤„ç†æ€§èƒ½
processing_metrics = await collector.collect_processing_metrics(["order_created"])
print(f"å¹³å‡å¤„ç†æ—¶é—´: {processing_metrics['order_created.avg_processing_time']}ms")
```

### æ­»ä¿¡é˜Ÿåˆ—ç®¡ç†

```python
# æŸ¥çœ‹æ­»ä¿¡é˜Ÿåˆ—
dlq_count = await mq.redis.llen("dlq:queue")
print(f"æ­»ä¿¡é˜Ÿåˆ—æ¶ˆæ¯æ•°: {dlq_count}")

# è·å–æ­»ä¿¡æ¶ˆæ¯åˆ—è¡¨
dlq_messages = await mq.redis.lrange("dlq:queue", 0, 9)  # è·å–å‰10æ¡
for msg_id in dlq_messages:
    payload = await mq.redis.hget("dlq:payload:map", msg_id)
    print(f"æ­»ä¿¡æ¶ˆæ¯: {msg_id} - {payload}")

# æ‰‹åŠ¨é‡è¯•æ­»ä¿¡æ¶ˆæ¯ï¼ˆéœ€è¦è‡ªå®šä¹‰å®ç°ï¼‰
async def retry_dead_message(message_id: str):
    # ä»æ­»ä¿¡é˜Ÿåˆ—è·å–æ¶ˆæ¯
    payload_json = await mq.redis.hget("dlq:payload:map", message_id)
    if payload_json:
        # è§£ææ¶ˆæ¯å¹¶é‡æ–°ç”Ÿäº§
        message = json.loads(payload_json)
        await mq.produce(message["topic"], message["payload"])
        # ä»æ­»ä¿¡é˜Ÿåˆ—ç§»é™¤
        await mq.redis.lrem("dlq:queue", 1, message_id)
        await mq.redis.hdel("dlq:payload:map", message_id)
```

### å®æ—¶ç›‘æ§è„šæœ¬

```python
import asyncio
import time

async def monitor_loop():
    """å®æ—¶ç›‘æ§å¾ªç¯"""
    collector = MetricsCollector(redis=mq.redis)
    
    while True:
        try:
            # æ”¶é›†æŒ‡æ ‡
            metrics = await collector.collect_all_metrics(["order_created"])
            
            # è¾“å‡ºå…³é”®æŒ‡æ ‡
            print(f"[{time.strftime('%H:%M:%S')}] é˜Ÿåˆ—çŠ¶æ€:")
            print(f"  å¾…å¤„ç†: {metrics.get('queue.order_created.pending', 0)}")
            print(f"  å¤„ç†ä¸­: {metrics.get('queue.order_created.processing', 0)}")
            print(f"  æ­»ä¿¡é˜Ÿåˆ—: {metrics.get('queue.dlq.count', 0)}")
            
            # æ£€æŸ¥å‘Šè­¦æ¡ä»¶
            pending = metrics.get('queue.order_created.pending', 0)
            if pending > 100:
                print(f"âš ï¸  å‘Šè­¦: å¾…å¤„ç†æ¶ˆæ¯ç§¯å‹ ({pending})")
                
            dlq_count = metrics.get('queue.dlq.count', 0)
            if dlq_count > 10:
                print(f"ğŸš¨ å‘Šè­¦: æ­»ä¿¡é˜Ÿåˆ—æ¶ˆæ¯è¿‡å¤š ({dlq_count})")
                
        except Exception as e:
            print(f"ç›‘æ§é”™è¯¯: {e}")
            
        await asyncio.sleep(10)  # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡

# å¯åŠ¨ç›‘æ§
asyncio.create_task(monitor_loop())
```

## éƒ¨ç½²æŒ‡å—

### æœ¬åœ°å¼€å‘ç¯å¢ƒ

```bash
# 1. å¯åŠ¨Redis
docker run -d --name redis -p 6379:6379 redis:7-alpine

# 2. è¿è¡Œåº”ç”¨
python your_app.py
```

### Docker éƒ¨ç½²

**Dockerfile:**
```dockerfile
FROM python:3.12-slim

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install -r requirements.txt

# å¤åˆ¶ä»£ç 
COPY . .

# å¯åŠ¨åº”ç”¨
CMD ["python", "main.py"]
```

**docker-compose.yml:**
```yaml
version: '3.8'
services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    
  app:
    build: .
    depends_on:
      - redis
    environment:
      - REDIS_URL=redis://redis:6379
      - MQ_MAX_WORKERS=10
    restart: unless-stopped
    
volumes:
  redis_data:
```

### ç”Ÿäº§ç¯å¢ƒé…ç½®

**Redis é…ç½®å»ºè®® (redis.conf):**
```conf
# å†…å­˜ç®¡ç†
maxmemory-policy allkeys-lru
maxmemory 2gb

# è¿æ¥ç®¡ç†
timeout 300
tcp-keepalive 300

# æŒä¹…åŒ–é…ç½®
save 900 1      # 900ç§’å†…è‡³å°‘1ä¸ªkeyå˜åŒ–æ—¶ä¿å­˜
save 300 10     # 300ç§’å†…è‡³å°‘10ä¸ªkeyå˜åŒ–æ—¶ä¿å­˜
save 60 10000   # 60ç§’å†…è‡³å°‘10000ä¸ªkeyå˜åŒ–æ—¶ä¿å­˜

# AOFæŒä¹…åŒ–
appendonly yes
appendfsync everysec

# æ€§èƒ½ä¼˜åŒ–
hz 10
dynamic-hz yes
```

**åº”ç”¨é…ç½®:**
```python
# ç”Ÿäº§ç¯å¢ƒé…ç½®
config = MQConfig(
    redis_url="redis://redis-cluster:6379",
    redis_password="your_secure_password",
    max_workers=20,
    task_queue_size=50,
    connection_pool_size=30,
    message_ttl=86400 * 7,  # 7å¤©
    processing_timeout=600,  # 10åˆ†é’Ÿ
    queue_prefix="prod",    # ç¯å¢ƒéš”ç¦»
)
```

### é«˜å¯ç”¨éƒ¨ç½²

**Redis Sentinel é…ç½®:**
```python
import redis.sentinel

# é…ç½®Sentinel
sentinels = [
    ('sentinel1', 26379),
    ('sentinel2', 26379), 
    ('sentinel3', 26379),
]

sentinel = redis.sentinel.Sentinel(sentinels, socket_timeout=0.1)

# å‘ç°ä¸»èŠ‚ç‚¹
redis_master = sentinel.master_for('mymaster', socket_timeout=0.1)

# è‡ªå®šä¹‰Redisè¿æ¥
config = MQConfig(redis_url="")  # ç•™ç©ºï¼Œä½¿ç”¨è‡ªå®šä¹‰è¿æ¥
mq = RedisMessageQueue(config)
mq.redis = redis_master  # ä½¿ç”¨Sentinelç®¡ç†çš„è¿æ¥
```

### ç›‘æ§å’Œå‘Šè­¦

**Prometheus æŒ‡æ ‡æš´éœ²:**
```python
from prometheus_client import start_http_server, Gauge, Counter

# å®šä¹‰æŒ‡æ ‡
queue_size = Gauge('mq_queue_size', 'Queue size', ['topic', 'status'])
messages_processed = Counter('mq_messages_processed_total', 'Messages processed', ['topic', 'status'])

async def export_metrics():
    """å¯¼å‡ºPrometheusæŒ‡æ ‡"""
    collector = MetricsCollector(redis=mq.redis)
    
    while True:
        metrics = await collector.collect_all_metrics(['order_created'])
        
        # æ›´æ–°PrometheusæŒ‡æ ‡
        queue_size.labels(topic='order_created', status='pending').set(
            metrics.get('queue.order_created.pending', 0)
        )
        queue_size.labels(topic='order_created', status='processing').set(
            metrics.get('queue.order_created.processing', 0)
        )
        
        await asyncio.sleep(30)

# å¯åŠ¨Prometheus HTTPæœåŠ¡å™¨
start_http_server(8000)
asyncio.create_task(export_metrics())
```

## æœ€ä½³å®è·µ

### 1. æ¶ˆæ¯è®¾è®¡

**âœ… æ¨èåšæ³•:**
```python
# æ¶ˆæ¯ç»“æ„æ¸…æ™°ï¼ŒåŒ…å«å¿…è¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
await mq.produce("order_created", {
    "order_id": "ORD_123456",
    "user_id": 789,
    "total_amount": 99.99,
    "currency": "USD",
    "timestamp": "2024-01-01T12:00:00Z",
    "metadata": {
        "source": "web",
        "version": "v1.0"
    }
})
```

**âŒ é¿å…åšæ³•:**
```python
# æ¶ˆæ¯è¿‡äºç®€å•ï¼Œç¼ºå°‘ä¸Šä¸‹æ–‡
await mq.produce("process", {"id": 123})

# æ¶ˆæ¯è¿‡äºå¤æ‚ï¼ŒåŒ…å«å¤§é‡æ•°æ®
await mq.produce("user_update", {
    "user": {...},  # åŒ…å«ç”¨æˆ·çš„æ‰€æœ‰ä¿¡æ¯
    "history": [...],  # åŒ…å«å®Œæ•´å†å²è®°å½•
    "related_data": {...}  # åŒ…å«å¤§é‡å…³è”æ•°æ®
})
```

### 2. é”™è¯¯å¤„ç†

**âœ… æ¨èåšæ³•:**
```python
async def handle_payment(payload: dict) -> None:
    try:
        order_id = payload["order_id"]
        amount = payload["amount"]
        
        # å‚æ•°éªŒè¯
        if not order_id or amount <= 0:
            raise ValueError(f"æ— æ•ˆçš„è®¢å•å‚æ•°: {payload}")
            
        # ä¸šåŠ¡é€»è¾‘
        result = await process_payment(order_id, amount)
        
        # è®°å½•æˆåŠŸæ—¥å¿—
        logger.info("æ”¯ä»˜å¤„ç†æˆåŠŸ", order_id=order_id, amount=amount)
        
    except ValueError as e:
        # å‚æ•°é”™è¯¯ï¼Œä¸é‡è¯•
        logger.error("æ”¯ä»˜å‚æ•°é”™è¯¯", error=str(e), payload=payload)
        raise  # é‡æ–°æŠ›å‡ºï¼Œè¿›å…¥æ­»ä¿¡é˜Ÿåˆ—
        
    except PaymentGatewayError as e:
        # å¤–éƒ¨æœåŠ¡é”™è¯¯ï¼Œå¯é‡è¯•
        logger.warning("æ”¯ä»˜ç½‘å…³é”™è¯¯", error=str(e), order_id=order_id)
        raise  # é‡æ–°æŠ›å‡ºï¼Œè§¦å‘é‡è¯•
        
    except Exception as e:
        # æœªçŸ¥é”™è¯¯
        logger.error("æ”¯ä»˜å¤„ç†å¤±è´¥", error=str(e), order_id=order_id)
        raise
```

### 3. å¹‚ç­‰æ€§å¤„ç†

```python
async def handle_order_created(payload: dict) -> None:
    order_id = payload["order_id"]
    
    # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†ï¼ˆå¹‚ç­‰æ€§ä¿æŠ¤ï¼‰
    if await is_order_processed(order_id):
        logger.info("è®¢å•å·²å¤„ç†ï¼Œè·³è¿‡", order_id=order_id)
        return
        
    try:
        # å¤„ç†è®¢å•
        await process_order(order_id)
        
        # æ ‡è®°ä¸ºå·²å¤„ç†
        await mark_order_processed(order_id)
        
    except Exception as e:
        logger.error("è®¢å•å¤„ç†å¤±è´¥", order_id=order_id, error=str(e))
        raise
```

### 4. æ€§èƒ½ä¼˜åŒ–

**å·¥ä½œåç¨‹æ•°è°ƒä¼˜:**
```python
import os
import multiprocessing

# æ ¹æ®CPUæ ¸å¿ƒæ•°å’ŒIOç‰¹æ€§è°ƒæ•´å·¥ä½œåç¨‹æ•°
cpu_count = multiprocessing.cpu_count()

config = MQConfig(
    # CPUå¯†é›†å‹ä»»åŠ¡ï¼šå·¥ä½œåç¨‹æ•° = CPUæ ¸å¿ƒæ•°
    max_workers=cpu_count if is_cpu_intensive else cpu_count * 2,
    
    # IOå¯†é›†å‹ä»»åŠ¡ï¼šå·¥ä½œåç¨‹æ•° = CPUæ ¸å¿ƒæ•° * 2-4
    # max_workers=cpu_count * 3,
    
    # ä»»åŠ¡é˜Ÿåˆ—å¤§å°åº”è¯¥å¤§äºå·¥ä½œåç¨‹æ•°
    task_queue_size=max_workers * 2,
)
```

**æ‰¹é‡å¤„ç†ä¼˜åŒ–:**
```python
async def handle_batch_emails(payload: dict) -> None:
    """æ‰¹é‡å¤„ç†é‚®ä»¶å‘é€"""
    email_list = payload["emails"]
    
    # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…å†…å­˜å ç”¨è¿‡å¤§
    batch_size = 10
    for i in range(0, len(email_list), batch_size):
        batch = email_list[i:i + batch_size]
        
        # å¹¶å‘å‘é€é‚®ä»¶
        tasks = [send_email(email) for email in batch]
        await asyncio.gather(*tasks, return_exceptions=True)
        
        # é¿å…è¿‡å¿«çš„è¯·æ±‚
        await asyncio.sleep(0.1)
```

### 5. å¤šç»„æ¶ˆè´¹çš„å®ç°æ–¹æ¡ˆ

ç”±äºç³»ç»Ÿä¸æ”¯æŒæ¶ˆè´¹è€…ç»„åŠŸèƒ½ï¼Œå¦‚éœ€å®ç°å¤šç»„æ¶ˆè´¹åŒä¸€æ¶ˆæ¯ï¼Œå»ºè®®é‡‡ç”¨ä»¥ä¸‹æ–¹æ¡ˆï¼š

**âœ… æ¨èåšæ³•:**
```python
# æ–¹æ¡ˆ1ï¼šåˆ›å»ºå¤šä¸ªtopicï¼Œå‘é€å¤šæ¬¡æ¶ˆæ¯
async def send_order_created(order_data: dict):
    """å‘é€è®¢å•åˆ›å»ºæ¶ˆæ¯åˆ°å¤šä¸ªå¤„ç†ç»„"""
    # å‘é€åˆ°ä¸åŒçš„å¤„ç†ç»„
    await mq.produce("order_created_payment", order_data)    # æ”¯ä»˜å¤„ç†ç»„
    await mq.produce("order_created_inventory", order_data)  # åº“å­˜å¤„ç†ç»„
    await mq.produce("order_created_analytics", order_data)  # åˆ†æå¤„ç†ç»„
    await mq.produce("order_created_notification", order_data) # é€šçŸ¥å¤„ç†ç»„

# æ³¨å†Œä¸åŒçš„å¤„ç†å™¨
@mq.register("order_created_payment")
async def handle_payment_processing(payload: dict):
    """å¤„ç†æ”¯ä»˜ç›¸å…³é€»è¾‘"""
    await process_payment(payload)

@mq.register("order_created_inventory")
async def handle_inventory_processing(payload: dict):
    """å¤„ç†åº“å­˜ç›¸å…³é€»è¾‘"""
    await update_inventory(payload)

@mq.register("order_created_analytics")
async def handle_analytics_processing(payload: dict):
    """å¤„ç†åˆ†æç›¸å…³é€»è¾‘"""
    await update_analytics(payload)

@mq.register("order_created_notification")
async def handle_notification_processing(payload: dict):
    """å¤„ç†é€šçŸ¥ç›¸å…³é€»è¾‘"""
    await send_notifications(payload)
```

**æ–¹æ¡ˆ2ï¼šä½¿ç”¨ç»Ÿä¸€çš„åˆ†å‘å™¨**
```python
# åˆ›å»ºä¸€ä¸ªåˆ†å‘å™¨topic
@mq.register("order_created")
async def order_dispatcher(payload: dict):
    """è®¢å•æ¶ˆæ¯åˆ†å‘å™¨"""
    order_id = payload["order_id"]
    
    # å¹¶å‘åˆ†å‘åˆ°å„ä¸ªå¤„ç†ç»„
    tasks = [
        mq.produce("order_payment", payload),
        mq.produce("order_inventory", payload),
        mq.produce("order_analytics", payload),
        mq.produce("order_notification", payload),
    ]
    
    try:
        await asyncio.gather(*tasks)
        logger.info("è®¢å•æ¶ˆæ¯åˆ†å‘æˆåŠŸ", order_id=order_id)
    except Exception as e:
        logger.error("è®¢å•æ¶ˆæ¯åˆ†å‘å¤±è´¥", order_id=order_id, error=str(e))
        raise
```

**æ–¹æ¡ˆ3ï¼šä½¿ç”¨topicå‘½åè§„èŒƒ**
```python
# ä½¿ç”¨ç»Ÿä¸€çš„å‘½åè§„èŒƒ
TOPIC_PATTERNS = {
    "order_created": [
        "order_created.payment",
        "order_created.inventory", 
        "order_created.analytics",
        "order_created.notification"
    ]
}

async def broadcast_message(base_topic: str, payload: dict):
    """å¹¿æ’­æ¶ˆæ¯åˆ°å¤šä¸ªç›¸å…³topic"""
    topics = TOPIC_PATTERNS.get(base_topic, [base_topic])
    
    tasks = [mq.produce(topic, payload) for topic in topics]
    await asyncio.gather(*tasks)
    
    logger.info("æ¶ˆæ¯å¹¿æ’­å®Œæˆ", base_topic=base_topic, target_topics=topics)

# ä½¿ç”¨ç¤ºä¾‹
await broadcast_message("order_created", order_data)
```

### 6. ç›‘æ§å’Œå‘Šè­¦

```python
async def setup_monitoring():
    """è®¾ç½®ç›‘æ§å’Œå‘Šè­¦"""
    collector = MetricsCollector(redis=mq.redis)
    
    while True:
        try:
            metrics = await collector.collect_all_metrics(["order_created"])
            
            # é˜Ÿåˆ—ç§¯å‹å‘Šè­¦
            pending = metrics.get('queue.order_created.pending', 0)
            if pending > 1000:
                await send_alert(f"é˜Ÿåˆ—ç§¯å‹ä¸¥é‡: {pending} æ¡æ¶ˆæ¯å¾…å¤„ç†")
                
            # æ­»ä¿¡é˜Ÿåˆ—å‘Šè­¦  
            dlq_count = metrics.get('queue.dlq.count', 0)
            if dlq_count > 50:
                await send_alert(f"æ­»ä¿¡é˜Ÿåˆ—æ¶ˆæ¯è¿‡å¤š: {dlq_count} æ¡")
                
            # å¤„ç†æ—¶é—´å‘Šè­¦
            avg_time = metrics.get('processing.order_created.avg_time', 0)
            if avg_time > 30000:  # 30ç§’
                await send_alert(f"æ¶ˆæ¯å¤„ç†æ—¶é—´è¿‡é•¿: {avg_time}ms")
                
        except Exception as e:
            logger.error("ç›‘æ§æ£€æŸ¥å¤±è´¥", error=str(e))
            
        await asyncio.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### Q1: æ¶ˆæ¯ä¸¢å¤±æ€ä¹ˆåŠï¼Ÿ

**ç—‡çŠ¶:** å‘é€çš„æ¶ˆæ¯æ²¡æœ‰è¢«å¤„ç†

**å¯èƒ½åŸå› :**
1. Redis è¿æ¥ä¸­æ–­
2. æ¶ˆè´¹è€…æ²¡æœ‰æ­£ç¡®å¯åŠ¨
3. æ¶ˆæ¯å¤„ç†å™¨æŠ›å‡ºå¼‚å¸¸ä½†æ²¡æœ‰æ­£ç¡®å¤„ç†

**è§£å†³æ–¹æ¡ˆ:**
```python
# 1. æ£€æŸ¥Redisè¿æ¥
try:
    await mq.redis.ping()
    print("Redisè¿æ¥æ­£å¸¸")
except Exception as e:
    print(f"Redisè¿æ¥å¤±è´¥: {e}")

# 2. æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦åœ¨é˜Ÿåˆ—ä¸­
pending_count = await mq.redis.llen("order_created:pending")
processing_count = await mq.redis.llen("order_created:processing")
print(f"å¾…å¤„ç†: {pending_count}, å¤„ç†ä¸­: {processing_count}")

# 3. æ£€æŸ¥æ­»ä¿¡é˜Ÿåˆ—
dlq_count = await mq.redis.llen("dlq:queue")
print(f"æ­»ä¿¡é˜Ÿåˆ—: {dlq_count}")
```

#### Q2: æ¶ˆæ¯å¤„ç†è¿‡æ…¢

**ç—‡çŠ¶:** é˜Ÿåˆ—ç§¯å‹ï¼Œæ¶ˆæ¯å¤„ç†ä¸åŠæ—¶

**å¯èƒ½åŸå› :**
1. å·¥ä½œåç¨‹æ•°ä¸è¶³
2. å¤„ç†å‡½æ•°æ‰§è¡Œæ—¶é—´è¿‡é•¿
3. Redisæ€§èƒ½ç“¶é¢ˆ

**è§£å†³æ–¹æ¡ˆ:**
```python
# 1. å¢åŠ å·¥ä½œåç¨‹æ•°
config = MQConfig(max_workers=20)  # å¢åŠ åˆ°20ä¸ª

# 2. ä¼˜åŒ–å¤„ç†å‡½æ•°
async def optimized_handler(payload: dict) -> None:
    # ä½¿ç”¨å¼‚æ­¥IO
    async with aiohttp.ClientSession() as session:
        response = await session.post(url, json=payload)
    
    # é¿å…é˜»å¡æ“ä½œ
    await asyncio.to_thread(blocking_operation, payload)

# 3. ç›‘æ§å¤„ç†æ—¶é—´
import time

async def timed_handler(payload: dict) -> None:
    start_time = time.time()
    try:
        await actual_handler(payload)
    finally:
        processing_time = time.time() - start_time
        if processing_time > 5:  # å¤„ç†æ—¶é—´è¶…è¿‡5ç§’
            logger.warning("å¤„ç†æ—¶é—´è¿‡é•¿", time=processing_time, payload=payload)
```

#### Q3: å†…å­˜ä½¿ç”¨è¿‡é«˜

**ç—‡çŠ¶:** åº”ç”¨å†…å­˜æŒç»­å¢é•¿

**å¯èƒ½åŸå› :**
1. æœ¬åœ°é˜Ÿåˆ—ç§¯å‹
2. æ¶ˆæ¯å¯¹è±¡æ²¡æœ‰æ­£ç¡®é‡Šæ”¾
3. Redisè¿æ¥æ± è¿‡å¤§

**è§£å†³æ–¹æ¡ˆ:**
```python
# 1. è°ƒæ•´é˜Ÿåˆ—å¤§å°
config = MQConfig(
    task_queue_size=10,  # å‡å°‘æœ¬åœ°é˜Ÿåˆ—å¤§å°
    connection_pool_size=10,  # å‡å°‘è¿æ¥æ± å¤§å°
)

# 2. ç›‘æ§å†…å­˜ä½¿ç”¨
import psutil
import gc

async def memory_monitor():
    while True:
        process = psutil.Process()
        memory_mb = process.memory_info().rss / 1024 / 1024
        
        if memory_mb > 500:  # å†…å­˜è¶…è¿‡500MB
            logger.warning("å†…å­˜ä½¿ç”¨è¿‡é«˜", memory_mb=memory_mb)
            gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
            
        await asyncio.sleep(60)
```

#### Q4: Redis è¿æ¥é”™è¯¯

**ç—‡çŠ¶:** `ConnectionError`, `TimeoutError`

**è§£å†³æ–¹æ¡ˆ:**

```python
# 1. æ£€æŸ¥Redisé…ç½®
config = MQConfig(
    redis_url="redis://localhost:6379",
    connection_pool_size=20,
    # æ·»åŠ è¿æ¥é‡è¯•
)

# 2. å®ç°è¿æ¥é‡è¯•
async def create_redis_with_retry(config: MQConfig, max_retries: int = 3):
    for attempt in range(max_retries):
        try:
            redis = aioredis.from_url(config.redis_host)
            await redis.ping()
            return redis
        except Exception as e:
            if attempt == max_retries - 1:
                raise
            logger.warning(f"Redisè¿æ¥å¤±è´¥ï¼Œé‡è¯•ä¸­ ({attempt + 1}/{max_retries})")
            await asyncio.sleep(2 ** attempt)
```

### æ€§èƒ½è¯Šæ–­

#### å»¶è¿Ÿåˆ†æ

```python
import time
from collections import defaultdict

class PerformanceAnalyzer:
    def __init__(self):
        self.metrics = defaultdict(list)
    
    async def analyze_handler(self, handler_name: str, handler_func):
        """åˆ†æå¤„ç†å™¨æ€§èƒ½"""
        async def wrapped_handler(payload: dict):
            start_time = time.time()
            try:
                result = await handler_func(payload)
                return result
            finally:
                end_time = time.time()
                processing_time = (end_time - start_time) * 1000  # æ¯«ç§’
                self.metrics[handler_name].append(processing_time)
                
                # å®šæœŸè¾“å‡ºç»Ÿè®¡ä¿¡æ¯
                if len(self.metrics[handler_name]) % 100 == 0:
                    times = self.metrics[handler_name]
                    avg_time = sum(times) / len(times)
                    max_time = max(times)
                    min_time = min(times)
                    
                    print(f"{handler_name} æ€§èƒ½ç»Ÿè®¡ (æœ€è¿‘100æ¬¡):")
                    print(f"  å¹³å‡æ—¶é—´: {avg_time:.2f}ms")
                    print(f"  æœ€å¤§æ—¶é—´: {max_time:.2f}ms") 
                    print(f"  æœ€å°æ—¶é—´: {min_time:.2f}ms")
        
        return wrapped_handler

# ä½¿ç”¨ç¤ºä¾‹
analyzer = PerformanceAnalyzer()

@mq.register("order_created")
async def handle_order(payload: dict):
    # å¤„ç†é€»è¾‘
    await process_order(payload)

# åŒ…è£…å¤„ç†å™¨è¿›è¡Œæ€§èƒ½åˆ†æ
mq.handlers["order_created"] = await analyzer.analyze_handler(
    "order_created", 
    handle_order
)
```

