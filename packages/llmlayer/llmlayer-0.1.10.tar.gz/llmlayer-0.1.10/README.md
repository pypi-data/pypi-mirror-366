# LLMLayerÂ PythonÂ SDK

![PyPI](https://img.shields.io/pypi/v/llmlayer?color=blue) ![Python](https://img.shields.io/pypi/pyversions/llmlayer.svg)

> **Search â€“ Reason â€“ Cite** with one function call.
>
> This library is the *official* Python interface for the [LLMLayer SearchÂ &Â Answer API](https://llmlayer.ai).

---

## âœ¨Â Features

| Â                           | Â                                                                                   |
| -------------------------- | ---------------------------------------------------------------------------------- |
| ðŸ—‚Â **Multiâ€‘provider**      | Seamlessly speak to OpenAI, DeepSeek, or Groq models âœ¨   |
| ðŸ”„Â **SyncÂ &Â Async**        | Choose the style that fits your stack â€“ both are firstâ€‘class citizens              |
| â±Â **Streaming**            | Get partial chunks via Serverâ€‘SentÂ Events; perfect for chatÂ UIs                    |
| ðŸ›¡Â **Elegant error model** | `InvalidRequest`, `ProviderError`, `RateLimitError`, â€¦ â€“ catch exactly what you need |
| ðŸ”ŒÂ **Dependencyâ€‘light**    | Only `httpx` and `pydantic` at runtime                                             |

---

\##Â TableÂ ofÂ Contents

* [Installation](#installation)
* [QuickÂ Start](#quick-start)

  * [SyncÂ call](#sync-call)
  * [AsyncÂ call](#async-call)
  * [Streaming](#streaming)
* [Configuration](#configuration)

  * [EnvironmentÂ Variables](#environment-variables)
* [APIÂ Reference](#api-reference)

  * [`LLMLayerClient`](#llmlayerclient)
  * [DataÂ Models](#data-models)
* [ErrorÂ Handling](#error-handling)
* [AdvancedÂ Usage](#advanced-usage)

  * [Reâ€‘using `httpx`Â clients](#re-using-httpx-clients)
  * [ProxiesÂ &Â Timeouts](#proxies--timeouts)
* [Development](#development)
* [License](#license)

---

## Installation

```bash
pip install llmlayer
```

> **PythonÂ â‰¥Â 3.9** is required.

---

## Quick Start

### Â SyncÂ call

```python
from llmlayer import LLMLayerClient

client = LLMLayerClient(
    api_key="<LLMLAYER_API_KEY>",          # Bearer token
)

resp = client.search(
    query="Why is the sky blue?",
    model="openai/gpt-4.1-mini",
    return_sources=True,
)

print(resp.llm_response)
for src in resp.sources:
    print(src["title"], src["link"])
```

### Â AsyncÂ call

```python
import asyncio
from llmlayer import LLMLayerClient

async def main():
    client = LLMLayerClient(
        api_key="<LLMLAYER_KEY>",
    )
    resp = await client.asearch(
        query="List three applications of quantum tunnelling",
        model="groq/kimi-k2",
    )
    print(resp.llm_response)

asyncio.run(main())
```

### Â Streaming

```python
from llmlayer import LLMLayerClient

client = LLMLayerClient(
    api_key="LLMLAYER_API_KEY",
)

for event in client.search_stream(
    query="Explain brown dwarfs in two paragraphs",
    model="openai/o3",
    return_sources=True,
):
    if event["type"] == "llm":
        print(event["content"], end="", flush=True)
    elif event["type"] == "sources":
        print("\nSources:", event["data"])
    elif event["type"] == "done":
        print(f"\nâœ“ finished in {event['response_time']}Â s")
```

---

## Â Configuration

### Â EnvironmentÂ Variables

| Variable                | Purpose                                                 | Fallback if unset                       |
|-------------------------|---------------------------------------------------------| --------------------------------------- |
| `LLMLAYER_API_KEY`      | Bearer token sent as `Authorization:Â BearerÂ â€¦`          | *required*                              |
| `LLMLAYER_PROVIDER_KEY`    | OPTIONAL : Providerâ€‘specific key, e.g. `OPENAI_API_KEY` | *required unless passed to constructor* |

All constructor args override envÂ vars.

---

## Â APIÂ Reference

### Â `LLMLayerClient`

| Parameter      | Type                     | Default  | Description                       |
| -------------- | ------------------------ |----------| --------------------------------- |
| `api_key`      | `str`                    | Â â€”Â       | LLMLayer bearer token (mandatory) |
| `timeout`      | `float \| httpx.Timeout` | `60.0`   | Request timeout                   |
| `client`       | `httpx.ClientÂ \|Â None`   | `None`   | Inject your own `httpx` client    |

#### Â Methods

| Method                     | Description                                |
| -------------------------- | ------------------------------------------ |
| `search(**params)`         | Blocking call â†’ `SimplifiedSearchResponse` |
| `search_stream(**params)`  | Generator yielding SSE events              |
| `asearch(**params)`        | `async` version of `search`                |
| `asearch_stream(**params)` | `async` generator                          |

---


### Â Search parameters

Below keys map 1â€‘toâ€‘1 to the backendâ€™s `SearchRequest` schema.

| Name                  | Type                             | Default     | Notes                                                                                                    |
|-----------------------|----------------------------------|-------------|----------------------------------------------------------------------------------------------------------|
| `query`               | `str`                            | â€”           | User question / search prompt                                                                            |
| `model`               | `str`                            | â€”           | Provider model name (`gpt-4o-mini`, `claudeâ€‘3â€‘sonnetâ€‘20240229`, â€¦)                                       |
| `date_context`        | `str?`                           | `None`      | Inject a date string the prompt can reference                                                            |
| `location`            | `str`                            | `"us"`      | Geographical search bias                                                                                 |
| `system_prompt`       | `str?`                           | Â â€”Â          | Override LLMLayerâ€™s default prompt                                                                       |
| `provider_key`        | `str?`                           | `None`      | your choosen model provider api key, if you want to be charged directly by your provider for model usage |
| `response_language`   | `str`                            | `"auto"`    | `"auto"` to detect user language                                                                         |
| `answer_type`         | `"markdown"Â \|Â "html"Â \|Â "json"` | `markdown`  | Output format                                                                                            |
| `search_type`         | `"general"Â \|Â "news"`            | `general`   | Vertical search bias                                                                                     |
| `json_schema`         | `str?`                           | Â â€”Â          | Required when `answer_type = json` json schema the response should follow                                |
| `citations`           | `bool`                           | `False`     | Embed `[n]` citations into answer                                                                        |
| `return_sources`      | `bool`                           | `False`     | Include `sources` in response                                                                            |
| `return_images`       | `bool`                           | `False`     | Include `images` (if available)                                                                          |
| `date_filter`         | `str`                            | `"anytime"` | `hour`, `day`, `week`, `month`, `year`                                                                   |
| `max_tokens`          | `int`                            | `1500`      | LLM max tokens                                                                                           |
| `temperature`         | `float`                          | `0.7`       | Adjust creativity                                                                                        |
| `domain_filter`       | `List[str]?`                     | Â â€”Â          | list of domains `["nytimes.com","-wikipedia.org]` `-` to exclude a domain from the search                |
| `max_queries`         | `int`                            | `1`         | How many search queries LLMLayer should generate. each query will cost 0,007$                            |
| `search_context_size` | `str?`                           | `medium`    | values : `low` `medium` `high`                                                                            |


---

### Â DataÂ Models

```python
from llmlayer.models import SearchRequest, SimplifiedSearchResponse
```

Both are `pydantic.BaseModel` subclasses â€“ perfect for validation, FastAPI, or serialization.

---

## Â ErrorÂ Handling

All exceptions inherit from `llmlayer.exceptions.LLMLayerError`.

| Class                 | RaisedÂ When                                    |
| --------------------- | ---------------------------------------------- |
| `InvalidRequest`      | Bad request parameters (400)                   |
| `AuthenticationError` | Missing/invalid LLMLayer or provider key (401) |
| `RateLimitError`      | Provider 429Â errors                            |
| `InternalServerError` | LLMLayerÂ 5xx                                   |

Example:

```python
from llmlayer.exceptions import RateLimitError

try:
    resp = client.search(...)
except RateLimitError:
    backoff_and_retry()
```

---

## Â AdvancedÂ Usage

### Â Reâ€‘using `httpx` clients

```python
import httpx
from llmlayer import LLMLayerClient

shared = httpx.Client(http2=True, timeout=60)
client = LLMLayerClient(
    api_key="...",
    client=shared,
)
```

### Â ProxiesÂ &Â Timeouts

```python
transport = httpx.HTTPTransport(proxy="https://proxy.corp:3128")
custom = httpx.Client(timeout=10, transport=transport)
client = LLMLayerClient(..., client=custom)
```

---



## Â License

MIT Â©Â 2025Â LLMLayerÂ Inc.
