# -*- coding: utf-8 -*-
"""Processor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mNuWsNHT4SNtLZi9xbpT-5hrJii9BdXM
"""

import pandas as pd
import numpy as np
import os
import json
import datetime
import joblib

import time
import re

import pandas as pd
import numpy as np
from collections import defaultdict

from collections import defaultdict
import numpy as np

from .Wave import Wave

#from MLAMA Package 2025 Data.ipynb

#processor.py updated

def extract_evaluation_from_dict(result_dict, models, metric="MAPE"):
    """
    Extracts evaluation metrics (e.g., MAPE, MSE) from a nested dictionary of model evaluation results.

    Parameters:
        result_dict (dict): Dictionary where keys are week start dates and values are DataFrames
                            containing prediction results and evaluation metrics.
        models (dict): Dictionary of models with their names as keys.
        metric (str): Evaluation metric to extract (e.g., 'MAPE' or 'MSE').

    Returns:
        pd.DataFrame: Consolidated DataFrame with columns: 'Week', '<model1>_<metric>', ..., '<modelN>_<metric>'
    """
    rows = []

    for week_start, df in result_dict.items():
        for i, row in df.iterrows():
            week = pd.to_datetime(row['Weeks'])
            eval_row = {'Week': week}

            for model_name in models.keys():
                metric_key = f"{model_name}_{metric}"
                if metric_key in row:
                    metric_val = row[metric_key]
                    # If value is inside a list or Series, extract the scalar
                    if hasattr(metric_val, 'values'):
                        metric_val = metric_val.values[0]
                    elif isinstance(metric_val, list):
                        metric_val = metric_val[0]
                    eval_row[metric_key] = metric_val
                else:
                    eval_row[metric_key] = None  # In case key doesn't exist

            rows.append(eval_row)

    result_df = pd.DataFrame(rows)
    result_df.sort_values(by='Week', inplace=True)
    result_df.reset_index(drop=True, inplace=True)
    return result_df



def extract_predictions_from_dict(result_dict, target_length, models):
    """
    Extracts predictions for a specific forecast length from a dictionary of model results.

    This function parses a result dictionary, which contains forecast outputs per week,
    and extracts prediction values for a specified prediction length. It supports both
    required and optional models and gracefully handles different data formats.

    Parameters:
    -----------
    result_dict : dict
        Dictionary where each key is a forecast start week (as string or datetime), and each
        value is a DataFrame with model outputs, including columns like 'Length', 'Shift',
        'Observed', and individual model predictions.

    target_length : int
        The forecast horizon (number of weeks ahead) for which predictions are to be extracted.

    models : dict
        Dictionary of model objects with names as keys. These are considered optional models.
        'MLAMA' and 'ARIMA' are always expected as required models.

    Returns:
    --------
    pd.DataFrame
        A combined DataFrame containing one row per prediction per model for the specified
        forecast length. Columns include:
            - 'CurrentWeek': the week forecast started from
            - 'Week': the target forecasted week
            - 'Shift': the adjustment applied during forecasting
            - 'Length': the prediction length (always equals `target_length`)
            - 'Observed': the true observed value
            - <model names>: prediction outputs for each model
    """
    rows = []

    required_models = ['MLAMA', 'ARIMA']
    optional_models = list(models.keys()) if models else []
    all_models = list(dict.fromkeys(required_models + optional_models))

    for week_start, df in result_dict.items():
        filtered_df = df[df['Length'] == target_length]

        for _, row in filtered_df.iterrows():
            prediction_row = {
                'CurrentWeek': pd.to_datetime(week_start),
                'Week': pd.to_datetime(row['Weeks']),
                'Shift': row['Shift'],
                'Length': row['Length'],
                'Observed': row['Observed']
            }

            for model_name in all_models:
                value = row.get(model_name, None)
                if isinstance(value, (list, np.ndarray)):
                    prediction_row[model_name] = value[0] if len(value) > 0 else None
                elif hasattr(value, 'values') and len(value.values) == 1:
                    prediction_row[model_name] = value.values[0]
                else:
                    prediction_row[model_name] = value

            rows.append(prediction_row)

    result_df = pd.DataFrame(rows)
    result_df.sort_values(by='Week', inplace=True)
    result_df.reset_index(drop=True, inplace=True)
    return result_df

def extract_all_predictions_from_dict(result_dict, models):
  """
  Extracts weekly predictions for all prediction lengths from a nested dictionary of results.

  Parameters:
      result_dict (dict): Dictionary where keys are current week and values are DataFrames.
      models (dict): Dictionary of model objects with names as keys.

  Returns:
      pd.DataFrame: Combined DataFrame for all prediction lengths across all models.
  """
  all_lengths = set()
  for df in result_dict.values():
      all_lengths.update(df['Length'].unique())

  all_dfs = []
  for length in sorted(all_lengths):
      df = extract_predictions_from_dict(result_dict, target_length=length, models=models)
      all_dfs.append(df)

  return pd.concat(all_dfs, axis=0).reset_index(drop=True)

def format_stan_optimal_weights(weights_dict):
    """
    Converts the stan_optimal_weights dictionary into a pandas DataFrame.

    Args:
        weights_dict (dict): Dictionary where keys are 'Shift Length' strings
                             and values are (models list, weights array).

    Returns:
        pd.DataFrame: A formatted DataFrame with columns ['Shift', 'Length', model names...].
    """
    data = []
    for key, (models, weights) in weights_dict.items():
        shift, length = key.split()
        row = {'Shift': shift, 'Length': length}
        row.update({model: weight for model, weight in zip(models, weights)})
        data.append(row)

    df = pd.DataFrame(data)
    return df

#from MLAMA Package 2025 Data.ipynb
def extract_shift_from_key(key):
    """
    Extracts the shift value from a key string like 'wave 3 n5_t4_shift 4'.

    Parameters:
        key (str): The key string.

    Returns:
        int: The extracted shift value.
    """
    match = re.search(r'shift\s*(\d+)', key)
    if match:
        return int(match.group(1))
    else:
        print(f"Error extracting shift from key: {key} - 'shift' pattern not found. Returned 0.")
        return 0

def generate_default_shift_matrix(waves, shifts):
    """
    Generates a default wave_start_shift_matrix for auto-generated waves.

    This matrix is used to adjust the start date of each wave when applying different
    trend adjustment steps (shifts). For auto-generated waves, no manual date adjustment
    is needed, so all shift values are set to 0.

    Parameters:
    ----------
    waves : list
        A list of Wave objects. Each object must have a `waveID` attribute.

    shifts : list of int
        A list of integers representing the shift values (e.g., [0, 1, 2, 3]).

    Returns:
    -------
    dict
        A dictionary with string wave IDs as keys. Each wave ID maps to another
        dictionary with each shift as a key and `0` as the default value.

        Example output:
        {
            '1': {0: 0, 1: 0, 2: 0},
            '2': {0: 0, 1: 0, 2: 0},
            '3': {0: 0, 1: 0, 2: 0}
        }
    """
    return {str(w.waveID): dict.fromkeys(shifts, 0) for w in waves}

# needed in visualization
# MLAMA Package 2025 Data.ipynb
def extract_wave_id(key):
    """
    Extracts the integer wave ID from a key like 'wave 1_shift 3'.
    """
    return int(key.split('_')[0].split()[1])

#used in Visulaization
def safe_to_datetime(x):
    if isinstance(x, (str, bytes)):
        return datetime.datetime.fromisoformat(x)
    return pd.to_datetime(x)  # handles datetime, pandas Timestamp, etc.

#from MLAMA Package 2025 Data.ipynb
def split_data_into_waves(df, K, F, R, O, trend_adjustment_steps=[0]):
    """
    Split the dataframe into K waves (train + test) with specified parameters.

    Args:
        df (pd.DataFrame): The full time-indexed dataframe. (check there is no skipping dates, if there are throw error)
        K (int): Number of waves/chunks.
        F (int): Forecast horizon (test prediction length).
        R (int): Responsiveness horizon (additional test length).
        O (int): Overlap amount (in time steps).
        trend_adjustment_steps (list): List of steps for trend adjustment (e.g., [1, 2, 3]). Needed in the previous version

    Returns:
        List[Wave]: A list of Wave objects.
    """
    total_length = len(df)
    test_size = F + R
    max_shift = max(trend_adjustment_steps)

    # Calculate total data needed for (K-1) waves with exact train sizes, last wave may absorb remainder
    total_test_data = K * test_size
    remaining_data = total_length - total_test_data + (K - 1) * O

    if remaining_data < K:
        raise ValueError("Not enough data to create K waves with the given parameters.")

    # Train size per chunk (equal split except the last)
    base_train_size = remaining_data // K
    extra_train = remaining_data % K  # Add to the last wave

    waves = []
    current_index = 0

    for i in range(K):
        waveID = i + 1

        train_size = base_train_size + (extra_train if i == K - 1 else 0)
        train_start_idx = current_index
        train_end_idx = train_start_idx + train_size

        test_start_idx = train_end_idx
        test_end_idx = test_start_idx + test_size

        if test_end_idx > len(df):
            raise IndexError("Test index out of bounds. Reduce K or adjust F/R.")

        # Define actual dates
        train_start_date = pd.to_datetime(df.index[train_start_idx]).strftime('%Y-%m-%d')
        train_end_date   = pd.to_datetime(df.index[train_end_idx - 1]).strftime('%Y-%m-%d')
        test_start_date  = pd.to_datetime(df.index[test_start_idx]).strftime('%Y-%m-%d')
        test_end_date    = pd.to_datetime(df.index[test_end_idx - 1]).strftime('%Y-%m-%d')


        wave_start_date = train_start_date
        wave_end_date = test_end_date

        # Include sliding room for trend adjustment
        wave_df = df.copy()

        wave = Wave(
            waveID=waveID,
            startDate=wave_start_date,
            endDate=wave_end_date,
            trainStartDate=train_start_date,
            trainEndDate=train_end_date,
            testStartDate=test_start_date,
            testEndDate=test_end_date,
            trend_adjustment_steps=trend_adjustment_steps,
            df=wave_df
        )
        waves.append(wave)

        # Calculate next wave's starting index with overlap
        current_index = test_end_idx - O

    return waves

#new from MLAMA 2025 forecast
def split_data_history_now_casting(df, now_cast_size):
    """
    Splits a DataFrame into historical and now-casting parts.

    Parameters:
        df (pd.DataFrame): The full time-indexed DataFrame.
        now_cast_size (int): The number of rows to include in the now-casting DataFrame.

    Returns:
        tuple: (history_df, now_cast_df)
            - history_df: DataFrame excluding the last `now_cast_size` rows.
            - now_cast_df: DataFrame containing only the last `now_cast_size` rows.
    """
    if now_cast_size < 0:
        raise ValueError("now_cast_size must be non-negative.")
    if now_cast_size > len(df):
        raise ValueError("now_cast_size cannot be greater than the length of the DataFrame.")

    history_df = df.iloc[:-now_cast_size] if now_cast_size > 0 else df.copy()
    now_cast_df = df.iloc[-now_cast_size:] if now_cast_size > 0 else df.iloc[0:0]

    return history_df, now_cast_df

#Visualization.py previous
def find_model_week_values_sum_shift(df, shift, weeks):
  """
  find values from the dataframe given shift for all weeks

  Args:
    df: the dataframe for a specific model
    shift: specific shift integer
    weeks: The list containing week values
  Returns:
    all_week_listTTT: Transposed according to week
    values: sum value for each week
  """
  #print(df[0])
  all_week_listTT = pd.DataFrame()
  values = pd.DataFrame()
  all_week_list = df[shift]#correct
  all_week_listT = all_week_list.T
  for week in weeks:
    all_week_listTT[str(week+1)] = all_week_listT.apply(lambda x: x[0][week], axis = 1)#to get the weeks as column names
  all_week_listTTT = all_week_listTT.T
  values = all_week_listTT.sum()
  return all_week_listTTT,values

# Generalized previous Visualization.py
def create_MAPE_df(WAVES, predictions, trend_adjustment_steps, model_evaluation_dictionary, inFoldername_pre, task_name, filename, models):
    """
    Constructs a dictionary of dataframes containing weekly predicted and observed values for different models,
    intended for visualizing and comparing MAPE (Mean Absolute Percentage Error) across waves, shifts, and prediction lengths.

    Parameters:
        WAVES (list): A list of Wave objects, each containing a unique `waveID` attribute.
        predictions (list): A list of integers representing different prediction lengths (e.g., 1-week, 2-week forecasts).
        trend_adjustment_steps (list): A list of integers indicating the number of steps used for trend adjustments (e.g., shifts).
        model_evaluation_dictionary (dict): A nested dictionary containing model evaluation metrics and predictions.
                                            Keys follow the format: 'wave {ID} shift {shift} prediction_length {length}'.
        inFoldername_pre (str): Prefix for the input folder name (currently unused in function but retained for consistency).
        task_name (str): Name of the forecasting task (currently unused in function but retained for consistency).
        filename (str): Name of the output file or configuration (currently unused in function but retained for consistency).
        models (dict): Dictionary mapping model names (str) to their corresponding class or object. Used for ordering and reference.

    Returns:
        dict: A dictionary where each key corresponds to a specific combination of wave, shift, and prediction length,
              and each value is a merged DataFrame of predictions and observed values for all models across weeks.
    """
    visualization_data_dictionary = {}
    for WAVE in WAVES:
        waveID = WAVE.waveID
        for prediction_length in predictions:
            for shift in trend_adjustment_steps:
                # print("prediction_length: ", prediction_length, "shift: ", shift, "Wave: ", waveID)
                wave_shift_prediction_length_key = f'wave {waveID} shift {shift} prediction_length {prediction_length}'

                metric_row = model_evaluation_dictionary[wave_shift_prediction_length_key]
                print(metric_row)
                model_dataframes = {}
                for model_name, model_class in models.items():

                    model_data = metric_row[model_name]['Fit_Pred']

                    model_mape = metric_row[model_name]['MAPE']#.values[0]
                    week_values = metric_row[model_name]['Weeks']#.values[0]
                    observed_values = metric_row[model_name]['Observed']#.values[0]

                    model_data['week'] = week_values
                    model_data['Observed'] = observed_values
                    model_data.columns = [model_name, 'week', 'Observed']
                    #print(model_data)
                    model_dataframes[model_name] = model_data

                # Merge the dataframes for all models
                task = model_dataframes[list(models.keys())[0]]
                for model_name in list(models.keys())[1:]:
                    task = pd.merge(task, model_dataframes[model_name], on=['week', 'Observed'], how='inner')

                task = task.set_index('week')

                visualization_data_dictionary[wave_shift_prediction_length_key] = task

    return visualization_data_dictionary

def process_metrics_df(WAVE, frame, recent_week_count):
    """
    Processes and formats a list of metric DataFrames for a given wave, returning a cleaned DataFrame
    containing data for the most recent weeks.

    Parameters:
        WAVE (object): The Wave object associated with the metrics (currently unused in function but retained for compatibility).
        frame (list of pd.DataFrame): A list of DataFrames containing weekly metrics (e.g., from different models or evaluations).
        recent_week_count (int): The number of most recent weeks to include in the output DataFrame.

    Returns:
        pd.DataFrame: A cleaned DataFrame containing the concatenated metrics for the most recent weeks,
                      with negative values replaced and 'week' set as the index.
    """
    # Concatenate the frames along the columns and get the tail based on recent_week_count
    metrics_df = pd.concat(frame, axis=1).tail(recent_week_count)

    # Reset the index to ensure it's a clean, sequential index
    metrics_df.reset_index(drop=True, inplace=True)

    # Set the 'Weeks' column as the index
    # Assuming 'Weeks' is one of the columns in the DataFrame
    metrics_df.set_index('week', inplace=True)

    # Replace negative values in the DataFrame
    metrics_df = metrics_df.map(replace_negatives)

    return metrics_df

def replace_negatives(x):
    """
    Replaces negative numeric values with zero.

    Parameters:
        x (int or float):
            A numeric value to be checked.

    Returns:
        int or float:
            Returns 0 if the input value is negative; otherwise, returns the input value unchanged.

    Notes:
        - This function is typically used with `DataFrame.map()` or `DataFrame.applymap()`
          to sanitize datasets by removing negative values.
    """
    if x < 0:
        return 0
    else:
        return x

#MLAMA Package 2025 Data.ipynb
# dictionary to dataframe function
def create_prediction_df(WAVES, predictions, trend_adjustment_steps, models, model_evaluation_dictionary, inFoldername_pre, task_name, filename):
    """
    Creates a combined DataFrame and dictionary of model predictions across multiple waves, shifts, and prediction lengths.
    Extracts and aligns weekly predicted and observed values from a nested evaluation dictionary, merges model outputs,
    and writes the final combined DataFrame to a CSV file.

    Parameters:
        WAVES (list): A list of Wave objects, each containing a unique waveID.
        predictions (list of int): A list of prediction lengths (e.g., [4, 8]) to iterate over.
        trend_adjustment_steps (list of int): A list of shift values representing different trend adjustments.
        models (dict): A dictionary of model names mapped to model class or configuration (used for column names).
        model_evaluation_dictionary (dict): A nested dictionary containing model outputs keyed by
                                            "wave {waveID} shift {shift} prediction_length {prediction_length}".
        inFoldername_pre (str): Folder path or prefix for saving the CSV file.
        task_name (str): Name of the task to include in the CSV filename.
        filename (str): Suffix to include in the final output CSV filename.

    Returns:
        tuple:
            - pd.DataFrame: A combined DataFrame containing all valid model predictions with associated metadata.
            - dict: A dictionary of merged DataFrames for each wave-shift-length combination.

    Side Effects:
        Writes the combined DataFrame to a CSV file named
        "<inFoldername_pre><task_name>_model_predictions_<filename>".
    """
    base_columns = ['Wave', 'Shift', 'Length', 'Observed']

    # Dynamically create visualization_columns
    visualization_columns = base_columns + list(models.keys())
    visualization_data = pd.DataFrame(columns=visualization_columns)
    visualization_data_dictionary = {}

    for WAVE in WAVES:
        waveID = WAVE.waveID
        for prediction_length in predictions:
            for shift in trend_adjustment_steps:

                wave_shift_prediction_length_key = f'wave {waveID} shift {shift} prediction_length {prediction_length}'

                metric_row = model_evaluation_dictionary[wave_shift_prediction_length_key]

                # Initialize list for model DataFrames
                model_dfs = []
                for model_name in models:
                    #print(metric_row[model_name])
                    model_data = metric_row[model_name]['Predictions']
                    week_values = metric_row[model_name]['Weeks']
                    observed_values = metric_row[model_name]['Observed']

                    # === HANDLE None VALUES ===
                    if model_data is None or week_values is None or observed_values is None:
                        print(f"Skipping model '{model_name}' for wave {waveID} shift {shift} prediction length {prediction_length} due to missing data.")
                        continue  # Skip this model

                    # === USE LAST N DATA POINTS ===
                    # === USE LAST N DATA POINTS WITHOUT CONVERTING TO pd.Series ===
                    week_values = week_values.iloc[-prediction_length:].reset_index(drop=True)#changed
                    observed_values = observed_values.iloc[-prediction_length:].reset_index(drop=True)#changed


                    model_df = pd.DataFrame(model_data).reset_index(drop=True)
                    model_df['Weeks'] = week_values
                    model_df['Observed'] = observed_values.astype(np.int64)

                    model_df.columns = [model_name, 'Weeks', 'Observed']

                    model_dfs.append(model_df)

                # Skip merging if less than two valid models
                if len(model_dfs) < 1:
                    print(f"No valid models found for wave {waveID}, shift {shift}, prediction_length {prediction_length}.")
                    continue  # <-- ADDED

                for i in range(len(model_dfs)):
                    model_dfs[i]['Observed'] = model_dfs[i]['Observed'].astype('int64')
                    model_dfs[i]['Weeks'] = pd.to_datetime(model_dfs[i]['Weeks'])

                result = model_dfs[0]
                for df in model_dfs[1:]:
                    result = pd.merge(result, df, on=['Weeks', 'Observed'], how='inner')

                # Add metadata columns
                result['Wave'] = waveID
                result['Shift'] = shift
                result['Length'] = prediction_length


                # Store original dtypes before concatenation
                original_dtypes = result.dtypes

                # Concatenate while preserving dtypes
                visualization_data_dictionary[wave_shift_prediction_length_key] = result
                visualization_data = pd.concat([visualization_data, result])

                # Restore original data types
                for col, dtype in original_dtypes.items():
                    if col in visualization_data.columns:
                        try:
                            visualization_data[col] = visualization_data[col].astype(dtype)
                        except Exception as e:
                            print(f"Warning: Could not convert column '{col}' to original dtype {dtype} due to {e}")

    # Save final visualization data
    visualization_data.to_csv(inFoldername_pre + task_name + '_model_predictions_' + filename)
    return visualization_data, visualization_data_dictionary

def add_time_features(df):#new
    """
    Adds seasonal time-based features to a time series DataFrame with datetime index.

    Assumes:
        - df.index is datetime-like (e.g., pd.to_datetime has been applied)
        - df contains a column 'weekcase'

    Adds:
        - Sin_Time1, Cos_Time1: short-term seasonality (e.g., annual)
        - Sin_Time2, Cos_Time2: longer-term seasonality (e.g., biannual)
    """

    # Convert datetime index to numerical representation (days since start)
    time_numeric = (df.index - df.index[0]).days

    # Example period definitions (adjust based on your seasonality understanding)
    period1 = 365.25      # yearly seasonality
    period2 = 365.25 / 2  # half-year seasonality

    # Compute features
    df["Sin_Time1"] = np.sin(2 * np.pi * time_numeric / period1)
    df["Cos_Time1"] = np.cos(2 * np.pi * time_numeric / period1)
    df["Sin_Time2"] = np.sin(2 * np.pi * time_numeric / period2)
    df["Cos_Time2"] = np.cos(2 * np.pi * time_numeric / period2)

    return df

def read_data(path):  # change
    """
    Reads a CSV file into a pandas DataFrame, handles common index column issues,
    and applies basic preprocessing including date parsing and feature addition.

    Parameters:
        path (str): The full file path to the CSV file to be read.

    Returns:
        pd.DataFrame: A cleaned and preprocessed DataFrame with:
            - Datetime index based on the 'week' column
            - Cleaned and integer-converted 'weekcase' values
            - Time-based features added via `add_time_features(df)`

    Requirements:
        - The CSV file must contain at least the following columns:
            * 'week': A date or string column representing weekly time points.
                      It will be parsed into datetime format and used as the index.
            * 'weekcase': A numeric or string column indicating weekly values.
                          If formatted with commas (e.g., "1,234"), they will be removed
                          and values converted to integers.

    Behavior:
        - If the CSV contains a common auto-generated index column (e.g., 'Unnamed: 0', 'index'),
          it is ignored.
        - The 'week' column is parsed as datetime and reformatted as '%Y-%m-%d'.
        - The DataFrame's index is set to the cleaned 'week' column.
        - Calls `add_time_features(df)` to add additional time-based variables.

    Notes:
        - This function assumes that the file is correctly formatted as described.
          Missing or incorrectly named required columns will cause a runtime error.
        - The function is designed for time series forecasting workflows that use weekly data.
    """
    filename = os.path.basename(path)  # Returns 'file.txt'
    inFoldername_pre = os.path.dirname(path)

    # Read the first few rows without index to check the header
    preview = pd.read_csv(path, nrows=5)
    first_col = preview.columns[0]

    # Check if the first column is a duplicate of the index or not
    if first_col.lower() in ['unnamed: 0', 'index']:  # heuristics for common index column names
        df = pd.read_csv(path, index_col=0)
    else:
        df = pd.read_csv(path)
    # df = pd.read_csv(inFoldername_pre+'/'+filename,index_col=0)

    df.index = df.week  # Transferring index as timeline
    # df.index = pd.to_datetime(df.index, format = '%m/%d/%Y').strftime("%Y-%m-%d")

    # Fix 'weekcase' format (remove commas and convert to int)
    df['weekcase'] = df['weekcase'].astype(str).str.replace(',', '', regex=False).astype(int)

    data = df.weekcase  # single variable data, with index as timeline

    df.index = pd.to_datetime(df.index)
    df = add_time_features(df)  # not used in the prediction anyways. we use single_test, full_test contains the time features

    # To make sure different Date formats work
    # need to be in the same cell as read, otherwise running the cell more than once after reading would cause error
    df['week'] = pd.to_datetime(df['week'], errors='coerce')
    df['week'] = df['week'].dt.strftime('%Y-%m-%d')

    # df.index = df.week
    df.set_index('week', inplace=True)
    return df



def read_model_results(
    base_path,
    tuned_on_filename,
    model_name,
    wave,
    shift,
    n_splits,
    test_size,
    file_template="best_params_wave {wave} n{n_splits}_t{test_size}_shift {shift}_{model}_results_{tuned_on_filename}.json"
):
    """
    Reads the saved grid search results from JSON and returns the loaded dictionary.

    Parameters:
        base_path (str): Directory where the JSON file is stored.
        tuned_on_filename (str): Descriptor for the tuning dataset.
        model_name (str): Name of the model (e.g., 'xgb').
        wave (int): Wave identifier.
        shift (int): Shift identifier.
        n_splits (int): Number of cross-validation splits.
        test_size (int): Size of the test set.
        file_template (str): Template string for the filename.

    Returns:
        dict: Dictionary containing 'input_params' and 'output_params' loaded from JSON.
    """
    file_name = file_template.format(
        wave=wave,
        n_splits=n_splits,
        test_size=test_size,
        shift=shift,
        model=model_name,
        tuned_on_filename=tuned_on_filename
    )
    file_path = os.path.join(base_path, file_name)

    if not os.path.exists(file_path):
        print(f"File not found: {file_path}")
        return None

    with open(file_path, "r") as file:
        data = json.load(file)


    return data



def populate_tuned_model_dict(
    base_path,
    tuned_on_filename,
    models,
    waves,
    shifts,
    n_splits,
    test_size,
    file_template="best_params_wave {wave} n{n_splits}_t{test_size}_shift {shift}_{model}_results_{tuned_on_filename}.json"
):
    """
    Populates two dictionaries: one with tuned model hyperparameters and another with tuning input parameters.
    If input_params/output_params are missing, treats the entire content as output_params.

    Parameters:
        base_path (str): The directory where the JSON files are stored.
        models (list of str): List of model names (e.g., ['xgb', 'rf']).
        waves (list of objects with 'waveID' attribute): List of wave objects.
        shifts (list of int): List of shift identifiers.
        n_splits (int): Number of CV splits.
        test_size (int): Test set size.
        file_template (str): Template for the filename.

    Returns:
        tuple: (tuned_model_dict, input_params_dict)
    """
    tuned_model_dict = {}
    input_params_dict = {}

    for model in models:
        for WAVE in waves:
            for shift in shifts:
                key = f'wave {WAVE.waveID} shift {shift}{model}'

                # Read the JSON file
                json_load = read_model_results(
                    base_path=base_path,
                    tuned_on_filename=tuned_on_filename,
                    model_name=model,
                    wave=WAVE.waveID,
                    shift=shift,
                    n_splits=n_splits,
                    test_size=test_size,
                    file_template=file_template
                )

                print(f"\n=== Model: {model} | Wave: {WAVE.waveID} | Shift: {shift} ===")

                if json_load is not None:
                    if "input_params" in json_load and "output_params" in json_load:
                        input_params = json_load["input_params"]
                        output_params = json_load["output_params"]
                    else:
                        input_params = {}
                        output_params = json_load  # Assume the full dict is output

                    print("Input Parameters:")
                    print(json.dumps(input_params, indent=4))
                    print("Tuned Output Parameters:")
                    print(json.dumps(output_params, indent=4))
                else:
                    # File not found — use default params and log the message
                    print(f"Parameter file not found for {key}. Using default parameters.")
                    input_params = None
                    output_params = default_params.get(model, {})

                    print("Default Parameters:")
                    print(json.dumps(output_params, indent=4))

                # Save into the dictionaries
                input_params_dict[key] = input_params
                tuned_model_dict[key] = output_params


    return tuned_model_dict, input_params_dict



def populate_tuned_model_dict_with_defaults(
    base_path,
    tuned_on_filename,
    models,
    waves,
    shifts,
    n_splits,
    test_size,
    default_params=None,  # Allow default_params to be optional
    file_template="best_params_wave {wave} n{n_splits}_t{test_size}_shift {shift}_{model}_results_{tuned_on_filename}.json"
):
    """
    Populates a dictionary with model tuning results for each combination of model, wave, and shift.
    If a parameter file is not found, it assigns default parameters for the specific combination.
    If default_params is not provided, it falls back to the original model defaults.

    Parameters:
        base_path (str): The directory where the JSON files are stored.
        tuned_on_filename (str): Filename for the data the models are tuned on.
        models (list of str): List of model names (e.g., ['xgb', 'rf']).
        waves (list of integer): List of wave identifiers.
        shifts (list of integer): List of shift identifiers.
        default_params (dict, optional): Dictionary containing default parameters for models.
                                         If None, will attempt to use the model's internal defaults.
        file_template (str): Template for the filename, containing placeholders for `wave`, `shift`, and `model`.

    Returns:
        dict: A dictionary containing the tuned model results, keyed by `wave+shift+model`.
    """
    tuned_model_dict = {}

    for model in models:
        for WAVE in waves:
            for shift in shifts:
                key = f'wave {WAVE.waveID} shift {shift}{model}'
                file_path = os.path.join(
                    base_path,
                    file_template.format(
                        wave=WAVE.waveID,
                        shift=shift,
                        model=model,
                        tuned_on_filename=tuned_on_filename
                    )
                )

                if os.path.exists(file_path):
                    with open(file_path, 'r') as f:
                        json_load = json.load(f)
                        print(f"Loaded parameters for {key} from {file_path}")
                else:
                    print(f"Parameter file not found for {key}. Using default parameters.")

                    # If default_params is provided, get model-specific defaults; otherwise, use an empty dict.
                    json_load = (default_params.get(model, {}) if default_params else {})

                tuned_model_dict[key] = json_load

    return tuned_model_dict

def read_grid_search_results(output_folder, output_filename):
    """
    Reads the grid search results from a JSON file and extracts useful information.

    Parameters:
        output_folder: Folder where the results are stored.
        output_filename: Filename of the saved grid search results.

    Returns:
        results: A dictionary containing:
            - best_params: Dictionary of the best hyperparameters.
            - best_score: The best score achieved.
            - all_results: Full content of the JSON file (for additional inspection).
    """
    result_path = os.path.join(output_folder, output_filename)

    # Check if the file exists
    if not os.path.exists(result_path):
        raise FileNotFoundError(f"File not found at {result_path}. Please check the folder and filename.")

    # Load the JSON file
    with open(result_path, "r") as file:
        grid_results = json.load(file)

    # Extract best parameters and best score
    if "rank_test_score" in grid_results and "mean_test_score" in grid_results:
        best_index = grid_results["rank_test_score"].index(1)  # Rank 1 corresponds to the best parameters
        best_params = {param: grid_results["params"][best_index][param] for param in grid_results["params"][best_index]}
        best_score = grid_results["mean_test_score"][best_index]
    else:
        raise ValueError("The JSON file does not contain expected fields: 'rank_test_score' or 'mean_test_score'.")

    # Return the parsed results
    return {
        "best_params": best_params,
        "best_score": best_score,
        "all_results": grid_results
    }

#Gnereralized
# Function to calculate MAPE
def calculate_mape(predicted, observed):
    return abs(predicted - observed) / observed

def calculate_model_MAPE(wave_map, visualization_data, models):
    """
    Generalized function to calculate MAPE for multiple ML models.

    Args:
        wave_map (dict): Mapping of wave numbers to labels.
        visualization_data (DataFrame): DataFrame containing model predictions and observed values.
        models (dict): Dictionary of models with their keys (e.g., {'ARIMA': SARIMAX, 'RF': RandomForestRegressor}).

    Returns:
        DataFrame: Final grouped DataFrame with average MAPE values.
    """
    # Map the Wave values to specified labels
    visualization_data['Wave'] = visualization_data['Wave'].map(wave_map)

    # Dynamically calculate MAPE for each model
    for model_name in models.keys():
        column_name = f"{model_name}_MAPE"
        visualization_data[column_name] = calculate_mape(
            visualization_data[model_name], visualization_data['Observed']
        )

    # Melt the DataFrame to long format for all MAPE columns
    mape_columns = [f"{model_name}_MAPE" for model_name in models.keys()]
    melted_df = visualization_data.melt(
        id_vars="Wave",
        value_vars=mape_columns,
        var_name="Model",
        value_name="MAPE"
    )

    # Clean up Model column by removing '_MAPE'
    melted_df['Model'] = melted_df['Model'].str.replace('_MAPE', '')

    # Rename 'Wave' column to 'Input'
    melted_df.rename(columns={'Wave': 'Input'}, inplace=True)

    # Calculate mean MAPE for each wave and model combination
    final_task_df = melted_df.groupby(['Input', 'Model'], as_index=False)['MAPE'].mean()

    # Display the final DataFrame
    print(final_task_df)
    return final_task_df

### Someone else's code.

"""
This module provides a class to split time-series data for back-testing and evaluation.
The aim was to extend the current sklearn implementation and extend it's uses.

Might be useful for some ;)
"""

import logging
from typing import Optional

import numpy as np
from sklearn.model_selection._split import _BaseKFold
from sklearn.utils import indexable
from sklearn.utils.validation import _num_samples

LOGGER = logging.getLogger(__name__)


class TimeSeriesSplit(_BaseKFold):  # pylint: disable=abstract-method
    """Time Series cross-validator

    Provides train/test indices to split time series data samples that are observed at fixed time intervals,
    in train/test sets. In each split, test indices must be higher than before, and thus shuffling in cross validator is
    inappropriate.

    This cross_validation object is a variation of :class:`TimeSeriesSplit` from the popular scikit-learn package.
    It extends its base functionality to allow for expanding windows, and rolling windows with configurable train and
    test sizes and shifts between each. i.e. train on weeks 1-8, skip week 9, predict week 10-11.

    In this implementation we specifically force the test size to be equal across all splits.

    Expanding Window:

            Idx / Time  0..............................................n
            1           |  train  | shift |  test  |                   |
            2           |       train     | shift  |  test  |          |
            ...         |                                              |
            last        |            train            | shift |  test  |

    Rolling Windows:
            Idx / Time  0..............................................n
            1           | train   | shift |  test  |                   |
            2           | step |  train  | shift |  test  |            |
            ...         |                                              |
            last        | step | ... | step |  train  | shift |  test  |

    Parameters:
        n_splits : int, default=5
            Number of splits. Must be at least 4.

        train_size : int, optional
            Size for a single training set.

        test_size : int, optional, must be positive
            Size of a single testing set

        shift : int, default=0, must be positive
            Number of index shifts to make between train and test sets
            e.g,
            shift=0
                TRAIN: [0 1 2 3] TEST: [4]
            shift=1
                TRAIN: [0 1 2 3] TEST: [5]
            shift=2
                TRAIN: [0 1 2 3] TEST: [6]

        force_step_size : int, optional
            Ignore split logic and force the training data to shift by the step size forward for n_splits
            e.g
            TRAIN: [ 0  1  2  3] TEST: [4]
            TRAIN: [ 0  1  2  3  4] TEST: [5]
            TRAIN: [ 0  1  2  3  4  5] TEST: [6]
            TRAIN: [ 0  1  2  3  4  5  6] TEST: [7]

    Examples
    --------
    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4],[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])
    >>> y = np.array([1, 2, 3, 4, 5, 6,7,8,9,10,11,12])
    >>> tscv = TimeSeriesSplit(train_size=3,n_splits=5)
    >>> print(tscv)  # doctest: +NORMALIZE_WHITESPACE
    TimeSeriesSplit(shift=0, force_step_size=None, n_splits=5, test_size=None,
        train_size=None)
    >>> for train_index, test_index in tscv.split(X):
    ...    print('TRAIN:', train_index, 'TEST:', test_index)
    ...    X_train, X_test = X[train_index], X[test_index]
    ...    y_train, y_test = y[train_index], y[test_index]
    TRAIN: [0] TEST: [1]
    TRAIN: [0 1] TEST: [2]
    TRAIN: [0 1 2] TEST: [3]
    TRAIN: [0 1 2 3] TEST: [4]
    TRAIN: [0 1 2 3 4] TEST: [5]
    """

    def __init__(self,
                 n_splits: Optional[int] = 5,
                 train_size: Optional[int] = None,
                 test_size: Optional[int] = None,
                 shift: int = 0,
                 force_step_size: Optional[int] = None):
        print('Custom TimeSeriesSplit')
        if n_splits and n_splits < 5:
            raise ValueError(f'Cannot have n_splits less than 5 (n_splits={n_splits})')
        super().__init__(n_splits, shuffle=False, random_state=None)

        self.train_size = train_size

        if test_size and test_size < 0:
            raise ValueError(f'Cannot have negative values of test_size (test_size={test_size})')
        self.test_size = test_size

        if shift < 0:
            raise ValueError(f'Cannot have negative values of shift (shift={shift})')
        self.shift = shift

        if force_step_size and force_step_size < 1:
            raise ValueError(f'Cannot have zero or negative values of force_step_size '
                             f'(force_step_size={force_step_size}).')

        self.force_step_size = force_step_size

    def split(self, X, y=None, groups=None):
        """Generate indices to split data into training and test set.

        Parameters:
            X : array-like, shape (n_samples, n_features)
                Training data, where n_samples is the number of samples  and n_features is the number of features.

            y : array-like, shape (n_samples,)
                Always ignored, exists for compatibility.

            groups : array-like, with shape (n_samples,), optional
                Always ignored, exists for compatibility.

        Yields:
            train : ndarray
                The training set indices for that split.

            test : ndarray
                The testing set indices for that split.
        """
        X, y, groups = indexable(X, y, groups)  # pylint: disable=unbalanced-tuple-unpacking
        n_samples = _num_samples(X)
        n_splits = self.n_splits
        n_folds = n_splits + 1
        shift = self.shift

        if n_folds > n_samples:
            raise ValueError(f'Cannot have number of folds={n_folds} greater than the number of samples: {n_samples}.')

        indices = np.arange(n_samples)
        split_size = n_samples // n_folds

        train_size = self.train_size or split_size #* self.n_splits
        test_size = self.test_size or n_samples // n_folds
        full_test = test_size + shift

        if full_test + n_splits > n_samples:
            raise ValueError(f'test_size\\({test_size}\\) + shift\\({shift}\\) = {test_size + shift} + '
                             f'n_splits={n_splits} \n'
                             f' greater than the number of samples: {n_samples}. Cannot create fold logic.')

        # Generate logic for splits.
        # Overwrite fold test_starts ranges if force_step_size is specified.
        if self.force_step_size:
            step_size = self.force_step_size
            final_fold_start = n_samples - (train_size + full_test)
            range_start = (final_fold_start % step_size) + train_size

            test_starts = range(range_start, n_samples, step_size)

        else:
            if not self.train_size:
                step_size = split_size
                range_start = (split_size - full_test) + split_size + (n_samples % n_folds)
            else:
                step_size = (n_samples - (train_size + full_test)) // n_folds
                final_fold_start = n_samples - (train_size + full_test)
                range_start = (final_fold_start - (step_size * (n_splits - 1))) + train_size

            test_starts = range(range_start, n_samples, step_size)

        # Generate data splits.
        for test_start in test_starts:
            idx_start = test_start - train_size if self.train_size is not None else 0
            # Ensure we always return a test set of the same size
            if indices[test_start:test_start + full_test].size < full_test:
                continue
            yield (indices[idx_start:test_start],
                   indices[test_start + shift:test_start + full_test])

#updated
def extract_model_weights_from_dict(weekly_prediction_future):
    """
    Extracts model weights per date and shift-length combination from weekly_prediction_future.

    Parameters:
        weekly_prediction_future (dict): Dictionary mapping each week (str) to a DataFrame containing model weights.

    Returns:
        dict: Nested dictionary structured as:
              {
                '2025-01-05': {
                    '0 1': (['ARIMA', 'RF', 'xgb'], [0.29, 0.33, 0.37]),
                    '0 2': (...),
                    ...
                },
                ...
              }
    """
    model_weights = {}

    for date_key, df in weekly_prediction_future.items():
        shift_weight_dict = {}
        for _, row in df.iterrows():
            shift_length = f"{int(row['Shift'])} {int(row['Length'])}"
            weights = np.array([row['weight_ARIMA'], row['weight_RF'], row['weight_xgb']])
            shift_weight_dict[shift_length] = (['ARIMA', 'RF', 'xgb'], weights)
        model_weights[date_key] = shift_weight_dict

    return model_weights



# def extract_predictions_from_dict(result_dict):
#     """
#     Extracts weekly predictions and observed values from a nested dictionary of model evaluation results.

#     Parameters:
#     result_dict (dict): Dictionary where keys are week start dates and values are DataFrames
#                         containing prediction results for multiple weeks.

#     Returns:
#     pd.DataFrame: Consolidated DataFrame with columns: 'Week', 'Observed', 'ARIMA', 'RF', 'xgb', 'MLAMA'
#     """
#     rows = []

#     for week_start, df in result_dict.items():
#         for i, row in df.iterrows():
#             week = row['Week']
#             observed = row['Observed']

#             # Extract predictions: handle Series or list
#             arima_pred = row['ARIMA'].values[0] if hasattr(row['ARIMA'], 'values') else row['ARIMA']
#             rf_pred = row['RF'][0] if isinstance(row['RF'], list) else row['RF']
#             xgb_pred = row['xgb'][0] if isinstance(row['xgb'], list) else row['xgb']
#             mlama_pred = row['MLAMA'].values[0] if hasattr(row['MLAMA'], 'values') else row['MLAMA']

#             rows.append({
#                 'Week': pd.to_datetime(week),
#                 'Observed': observed,
#                 'ARIMA': arima_pred,
#                 'RF': rf_pred[0],
#                 'xgb': xgb_pred[0],
#                 'MLAMA': mlama_pred
#             })

#     result_df = pd.DataFrame(rows)
#     result_df.sort_values(by='Week', inplace=True)
#     result_df.reset_index(drop=True, inplace=True)
#     return result_df

# def extract_evaluation_from_dict(result_dict, models, metric="MAPE"):
#     """
#     Extracts evaluation metrics (e.g., MAPE, MSE) from a nested dictionary of model evaluation results.

#     Parameters:
#         result_dict (dict): Dictionary where keys are week start dates and values are DataFrames
#                             containing prediction results and evaluation metrics.
#         models (dict): Dictionary of models with their names as keys.
#         metric (str): Evaluation metric to extract (e.g., 'MAPE' or 'MSE').

#     Returns:
#         pd.DataFrame: Consolidated DataFrame with columns: 'Week', '<model1>_<metric>', ..., '<modelN>_<metric>'
#     """
#     rows = []

#     for week_start, df in result_dict.items():
#         for i, row in df.iterrows():
#             week = pd.to_datetime(row['Week'])
#             eval_row = {'Week': week}

#             for model_name in models.keys():
#                 metric_key = f"{model_name}_{metric}"
#                 if metric_key in row:
#                     metric_val = row[metric_key]
#                     # If value is inside a list or Series, extract the scalar
#                     if hasattr(metric_val, 'values'):
#                         metric_val = metric_val.values[0]
#                     elif isinstance(metric_val, list):
#                         metric_val = metric_val[0]
#                     eval_row[metric_key] = metric_val
#                 else:
#                     eval_row[metric_key] = None  # In case key doesn't exist

#             rows.append(eval_row)

#     result_df = pd.DataFrame(rows)
#     result_df.sort_values(by='Week', inplace=True)
#     result_df.reset_index(drop=True, inplace=True)
#     return result_df


# import numpy as np
# import pandas as pd

# def extract_predictions_from_dict(result_dict, target_length, models):
#     import numpy as np
#     import pandas as pd

#     rows = []

#     required_models = ['MLAMA', 'ARIMA']
#     optional_models = list(models.keys()) if models else []
#     all_models = list(dict.fromkeys(required_models + optional_models))

#     for week_start, df in result_dict.items():
#         filtered_df = df[df['Length'] == target_length]

#         for _, row in filtered_df.iterrows():
#             prediction_row = {
#                 'CurrentWeek': pd.to_datetime(week_start),        # ✅ This is the dictionary key
#                 'Week': pd.to_datetime(row['Week']),              # ✅ This is the prediction target week
#                 'Shift': row['Shift'],
#                 'Length': row['Length'],
#                 'Observed': row['Observed']
#             }

#             for model_name in all_models:
#                 value = row.get(model_name, None)
#                 if isinstance(value, (list, np.ndarray)):
#                     prediction_row[model_name] = value[0] if len(value) > 0 else None
#                 elif hasattr(value, 'values') and len(value.values) == 1:
#                     prediction_row[model_name] = value.values[0]
#                 else:
#                     prediction_row[model_name] = value

#             rows.append(prediction_row)

#     result_df = pd.DataFrame(rows)
#     result_df.sort_values(by='Week', inplace=True)
#     result_df.reset_index(drop=True, inplace=True)
#     return result_df


def extract_all_predictions_from_dict(result_dict, models):
    """
    Extracts weekly predictions for all prediction lengths from a nested dictionary of results.

    Parameters:
        result_dict (dict): Dictionary where keys are current week and values are DataFrames.
        models (dict): Dictionary of model objects with names as keys.

    Returns:
        pd.DataFrame: Combined DataFrame for all prediction lengths across all models.
    """
    all_lengths = set()
    for df in result_dict.values():
        all_lengths.update(df['Length'].unique())

    all_dfs = []
    for length in sorted(all_lengths):
        df = extract_predictions_from_dict(result_dict, target_length=length, models=models)
        all_dfs.append(df)

    return pd.concat(all_dfs, axis=0).reset_index(drop=True)