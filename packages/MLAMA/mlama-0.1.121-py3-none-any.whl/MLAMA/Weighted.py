# -*- coding: utf-8 -*-
"""Weighted.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YQ95wI-wzXmHgpq41Y0fTDgia0tEZq7o
"""

#test.r from the main version to python Post-hoc Optimization Code.r from the cleaner version
#weighted ARIMA used in the following parts????

#observe kivabe calculate korse?
#MAIN MLAMA Visualization here

import pandas as pd
import numpy as np
from scipy.optimize import minimize

from scipy.optimize import minimize

#Weighted.py
#test.r from the main version to python Post-hoc Optimization Code.r from the cleaner version
#weighted ARIMA used in the following parts????

#generalized from MLAMA Package 2025 Data.ipynb


#generalize
def find_diff(data, models, observed_column_name='Observed'):
    """
    Generalized function to calculate differences between model predictions and observed values.

    Parameters:
    - data: DataFrame containing model predictions and observed values
    - models: Dictionary of models where keys are model names
    - observed_column_name: Column name for the observed data (default is 'Observed')

    Returns:
    - DataFrame with new columns containing the differences for each model
    """
    # Iterate through the models dictionary and calculate differences
    #print('find_diff', data.dtypes)
    for model_name in models.keys():
        if model_name in data.columns:
            diff_column = f'diff_{model_name}'
            data[diff_column] = data[model_name] - data[observed_column_name]

    return data



from scipy.optimize import minimize

# Cost function to minimize
def cost_function(weights, predictions, observed):
    """
    Computes the sum of squared errors between weighted model predictions and observed values.

    Parameters:
    -----------
    weights : np.ndarray
        Array of weights to be applied to the model predictions.
    predictions : np.ndarray
        2D array where each column represents predictions from a different model.
    observed : np.ndarray
        1D array of observed (true) values.

    Returns:
    --------
    float
        The sum of squared errors between the weighted predictions and the observed values.
    """
    weighted_predictions = predictions @ weights  # Matrix multiplication
    r = np.sum((observed - weighted_predictions) ** 2)  # Squared error
    return r


def find_optimal_weights(data, predictions, trend_adjustment_steps, observed_column_name, models):
    """
    Function to find optimal weights for each unique combination of shift and Length.

    Parameters:
        data (pd.DataFrame): The dataset containing predictions and observed values.
        predictions (list): List of prediction lengths to iterate over.
        trend_adjustment_steps (list): List of shift values to iterate over.
        observed_column_name (str): Column name for the observed values.
        models (dict): Dictionary of models with their names as keys and classes as values.

    Returns:
        dict: A dictionary containing model names and optimal weights for each shift and length combination.
    """
    optimal_weights = {}

    # Extract model names
    model_names = list(models.keys())

    # Iterate over unique combinations of shift and Length
    for length in predictions:
        for shift in trend_adjustment_steps:
            # Filter the data for the specific combination of shift and length
            combo_data = data[(data['Shift'] == shift) & (data['Length'] == length)]

            # Keep only numeric columns for groupby mean
            numeric_columns = combo_data.select_dtypes(include=['number']).columns
            combo_data = combo_data[numeric_columns].groupby('Wave').mean()

            # Extract the predictions for the selected models
            predicted_values = combo_data[model_names].values
            observed = combo_data[observed_column_name].values

            # Initial weights: inverse squared differences
            diff_columns = [f"diff_{model}" for model in model_names]
            x = 1 / combo_data[diff_columns].pow(2)
            initial_weights = (x.div(x.sum(axis=1), axis=0)).mean().values

            # Optimizing weights using minimize
            opt_res = minimize(
                cost_function,  # Assumes cost_function is predefined
                initial_weights,
                args=(predicted_values, observed),
                method='L-BFGS-B',
                bounds=[(0, 1)] * len(model_names)
            )

            # Storing the results as a tuple (model_names, weights)
            optimal_weights[f"{shift} {length}"] = (model_names, opt_res.x)

    return optimal_weights


def find_stan_optimal_weights(data, predictions, trend_adjustment_steps, observed_column_name, models):
    """
    Computes standardized optimal weights for model ensemble predictions using
    the output from `find_optimal_weights`.

    This function first calls `find_optimal_weights` to compute optimal weights for
    each combination of shift and prediction length, then normalizes these weights
    so that they sum to 1.

    Parameters:
    -----------
    data : pd.DataFrame
        The dataset containing model predictions, shift and length identifiers, and observed values.
    predictions : list
        A list of prediction lengths for which weights should be optimized.
    trend_adjustment_steps : list
        A list of shift values representing temporal adjustment steps.
    observed_column_name : str
        The name of the column in `data` containing the true observed values.
    models : dict
        A dictionary where keys are model names and values are model class references.

    Returns:
    --------
    dict
        A dictionary with keys as "shift length" strings and values as tuples:
        (list of model names, np.ndarray of standardized weights that sum to 1).
    """
    optimal_weights = find_optimal_weights(data, predictions, trend_adjustment_steps, observed_column_name, models)

    # Standardizing the optimal weights
    stan_optimal_weights = {k: (v[0], v[1] / np.sum(v[1])) for k, v in optimal_weights.items()}

    return stan_optimal_weights

#Weighted.py
#Generalized

def find_weighted_predictions(data, stan_optimal_weights, observed_column_name, models, metrics=["MAPE", "MSE"]):
    """
    Function to calculate weighted predictions and evaluate performance for given models and optimal weights.

    Parameters:
        data (pd.DataFrame): The dataset containing predictions, observed values, and scenario details (e.g., shift, Length).
        stan_optimal_weights (dict): Dictionary containing optimal weights for each scenario as {scenario_name: weights}.
        observed_column_name (str): Column name for the observed values.
        models (dict): Dictionary of models with their names as keys and classes as values.
        metrics (list): List of evaluation metrics to calculate (e.g., ["MAPE", "MSE"]).

    Returns:
        theDD (pd.DataFrame): Summary of average metric values by algorithm and adaptability (shift).
        theD_long (pd.DataFrame): Long-format DataFrame with metric values for visualization.
        joined_df (pd.DataFrame): DataFrame containing weighted predictions and metrics.
    """
    combined_df = pd.DataFrame()

    # Iterate through each scenario and its weights
    for scenario_name, weights in stan_optimal_weights.items():
        shift, length = map(float, scenario_name.split())
        temp_df = pd.DataFrame([weights[1]], columns=[f"weight_{model}" for model in weights[0]])#models.keys()
        temp_df['Shift'] = shift
        temp_df['Length'] = length
        combined_df = pd.concat([combined_df, temp_df], ignore_index=True)

    # Merge the weights with the original data
    joined_df = pd.merge(combined_df, data, on=['Shift', 'Length'])

    # Calculate the weighted predictions dynamically
    joined_df['MLAMA'] = sum(
        joined_df[model] * joined_df[f"weight_{model}"] for model in models.keys()
    )

    # Calculate specified evaluation metrics for each model and weighted predictions
    for model in models.keys():
        if "MAPE" in metrics:
            joined_df[f"{model}_MAPE"] = (
                abs(joined_df[model] - joined_df[observed_column_name]) / joined_df[observed_column_name]
            )
        if "MSE" in metrics:
            joined_df[f"{model}_MSE"] = (
                (joined_df[model] - joined_df[observed_column_name]) ** 2
            )

    if "MAPE" in metrics:
        joined_df['MLAMA_MAPE'] = (
            abs(joined_df['MLAMA'] - joined_df[observed_column_name]) / joined_df[observed_column_name]
        )
    if "MSE" in metrics:
        joined_df['MLAMA_MSE'] = (
            (joined_df['MLAMA'] - joined_df[observed_column_name]) ** 2
        )

    # Grouping by shift, Wave, and Length, and summarizing specified metrics
    metric_columns = {}
    for metric in metrics:
        metric_columns.update({f"{model}_{metric}": (f"{model}_{metric}", 'mean') for model in models.keys()})
        metric_columns[f"MLAMA_{metric}"] = (f"MLAMA_{metric}", 'mean')

    theD = joined_df.groupby(['Shift', 'Wave', 'Length']).agg(**metric_columns).reset_index()

    # Convert percentages for MAPE
    for metric in metrics:
        if metric == "MAPE":
            metric_cols = [f"{model}_MAPE" for model in models.keys()] + ["MLAMA_MAPE"]
            theD[metric_cols] /= 100

    # Rename columns
    theD.columns = ['Adaptability', 'Wave', 'Prediction Length'] + list(metric_columns.keys())

    # Convert from wide to long format for visualization
    theD_long = pd.melt(
        theD,
        id_vars=["Wave", "Adaptability", "Prediction Length"],
        value_vars=list(metric_columns.keys()),
        var_name="Algorithm",
        value_name="Value"
    )

    # Create an empty list to store results
    results = []

    for metric in metrics:
        # Filter rows that contain the current metric in the 'Algorithm' column
        filtered_df = theD_long[theD_long['Algorithm'].str.endswith(f'_{metric}')].copy()

        # Remove metric suffix from Algorithm names
        filtered_df['Algorithm'] = filtered_df['Algorithm'].str.replace(f'_{metric}', '', regex=False)

        # Group by Algorithm and Adaptability, calculating the mean Value
        grouped = filtered_df.groupby(['Algorithm', 'Adaptability']).agg({'Value': 'mean'}).reset_index()

        # Add a new column for the metric name
        grouped['Metric'] = metric

        # Append to results
        results.append(grouped)

    # Concatenate all results into a single dataframe
    theDD = pd.concat(results, ignore_index=True)

    # Reorder columns
    theDD = theDD[['Algorithm', 'Adaptability', 'Metric', 'Value']]


    return theDD, theD_long, joined_df