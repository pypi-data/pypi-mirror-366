# -*- coding: utf-8 -*-
"""Models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gb8EKmVgAa2u7qW6ONnsbMaBBqVblgJ1
"""

import numpy as np
import pandas as pd
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error
from math import sqrt
from xgboost import XGBRegressor
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX

from statsmodels.tsa.stattools import adfuller

from .Processor import replace_negatives

#Yushu
#Models.py
#### Function for Statistics Model
#from MLAMA Package 2025 Data.ipynb
def stats_data(df_use):
    """
    Prepares datasets for statistical models from the given input dataframe.

    Parameters:
    -----------
    df_use : pd.DataFrame
        The input dataset containing the target variable ('weekcase') and covariates
        ('Sin_Time1', 'Cos_Time1', 'Sin_Time2', 'Cos_Time2').

    Returns:
    --------
    data : pd.DataFrame
        Univariate time series data containing only the 'weekcase' column,
        formatted for models like ARIMA.

    finaldf_poi : pd.DataFrame
        Multivariate dataset containing 'weekcase' and seasonal covariates,
        intended for use in other statistical models.

    pd.DataFrame
        A dataframe with a single 'week' column representing the datetime index
        of the original data.
    """
    # generating dataset for statistics model, which includes only the weekcase and the covariates
    # df_use: the current dataset to process
    # data - input for ARIMA model (single variate), only contains the weekcase
    # finaldf_poi - input for other statis models (multivariate), contains the covariates
    data = pd.DataFrame(df_use.weekcase)  # only weekcase extracted
    data.index = df_use.index
    data.index = pd.to_datetime(data.index)
    data = data.reset_index(drop=True)
    data_poi = df_use[["weekcase", "Sin_Time1", "Cos_Time1", "Sin_Time2", "Cos_Time2"]]
    finaldf_poi = data_poi.reset_index(drop=True)
    return data, finaldf_poi, pd.DataFrame(df_use.index, columns=['week'])

#Yushu

def ARIMA_model(ARMAmodel, test, predict_length, confidence_interval):
    """
    Fits a passed ARIMA model, generates forecasts, and computes various error metrics.

    Parameters:
        ARMAmodel : statsmodels.tsa.arima.model.ARIMA
            A pre-specified ARIMA model instance (e.g., ARIMA(endog, order=(p,d,q))) to be fitted.
        test : pd.DataFrame
            A DataFrame containing the test data. Must include a column named 'weekcase' and a datetime index.
        predict_length : int
            Total number of time points (fitting + forecasting) for which predictions are needed.
        confidence_interval : float
            Significance level (e.g., 0.05 for 95% CI) used to compute the forecast confidence intervals.

    Returns:
        arima_mse : float
            Mean Squared Error between the forecast and true test values.
        arima_rmse : float
            Root Mean Squared Error.
        arima_mae : float
            Mean Absolute Error.
        arima_mape : float
            Mean Absolute Percentage Error.
        list_mape : np.ndarray
            Array of Absolute Percentage Errors (APE) for each test point.
        arima : pd.DataFrame
            Concatenated DataFrame containing fitted values and forecasted values, labeled as 'ARIMA'.
        arimap : pd.Series
            Forecasted ARIMA values (length equal to the test set).
        arimaf : pd.Series
            Fitted values from the training portion (of length `predict_length - len(test)`).

    Notes:
        - The `test` DataFrame must have a column named 'weekcase'.
        - The function assumes that `predict_length` was computed as training_length + test_length.
        - The ARIMA model is fit within the function before forecasting.
        - The resulting predictions are aligned with the index of `test`.
    """
    ARMAmodel = ARMAmodel.fit() #ARMAmodel is already trained and passed as a parameter
    test_length = len(test)
    y_pred = ARMAmodel.get_forecast(test_length)#forecast for the length
    y_pred_df = y_pred.conf_int(alpha = confidence_interval)
    y_pred_df["Predictions"] = ARMAmodel.predict(start = y_pred_df.index[0], end = y_pred_df.index[-1])
    y_pred_df.index = test.index
    y_pred_out = y_pred_df["Predictions"] ### Prediction of ARIMA model
    arima_mse = mean_squared_error(test, y_pred_out) ### Calculate MSE
    arima_rmse = np.sqrt(mean_squared_error(test, y_pred_out)) ### Calculate rMSE
    arima_mae = mean_absolute_error(test, y_pred_out) ### Calculate MAE
    arima_mape = mean_absolute_percentage_error(test, y_pred_out) ### Calculate the MAPE
    arima_list = np.abs(np.array(y_pred_out) - np.array(test)) / np.array(test)
    list_mape = np.abs((test['weekcase'] - y_pred_df['Predictions']) / test['weekcase'])
    arimap = y_pred_df["Predictions"]

    arimaf = ARMAmodel.fittedvalues.tail(predict_length - test_length)
    arimap_df = arimap.rename("ARIMA").to_frame()

    arimaf_df = pd.DataFrame(arimaf, columns=["ARIMA"])

    arima = pd.concat([arimaf_df, arimap_df], axis=0) ### Combine fitting value with prediction value
    arima.columns = ['ARIMA']

    return arima_mse, arima_rmse, arima_mae, arima_mape, list_mape, arima, arimap, arimaf


# def ARIMA_model(ARMAmodel,test,predict_length,confidence_interval):
#     ARMAmodel = ARMAmodel.fit() #ARMAmodel is already trained and passed as a parameter
#     test_length = len(test)
#     y_pred = ARMAmodel.get_forecast(test_length)#forecast for the length
#     y_pred_df = y_pred.conf_int(alpha = confidence_interval)
#     y_pred_df["Predictions"] = ARMAmodel.predict(start = y_pred_df.index[0], end = y_pred_df.index[-1])
#     y_pred_df.index = test.index
#     y_pred_out = y_pred_df["Predictions"] ### Prediction of ARIMA model
#     arima_mse = mean_squared_error(test,y_pred_out) ### Calculate MSE
#     arima_rmse = np.sqrt(mean_squared_error(test,y_pred_out)) ### Calculate rMSE
#     arima_mae = mean_absolute_error(test,y_pred_out) ### Calculate MAE
#     arima_mape = mean_absolute_percentage_error(test,y_pred_out) ### Calculate the MAPE
#     arima_list = np.abs(np.array(y_pred_out) - np.array(test)) / np.array(test)#np.array(np.abs(np.array(y_pred_out)-np.transpose(np.array(test)))/np.transpose(np.array(test))) ### Calculate the APE for each prediction value
#     #print('y_pred_out[Predictions]', y_pred_df['Predictions'])
#     list_mape = np.abs((test['weekcase'] - y_pred_df['Predictions']) / test['weekcase'])
#     #print('arima_list', arima_list)
#     #print('test', test['weekcase'])
#     #print('y_pred_out', y_pred_out)
#     #print('list_mape', list_mape)
#     arimap= y_pred_df["Predictions"] # length - test_length
#     #print('arima fitting size: ',predict_length-test_length)

#     arimaf = ARMAmodel.fittedvalues.tail(predict_length-test_length) # length 25, always fixed cz, during parameter passing I added test_length to prediction_length, now sub
#     arimap_df = arimap.rename("ARIMA").to_frame()


#     arimaf_df = pd.DataFrame(arimaf, columns=["ARIMA"])


#     arima = pd.concat([arimaf_df,arimap_df],axis=0) ### Combine fitting value with prediction value
#     arima.columns = ['ARIMA']

#     return arima_mse, arima_rmse,arima_mae,arima_mape, list_mape,arima, arimap, arimaf

#Models.py (updated in models.py) MLAMA Package 2025 Data.ipynb
# Updated docstring
def generate_ml_features(data_ml, parameter_length):
    """
    Generates lagged features for machine learning model input.

    Parameters:
    -----------
    data_ml : pd.DataFrame
        A DataFrame containing a column named 'weekcase' representing the time series data.
    parameter_length : int
        The number of lag features to generate (e.g., 5 will generate t+5 to t+1).

    Returns:
    --------
    finaldf : pd.DataFrame
        A DataFrame containing the original data and the newly generated lag features, with NA values dropped.
    data_index : pd.DataFrame
        A DataFrame containing the index (week dates) of the processed data.
    """
    dataframe = pd.DataFrame()
    for i in range(parameter_length, 0, -1):
        dataframe['t+' + str(i)] = data_ml.weekcase.shift(i)
    final_data = pd.concat([data_ml, dataframe], axis=1)
    final_data.dropna(inplace=True)  # first 'lag_reserve' points gone here
    finaldf = final_data.copy()

    data_index = pd.DataFrame(final_data.index, columns=['week'])
    finaldf = finaldf.reset_index(drop=True)
    return finaldf, data_index


def gen_ml(wave, test_length, parameter_length, lag_reserve, predictions, resid, shift,
           trend_adjustment_steps, adjustment=0, predefined_wave=True,
           current_week_prediction=False, current_week_length=0):
    """
    Splits time series data into training and testing sets with lagged features for machine learning models.

    Parameters:
    -----------
    wave : pd.DataFrame
        Input DataFrame containing the 'weekcase' column (the target variable).
    test_length : int
        Number of data points in the test set.
    parameter_length : int
        Number of lag features to generate.
    lag_reserve : int
        Reserved length for lag (not explicitly used inside the function).
    predictions : list
        List of prediction steps, used to determine the size of the input sequence.
    resid : bool
        Whether residual-based adjustment is used for train/test split (not used in this project anymore).
    shift : int
        Number of steps to shift for prediction start.
    trend_adjustment_steps : list
        A list of shift values to apply to each wave.
    adjustment : int, optional
        Number of steps to trim from the end of the training set (default is 0). Needed for shift = Delay
    predefined_wave (auto_generated_wave by code or not) : bool, optional
        Indicates whether the wave is user-defined (True) or generated automatically (False).
    current_week_prediction : bool, optional
        If True means that we are forecasting current weeks, no need to calculate test set explicitly,
        the last `current_week_length` data points are used as the test set (default is False).
    current_week_length : int, optional
        Number of data points considered for current-week prediction/forecast (default is 0).

    Returns:
    --------
    finaldf_train_x : pd.DataFrame
        Feature matrix for the training set (lag features only).
    finaldf_test_x : pd.DataFrame
        Feature matrix for the testing set (lag features only).
    finaldf_train_y : pd.Series
        Target values for the training set ('weekcase').
    finaldf_test_y : pd.Series
        Target values for the testing set ('weekcase').
    train_index : pd.DataFrame
        DataFrame containing index (week) of the training set.
    test_index : pd.DataFrame
        DataFrame containing index (week) of the testing set.
    """
    max_prediction_length = max(predictions)

    data_ml = pd.DataFrame(wave.weekcase)
    finaldf, data_index = generate_ml_features(data_ml, parameter_length)
    end_point = len(finaldf.index)

    if resid == False:
        x = end_point - max_prediction_length
        if current_week_prediction == False:
            if predefined_wave == False:
                x = x - max(trend_adjustment_steps) + shift
        else:
            x = end_point - current_week_length
    else:
        x = end_point - test_length

    finaldf_test = finaldf.loc[x:x + test_length - 1, :]
    finaldf_train = finaldf.loc[:x - 1 - adjustment, :]

    test_index = data_index.loc[x:x + test_length - 1, :]
    train_index = data_index.loc[:x - 1 - adjustment, :]

    finaldf_test_x = finaldf_test.loc[:, finaldf_test.columns != 'weekcase']
    finaldf_test_y = finaldf_test['weekcase']
    finaldf_train_x = finaldf_train.loc[:, finaldf_train.columns != 'weekcase']
    finaldf_train_y = finaldf_train['weekcase']

    return finaldf_train_x, finaldf_test_x, finaldf_train_y, finaldf_test_y, train_index, test_index

def ml_model(test_x, test_y, model, data):
    """
    Evaluates a machine learning model's predictions against test data using standard error metrics.

    Parameters:
        test_x : pd.DataFrame or np.ndarray
            Feature matrix used for prediction.
        test_y : pd.Series or np.ndarray
            True target values corresponding to `test_x`.
        model : sklearn-like estimator
            A trained machine learning model implementing a `.predict()` method.
        data : pd.DataFrame
            Original dataset containing a column 'weekcase' used as the ground truth for comparison.
            It must be at least as long as the number of predictions and indexed appropriately.

    Returns:
        mape_list : np.ndarray
            Element-wise Mean Absolute Percentage Error (MAPE) between predicted and true values.
        mse : float
            Mean Squared Error of the model’s predictions vs. `data['weekcase']` over the forecast range.
        rmse : float
            Root Mean Squared Error.
        mae : float
            Mean Absolute Error.
        mape : float
            Mean Absolute Percentage Error.
        predict : np.ndarray
            Model predictions for the input features `test_x`.

    Notes:
        - The function compares the model’s predictions to the last `len(predict)` values of `data['weekcase']`.
        - `mape_list` is computed using `test_y`, while the summary metrics use `data['weekcase']` for comparison.
        - Ensure consistency between `test_y` and `data['weekcase']` alignment for meaningful metrics.
    """
    predict = model.predict(test_x) # prediction
    mape_list = np.abs(predict - test_y)/test_y # MAPE for each prediction value??? not sure what's this
    mse = mean_squared_error(data['weekcase'][-len(predict):].values, predict[:]) # MSE for prediction model
    rmse = sqrt(mean_squared_error(data['weekcase'][-len(predict):].values, predict[:]))
    mae = mean_absolute_error(data['weekcase'][-len(predict):].values, predict[:])
    mape = mean_absolute_percentage_error(data['weekcase'][-len(predict):].values, predict[:])
    return mape_list, mse, rmse, mae, mape, predict


# def ml_model(test_x,test_y,model,data):
#     predict = model.predict(test_x) # prediction
#     mape_list = np.abs(predict - test_y)/test_y # MAPE for each prediction value??? not sure what's this
#     mse = mean_squared_error(data['weekcase'][-len(predict):].values,predict[:]) # MSE for prediction model
#     rmse = sqrt(mean_squared_error(data['weekcase'][-len(predict):].values,predict[:]))
#     mae = mean_absolute_error(data['weekcase'][-len(predict):].values,predict[:])
#     mape = mean_absolute_percentage_error(data['weekcase'][-len(predict):].values,predict[:])
#     return mape_list,mse,rmse,mae,mape,predict


##Yushu
#### Prepare for Esemble Model not used anymore
# def arima_res_xgb(ARMAmodel,data,train,test,fit_length,cv,training_period, overlap,confidence_interval, lag_reserve, resid,booster,learning_rate, n_estimators):
#     test_length = len(test)
#     data_length = len(train) + test_length
#     arima_mse, arima_rmse,arima_mae,arima_mape, arima_list,arima, arimap, arimaf = ARIMA_model(ARMAmodel,test,data_length,confidence_interval)
#     residual =  data.reset_index(drop = True).head(len(arima)).sub(arima.reset_index(drop = True),axis = 0) ## Residual = Real value - Fitted/Predicted
#     finaldf_train_resid_x,finaldf_test_resid_x,finaldf_train_resid_y,finaldf_test_resid_y, train_index, test_index = gen_ml(residual,test_length,cv, lag_reserve,resid)#I should pass the lag reserve amount here
#     xgb_resid = XGBRegressor(booster = booster, learning_rate = learning_rate, n_estimators = n_estimators).fit(finaldf_train_resid_x, finaldf_train_resid_y)
#     xgb_resid_predict1 = xgb_resid.predict(finaldf_test_resid_x) #Predictions on Testing data
#     A =(arima.reset_index(drop = True).tail(test_length) + xgb_resid_predict1)# Adding predicted residual on the predictted ARIMA value
#     xgb_resid_list= np.abs(A.reset_index(drop = True)- test.weekcase)/ test.weekcase
#     xgb_resid_p = A.reset_index(drop=True)
#     xgb_resid_f = pd.DataFrame(xgb_resid.predict(finaldf_train_resid_x))
#     xgb_resid_f = xgb_resid_f.add(pd.DataFrame(arima).tail(-overlap).reset_index(drop=True))
#     xgb_resid_f  = xgb_resid_f.head(-test_length)
#     xgb_resid_f = xgb_resid_f.tail(training_period).reset_index(drop=True)
#     #print('**xgb_resid_predict1')
#     xgb_resid = pd.concat([xgb_resid_f, xgb_resid_p], axis=0)

#     #print(xgb_resid_predict1.shape)
#     return xgb_resid, xgb_resid_predict1,arima, xgb_resid_list, xgb_resid_p, xgb_resid_f

def check_stationarity(time_series):
    """
    Performs the Augmented Dickey-Fuller (ADF) test to assess the stationarity of a time series.

    Parameters:
        time_series : pd.Series or array-like
            The time series data to be tested for stationarity.

    Prints:
        - ADF Statistic: The test statistic from the ADF test.
        - p-value: The significance level of the test.
        - Critical Values: Thresholds for different confidence levels (1%, 5%, 10%).
        - Interpretation of the result based on the p-value:
            - If p-value <= 0.05: Series is considered stationary.
            - If p-value > 0.05: Series is considered non-stationary.
    """
    result = adfuller(time_series)
    print('ADF Statistic:', result[0])
    print('p-value:', result[1])
    for key, value in result[4].items():
        print(f'Critical Value {key}:', value)
    if result[1] <= 0.05:
        print("The series is stationary.")
    else:
        print("The series is not stationary.")