# -*- coding: utf-8 -*-
"""Predictions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R_OWOjyFLje8iOhoCfpmJmMRDhOX6tLu
"""

from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX

from .Wave import Wave
from .Models import stats_data, ARIMA_model, ml_model, gen_ml, check_stationarity, generate_ml_features#arima_res_xgb,
from .Processor import process_metrics_df, create_prediction_df, replace_negatives
from .Weighted import find_diff, find_stan_optimal_weights, find_weighted_predictions

from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
import pandas as pd
import numpy as np

import copy

from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error

#Currently Prediction.py, I might move this to Models

#### Prediction on Wave 1,2,3 Data [Prediction 1,2,3,4,5,6 weeks]
#tuned_model_dict contains param for specific shift, wave all models
#generalized version
#MLAMA Package 2025 Data.ipynb

def wave_prediction(shift_type, wave_start_shift_matrix, predictions, prediction_length, shift, WAVE, trend_adjustment_steps, waveID,
                    training_period, lag_reserve, lagged_amount, confidence_interval, tuned_model_dict, models, predefined_wave = True):
    """
    Predicts weekly cases for multiple models dynamically.

    Args:
        shift_type: Flag for shift definition. 'Delay'/'Responsiveness'.
        predictions: List of prediction steps.
        prediction_length: Length of the prediction.
        shift: shift in weeks.
        WAVE: Wave data object.
        waveID: Identifier for the wave.
        training_period: Training period length.
        lag_reserve: Reserve for lag data.
        lagged_amount: How much past data is used.
        tuned_model_dict: Dictionary of tuned parameters for all models.
        models: Dictionary of models with their initialization functions. Example: {'ARIMA': SARIMAX, 'RF': RandomForestRegressor}.

    Returns:
        metric_row: Dictionary containing metrics and predictions for all models.
    """

    # Generate wave, train, and test data with shift
    wave, train, test = WAVE.get_wave_dates_with_shift(shift, wave_start_shift_matrix, shift_type)
    # To Do: I CAN GET WAVE TRAIN TEST FROM DATA_DICT AS WELL (DO IT LATER)
    single_train, full_train, fit_weeks_stat = stats_data(train) #single_ means only one feature weekcase is used
    single_test, full_test, pred_weeks_stat = stats_data(test.head(prediction_length))

    fit_pred_weeks_stat = pd.concat([fit_weeks_stat.tail(training_period), pred_weeks_stat], axis = 0).reset_index(drop = True)
    fit_pred_y_STAT = pd.concat([single_train[-training_period:], single_test], axis = 0).reset_index(drop = True)

    # Output metrics container
    metric_row = {}

    #### ARIMA Model
    if 'ARIMA' in models:
        arima_params = tuned_model_dict['ARIMA']

        ARMAmodel = SARIMAX(single_train,
                            order=arima_params['order'],
                            seasonal_order=arima_params['seasonal_order'])
        arima_mse, arima_rmse, arima_mae, arima_mape, arima_list, arima, arimap, arimaf = ARIMA_model(ARMAmodel, single_test,
                                                                                       training_period + prediction_length,
                                                                                       confidence_interval)

        arimap = arimap.reset_index(drop=True)
        arima = arima.reset_index(drop=True)

        metric_row['ARIMA'] = {
            'MSE': arima_mse,
            'RMSE': arima_rmse,
            'MAE': arima_mae,
            'MAPE': arima_mape,
            'Predictions': arimap,
            'Fit': arimaf,
            'List': arima_list,
            'Weeks': fit_pred_weeks_stat,
            'Observed': fit_pred_y_STAT,
            'Fit_Pred': arima
        }

    #### Generate ML Data
    adjustment = 0
    if shift_type=='Delay':
        adjustment = shift

    train_x, test_x, train_y, test_y, fit_weeks_ml, pred_weeks_ml = gen_ml(wave, prediction_length, lagged_amount,
                                                                           prediction_length, predictions, False, shift, trend_adjustment_steps, adjustment, predefined_wave=predefined_wave)
    fit_pred_weeks_ml = pd.concat([fit_weeks_ml.tail(training_period), pred_weeks_ml], axis=0).reset_index(drop=True)
    fit_pred_y_ML = pd.concat([train_y[-training_period:], test_y], axis=0).reset_index(drop=True)

    #### Machine Learning Models
    for model_name, model_class in models.items():
        if model_name == 'ARIMA':
            continue  # ARIMA is handled separately

        # Extract parameters for the current model
        tuned_params = tuned_model_dict.get(model_name, {})
        print(f"Training {model_name} with Parameters:", tuned_params)

        # Initialize and fit the model dynamically
        model = model_class(**tuned_params)
        model.fit(train_x, train_y)

        # Predictions and metrics
        predictions = model.predict(test_x)

        mse = mean_squared_error(test_y, predictions)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(test_y, predictions)
        list_mape = np.abs((test_y - predictions) / test_y)
        mape = np.mean(np.abs((test_y - predictions) / test_y)) * 100
        fit = model.predict(train_x)
        fit = pd.DataFrame(fit).reset_index(drop = True).tail(training_period)
        fit = fit.reset_index(drop=True)

        predictions = pd.DataFrame(predictions).reset_index(drop = True)

        fit_pred = pd.concat([fit,predictions],axis=0)

        fit_pred = fit_pred.reset_index(drop=True)

        # Store metrics
        metric_row[model_name] = {
            'MSE': mse,
            'RMSE': rmse,
            'MAE': mae,
            'MAPE': mape,
            'Predictions': predictions,
            'Fit': fit,
            'List': list_mape,
            'Weeks': fit_pred_weeks_ml,
            'Observed': fit_pred_y_ML,
            'Fit_Pred': fit_pred
        }

    return metric_row



#from MLAMA Package 2025 Data.ipynb
def seperate_model_eval(
    WAVES,
    models,
    predictions,
    trend_adjustment_steps,
    inFoldername_pre,
    filename,
    tuned_model_dict,
    shift_type,
    wave_start_shift_matrix,
    training_period,
    lag_reserve,
    lagged_amount,
    wave,
    confidence_interval,
    predefined_wave = True
):
    """
    Evaluates models across waves, shifts, and prediction lengths.

    Parameters:
        WAVES: list of wave objects.
        models: dict of model name -> model class.
        predictions: list of prediction lengths.
        trend_adjustment_steps: list of shift values.
        tuned_model_dict: dict mapping model/wave/shift combinations to parameter sets.
        Other parameters: control settings for training/testing and output.

    Returns:
        model_evaluation_dictionary: dict of evaluation metrics.
        concise_data: formatted results DataFrame.
    """
    #print(tuned_model_dict)
    model_evaluation_dictionary = {}

    for WAVE in WAVES:
        waveID = WAVE.waveID

        for prediction_length in predictions:
            for shift in trend_adjustment_steps:
                wave_shift_tuned_params = {}

                for model_name in models.keys():
                    wave_shift_model_key = f'wave {waveID} shift {shift}{model_name}'

                    wave_shift_tuned_params[model_name] = tuned_model_dict.get(wave_shift_model_key, {})

                metric_row = wave_prediction(
                    shift_type=shift_type,
                    wave_start_shift_matrix=wave_start_shift_matrix,
                    predictions=predictions,
                    prediction_length=prediction_length,
                    shift=shift,
                    WAVE=WAVE,
                    trend_adjustment_steps = trend_adjustment_steps,
                    waveID=waveID,
                    training_period=training_period,
                    lag_reserve=lag_reserve,
                    lagged_amount=lagged_amount,
                    confidence_interval=confidence_interval,
                    tuned_model_dict=wave_shift_tuned_params,
                    models=models,
                    predefined_wave = predefined_wave
                )

                key = f'wave {waveID} shift {shift} prediction_length {prediction_length}'
                model_evaluation_dictionary[key] = metric_row

    base_columns = ['Wave', 'Shift', 'Length', 'Observed']
    visualization_columns = base_columns + list(models.keys())

    task_name = 'all'
    concise_data, concise_data_dictionary = create_prediction_df(
        WAVES,
        predictions,
        trend_adjustment_steps,
        models,
        model_evaluation_dictionary,
        inFoldername_pre,
        task_name,
        filename
    )

    return model_evaluation_dictionary, concise_data

#generalized version
#predictions.py


from collections import defaultdict
import pandas as pd

def individual_model_weekly_count(WAVES, predictions, trend_adjustment_steps, model_evaluation_dictionary, models, recent_week_count):
    """
    Generalized function to process weekly counts for multiple models, storing results by model, wave, and shift.

    Args:
        WAVES (list): List of WAVE objects.
        predictions (list): List of prediction lengths.
        trend_adjustment_steps (list): List of shifts.
        model_evaluation_dictionary (dict): Dictionary containing model evaluation results.
        models (dict): Dictionary of model names and their corresponding classes.

    Returns:
        model_results (dict): A dictionary where keys are model names, and values are dictionaries
                              containing processed results for each WAVE and shift.
                              model_results[model_name][waveID][shift] = processed DataFrame
    """
    # Initialize dictionary to store results for each model, wave, and shift
    model_results = {model_name: defaultdict(dict) for model_name in models.keys()}

    for WAVE in WAVES:
        waveID = WAVE.waveID  # Extract wave ID

        for shift in trend_adjustment_steps:
            # Initialize storage for predictions of each model
            model_frames = {model_name: [] for model_name in models.keys()}
            # Initialize a dictionary to store the most recent fit_pred_weeks for each model
            most_recent_fit_pred_weeks = {model_name: None for model_name in models.keys()}

            for prediction_length in predictions:
                # Generate key for accessing the metrics
                wave_shift_prediction_key = f'wave {waveID} shift {shift} prediction_length {prediction_length}'

                # Check if key exists in the dictionary
                if wave_shift_prediction_key not in model_evaluation_dictionary:
                    print(f"Warning: Key {wave_shift_prediction_key} not found in model_evaluation_dictionary.")
                    continue

                metrics_row = model_evaluation_dictionary[wave_shift_prediction_key]

                # Extract results for each model dynamically
                for model_name in models.keys():
                    if model_name not in metrics_row:
                        print(f"Warning: Model {model_name} not found in metrics for {wave_shift_prediction_key}.")
                        continue

                    model_prediction = metrics_row[model_name]['Fit_Pred']
                    fit_pred_weeks = metrics_row[model_name]['Weeks']

                    if isinstance(model_prediction, pd.DataFrame):  # Ensure it's a DataFrame
                        model_prediction.columns = [f'{model_name}_{prediction_length}']
                        model_frames[model_name].append(model_prediction)

                        # Store the most recent fit_pred_weeks for each model
                        most_recent_fit_pred_weeks[model_name] = fit_pred_weeks
                    else:
                        print(f"Warning: Model {model_name} prediction is not a DataFrame for {wave_shift_prediction_key}.")

            # Append the most recent fit_pred_weeks for each model after the loop
            for model_name in models.keys():
                if most_recent_fit_pred_weeks[model_name] is not None:
                    model_frames[model_name].append(most_recent_fit_pred_weeks[model_name])

            # Process predictions for each model
            for model_name, frames in model_frames.items():
                if frames:  # Only process if there are valid frames
                    processed_df = process_metrics_df(WAVE, frames, recent_week_count)
                    #processed_df = processed_df.applymap(replace_negatives)
                    model_results[model_name][waveID][shift] = processed_df  # Store with shift key

    return model_results

#generalized
def individual_model_weekly_MAPE(WAVES, predictions, trend_adjustment_steps, model_evaluation_dictionary, models):
    """
    Generalized function for calculating weekly MAPE values for multiple models.

    Args:
        WAVES: List of wave objects, each with a unique waveID.
        predictions: List of prediction lengths.
        trend_adjustment_steps: List of shifts.
        model_evaluation_dictionary: Dictionary containing evaluation metrics for models.
        models: Dictionary mapping model names to their respective prediction keys in model_evaluation_dictionary.

    Returns:
        A dictionary with model names as keys and corresponding weekly MAPE dataframes as values.
    """
    # Initialize a dictionary to hold results for all models
    model_results_wave_collection = {model: {} for model in models}

    weeks_column = [x for x in predictions]

    #print(weeks_column)
    #print(predictions)

    for WAVE in WAVES:  # Process each wave
        waveID = WAVE.waveID

        # Initialize per-wave collections for each model
        model_results_collection = {model: {} for model in models}

        for shift in trend_adjustment_steps:  # Process each shift
            print('Shift:', shift)

            # Initialize per-shift lists for each model
            all_week_model_lists = {model: [] for model in models}

            for week in predictions:  # Process each week
                print('WEEK:', week)

                # Initialize per-week lists for each model
                week_model_lists = {model: [] for model in models}

                for prediction_length in predictions:  # Process each prediction length
                    print('PREDICTION LENGTH:', prediction_length)

                    # Initialize weekly values for each model
                    weekly_model_shift_values = {model: np.nan for model in models}

                    if prediction_length >= week:
                        # Construct the key to fetch metrics
                        wave_shift_prediction_length_key = f'wave {WAVE.waveID} shift {shift} prediction_length {prediction_length}'
                        metrics_row = model_evaluation_dictionary[wave_shift_prediction_length_key]
                        #print(metrics_row)
                        for model_name in models.keys():
                            model_list = metrics_row[model_name]['List']
                            print(model_name, model_list)
                            # Calculate weekly value
                            weekly_model_shift_values[model_name] = (
                                np.array(model_list)[week-1] / prediction_length * 100
                            )

                    # Append weekly values to the week lists for each model
                    for model in models:
                        week_model_lists[model].append(weekly_model_shift_values[model])

                # Append week lists to the all-week lists for each model
                for model in models:
                    all_week_model_lists[model].append(week_model_lists[model])

            # Create dataframes for all-week lists and store them in the shift collections
            for model in models:
                model_results_collection[model][shift] = pd.DataFrame(
                    [all_week_model_lists[model]], columns=weeks_column
                )

        # Store shift collections in the wave collections
        for model in models:

            model_results_wave_collection[model][waveID] = model_results_collection[model]

    return model_results_wave_collection

def find_prediction_current_week(trained_models, current_week_data, current_week_data_ml, lagged_amount, confidence_interval=0.05):
    """
    Generates predictions for the current week for a specific shift and prediction length.

    Parameters:
    - trained_models (dict): Dictionary containing trained models for a specific shift and prediction length.
    - current_week_data (DataFrame): Current week's observed data.
    - current_week_data_ml (DataFrame): ML feature dataset for the current week.
    - lagged_amount (int): Number of lagged features to use.
    - confidence_interval (float): Confidence interval for ARIMA predictions.

    Returns:
    - DataFrame: Predictions for the given shift and prediction length.
    """
    current_week_features, data_index = generate_ml_features(current_week_data_ml[['weekcase']], lagged_amount)
    current_week_features = current_week_features.drop(columns=['weekcase'])

    current_week_prediction = {}

    # Ensure ARIMA is always predicted first
    if 'ARIMA' in trained_models:
        ARMAmodel = trained_models['ARIMA']
        ARMAmodel = ARMAmodel.fit()  # Fit the trained model
        arima_pred = ARMAmodel.get_forecast(current_week_data.shape[0])  # Forecast

        arima_pred_df = arima_pred.conf_int(alpha=confidence_interval)
        arima_pred_df["Predictions"] = ARMAmodel.predict(start=arima_pred_df.index[0], end=arima_pred_df.index[-1])

        arima_pred_df.index = current_week_data.index
        current_week_prediction['ARIMA'] = arima_pred_df["Predictions"]
        #print(f"ARIMA Prediction: {current_week_prediction['ARIMA']}")

    # Predict using all available ML models
    for model_name, model in trained_models.items():
        if model_name == 'ARIMA':
            continue  # Skip ARIMA as it's already handled

        ml_pred = model.predict(current_week_features)
        current_week_prediction[model_name] = ml_pred

    # Convert predictions to DataFrame
    current_week_prediction_df = pd.DataFrame([current_week_prediction])

    # Insert metadata columns
    current_week_prediction_df.insert(0, 'Week', current_week_data['Week'].values[0])
    current_week_prediction_df.insert(1, 'Observed', current_week_data['weekcase'].values[0])

    return current_week_prediction_df

#from MLAMA Package 2025 Data.ipynb

def wave_prediction_with_models(shift_type, wave_start_shift_matrix, trend_adjustment_steps, predictions, prediction_length, shift, WAVE, waveID,
                    training_period, lag_reserve, lagged_amount, overlap, confidence_interval, tuned_model_dict, models):
    """
    Predicts weekly cases for multiple models dynamically and returns trained models.
    #get rid of overlap, not needed
    Args:
        shift_type: Flag for shift definition. True=Yushu, False=JMM.
        predictions: List of prediction steps.
        prediction_length: Length of the prediction.
        shift: shift in weeks.
        WAVE: Wave data object.
        waveID: Identifier for the wave.
        training_period: Training period length.
        lag_reserve: Reserve for lag data.
        lagged_amount: How much past data is used.
        tuned_model_dict: Dictionary of tuned parameters for all models.
        models: Dictionary of models with their initialization functions.

    Returns:
        metric_row: Dictionary containing metrics and predictions for all models.
        trained_models: Dictionary containing trained models.
    """
    # Generate wave, train, and test data with shift
    wave, train, test = WAVE.get_wave_dates_with_shift(shift, wave_start_shift_matrix, shift_type)

    single_train, full_train, fit_weeks_stat = stats_data(train)
    single_test, full_test, pred_weeks_stat = stats_data(test.head(prediction_length))

    fit_pred_weeks_stat = pd.concat([fit_weeks_stat.tail(training_period), pred_weeks_stat], axis=0).reset_index(drop=True)
    fit_pred_y_STAT = pd.concat([single_train[-training_period:], single_test], axis=0).reset_index(drop=True)

    # Output containers
    metric_row = {}
    trained_models = {}  # Store trained models

    #### ARIMA Model
    if 'ARIMA' in models:
        arima_params = tuned_model_dict['ARIMA']

        ARMAmodel = SARIMAX(single_train,
                            order=arima_params['order'],
                            seasonal_order=arima_params['seasonal_order'])
        arima_mse, arima_rmse, arima_mae, arima_mape, arima_list, arima, arimap, arimaf = ARIMA_model(
            ARMAmodel, single_test, training_period + prediction_length, confidence_interval
        )

        # Store trained ARIMA model

        trained_models['ARIMA'] = ARMAmodel

        metric_row['ARIMA'] = {
            'MSE': arima_mse,
            'RMSE': arima_rmse,
            'MAE': arima_mae,
            'MAPE': arima_mape,
            'Predictions': arimap.reset_index(drop=True),
            'Fit': arimaf.reset_index(drop=True),
            'List': arima_list,
            'Weeks': fit_pred_weeks_stat,
            'Observed': fit_pred_y_STAT,
            'Fit_Pred': arima.reset_index(drop=True)
        }

    #### Generate ML Data
    adjustment = shift if shift_type == 'Delay' else 0

    train_x, test_x, train_y, test_y, fit_weeks_ml, pred_weeks_ml = gen_ml(
        wave=wave,
        test_length=prediction_length,
        parameter_length=lagged_amount,
        lag_reserve=prediction_length,
        predictions=predictions,
        resid=False,
        shift=shift,
        trend_adjustment_steps=trend_adjustment_steps,
        adjustment=adjustment,
        predefined_wave=False,
        current_week_prediction = True,
        current_week_length=len(test)
    )

    fit_pred_weeks_ml = pd.concat([fit_weeks_ml.tail(training_period), pred_weeks_ml], axis=0).reset_index(drop=True)
    fit_pred_y_ML = pd.concat([train_y[-training_period:], test_y], axis=0).reset_index(drop=True)

    #### Machine Learning Models
    for model_name, model_class in models.items():
        if model_name == 'ARIMA':
            continue  # ARIMA is handled separately

        # Extract parameters for the current model
        tuned_params = tuned_model_dict.get(model_name, {})

        # Initialize and fit the model
        model = model_class(**tuned_params)
        model.fit(train_x, train_y)

        # Store trained model
        trained_models[model_name] = model

        # Predictions and metrics
        predictions = model.predict(test_x)
        mse = mean_squared_error(test_y, predictions)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(test_y, predictions)
        mape = np.mean(np.abs((test_y - predictions) / test_y)) * 100
        list_mape = np.abs((test_y - predictions) / test_y)

        fit = model.predict(train_x)
        fit = pd.DataFrame(fit).reset_index(drop = True).tail(training_period)
        fit = fit.reset_index(drop=True)

        predictions = pd.DataFrame(predictions).reset_index(drop = True)
        fit_pred = pd.concat([fit,predictions],axis=0)
        fit_pred = fit_pred.reset_index(drop=True)

        metric_row[model_name] = {
            'MSE': mse,
            'RMSE': rmse,
            'MAE': mae,
            'MAPE': mape,
            'Predictions': predictions,
            'Fit': fit,
            'List': list_mape,
            'Weeks': fit_pred_weeks_ml,
            'Observed': fit_pred_y_ML,
            'Fit_Pred': fit_pred
        }

    return metric_row, trained_models  # Now also returns trained models

##from MLAMA Package 2025 Data.ipynb
def forecast_current_weeks(
    history_df,
    training_window,
    forecast_df,
    tuned_hyperparameter,
    stan_optimal_weights,
    predictions,
    models,
    lagged_amount,
    training_period,
    lag_reserve,
    overlap,
    confidence_interval,
    inFoldername_pre,
    filename,
    observed_column_name,
    shift=0
):
    """
    Forecasts multiple future weeks using retrained models on expanding training windows.

    For each week in the forecast period, the model is retrained using the most recent
    training window. Predictions are made using various models, and ensemble predictions
    are generated using optimal weights. Forecasting is done across various prediction
    lengths and shift values.

    Parameters:
    -----------
    history_df : pd.DataFrame
        Historical time series data used for training the models.

    training_window : int
        Number of past observations to use in the training window for model retraining.

    forecast_df : pd.DataFrame
        DataFrame of weeks to forecast. Must include columns compatible with `history_df`.

    tuned_hyperparameter : dict
        Dictionary containing model-specific hyperparameters tuned previously.

    stan_optimal_weights : dict
        Dictionary of optimal weights for combining model forecasts.

    predictions : list of int
        List of prediction lengths (horizons) for which forecasts will be generated.

    models : dict
        Dictionary of model instances used for forecasting, keyed by model name.

    lagged_amount : int
        Number of past values (lags) used as features for machine learning models.

    training_period : int
        Length of the training period used in modeling (e.g., for ARIMA fitting).

    lag_reserve : int
        Reserved lag space used when building training and test datasets.

    overlap : bool
        Whether to allow overlapping prediction windows during evaluation.

    confidence_interval : float
        Confidence level for interval estimation in statistical models (e.g., ARIMA).

    inFoldername_pre : str
        Path prefix or folder name used for saving or accessing intermediate files.

    filename : str
        File name used when writing out or reading prediction outputs.

    observed_column_name : str
        Name of the target column containing observed values (e.g., "weekcase").

    shift : int, optional (default=0)
        The shift or delay value used in evaluating responsiveness of models.

    Returns:
    --------
    weekly_prediction_future : dict
        Dictionary mapping forecasted weeks to prediction DataFrames. Each entry contains
        the ensemble prediction and individual model outputs for all configured shifts and
        prediction lengths.
    """

    trend_adjustment_steps = [0]  # trend_adjustment_steps not needed for model training, it's not condition specific. it's generic model training


    # I have separate train and test df, I frst concat them into one dataframe, and feed that into previously defined function
    train_df = history_df.tail(training_window).copy()
    test_df = forecast_df.copy()

    # Concatenate train and test dataframes
    df = pd.concat([train_df, test_df])
    # Convert index to datetime if it's not already
    # Define overall date range
    start_week = df.index.min()
    end_week = df.index.max()

    # Define training date range
    train_start_week = train_df.index.min()
    train_end_week = train_df.index.max()

    # Define testing date range
    test_start_week = test_df.index.min()
    test_end_week = test_df.index.max()
    # all will be used for training the model, do not need separate train/test here

    wave_history_r0 = Wave(0, start_week, end_week, train_start_week, train_end_week, test_start_week, test_end_week, trend_adjustment_steps, df)
    waveID_history = wave_history_r0.waveID
    wave_start_shift_matrix = {
        str(waveID_history): [0],
    }

    tune_list = list(tuned_hyperparameter.keys())
    tuned_model_names = list(models.keys())

    weekly_prediction_future = {}
    all_predictions = []  # List to store individual DataFrames

    for index, row in forecast_df.iterrows():
        model_evaluation_dictionary = {}
        trained_models_dictionary = {}

        week = pd.to_datetime(index).strftime('%Y-%m-%d')
        row_df = pd.DataFrame([row], index=[index])
        wave_history_df = wave_history_r0.get_wave_df()

        current_week_data = pd.DataFrame({'Week': [week], 'weekcase': [row['weekcase']]})

        lagged_data = wave_history_df.loc[:index].tail(lagged_amount + 1)
        lagged_data = wave_history_df.loc[:index].iloc[:-1].tail(lagged_amount)

        if week in lagged_data.index:
            continue
        if index in lagged_data.index:
            continue
        if len(lagged_data) < lagged_amount:
            print(f"Not enough historical data for ML predictions at {week}. Required: {lagged_amount}, Available: {len(lagged_data)}")
            current_week_data_ml = None
            wave_history_r0.slide_one_week()
            continue
        else:
            current_week_data_ml = lagged_data.copy()
            current_week_data_ml = current_week_data_ml[['weekcase']]

        shift_type = 'Delay'

        for prediction_length in predictions:
            wave_shift_tuned_params = {}

            for shift in trend_adjustment_steps:
                for tuned_model_name in tuned_model_names:
                    hyperparmeter_key = f'current_week{tuned_model_name}'
                    wave_shift_tuned_params[tuned_model_name] = tuned_hyperparameter[hyperparmeter_key]

                metric_row, trained_models = wave_prediction_with_models(
                    shift_type, wave_start_shift_matrix, trend_adjustment_steps, predictions,
                    prediction_length, shift, wave_history_r0, waveID_history,
                    training_period, lag_reserve, lagged_amount, overlap,
                    confidence_interval, wave_shift_tuned_params, models)

                wave_shift_prediction_length_key = f'wave {waveID_history} shift {shift} prediction_length {prediction_length}'

                model_evaluation_dictionary[wave_shift_prediction_length_key] = metric_row
                trained_models_dictionary[wave_shift_prediction_length_key] = trained_models

        base_columns = ['Wave', 'Shift', 'Length', 'Observed']
        visualization_columns = base_columns + list(models.keys())
        task_name = 'weekly'

        concise_data, concise_data_dictionary = create_prediction_df(
            [wave_history_r0], predictions, trend_adjustment_steps, models, model_evaluation_dictionary,
            inFoldername_pre, task_name, filename)

        diff_data = find_diff(concise_data, models, observed_column_name)

        theDD, theD_long, current_week_prediction_MLAMA_df = find_weighted_predictions(
            diff_data, stan_optimal_weights, observed_column_name, models)

        all_predictions.append(current_week_prediction_MLAMA_df)

        weekly_prediction_future[week] = current_week_prediction_MLAMA_df

        wave_history_r0.add_one_week(row_df)

    return weekly_prediction_future

def model_eval_on_future(
    WAVE_history, WAVES_future, model_update, weight_update, models, observed_column_name,
    predictions, tuned_model_names, tuned_model_dict, wave_start_shift_matrix, training_period, lag_reserve,
    lagged_amount, overlap, confidence_interval, inFoldername_pre, filename):

    model_evaluation_dictionary = {}
    trained_models_dictionary = {}
    weekly_prediction_future = {}

    waveID_history = WAVE_history.waveID
    WAVE_history = copy.deepcopy(WAVE_history)

    for wave in WAVES_future:
        waveID_future = wave.waveID
        wave_df = wave.get_wave_df()

        for index, row in wave_df.iterrows():
            week = pd.to_datetime(index).strftime('%Y-%m-%d')
            row_df = pd.DataFrame([row], index=[index])
            wave_history_df = WAVE_history.get_wave_df()
            # Create a DataFrame with Week and weekcase
            current_week_data = pd.DataFrame({'Week': [week], 'weekcase': [row['weekcase']]})


            # Get the previous 'lagged_amount' rows
            lagged_data = wave_history_df.loc[:index].tail(lagged_amount+1)  # Ensure we get the latest 'lagged_amount' rows

            lagged_data = lagged_data[~lagged_data.index.duplicated(keep='first')]  # Remove duplicate indices


            if week in lagged_data.index:

                continue
            if index in lagged_data.index:
                #print(index, 'in lagged data index')
                continue
            if len(lagged_data) < lagged_amount+1:#what I can do is, start from history wave to get the ML data
                print(f"Not enough historical data for ML predictions at {week}. Required: {lagged_amount}, Available: {len(lagged_data)}")
                current_week_data_ml = None  # ML predictions can't be done
                WAVE_history.add_one_week(row_df)
                continue
            else:
                # Include lagged data in the new DataFrame
                current_week_data_ml = lagged_data.copy()

                current_week_data_ml = current_week_data_ml[['weekcase']]  # Keep only relevant columns

                #current_week_data_ml = current_week_data_ml.rename(columns={'week': 'Week'})
            #print('current_week_data_ml....', current_week_data_ml)
            trend_adjustment_steps = [0]
            shift_type = 'Delay'

            for prediction_length in predictions:
                wave_shift_tuned_params = {}

                for shift in trend_adjustment_steps:  # Iterate over all shift values
                    for tuned_model_name in tuned_model_names:
                        wave_shift_model_key = f'wave {waveID_history} shift {shift}{tuned_model_name}'
                        wave_shift_tuned_params[tuned_model_name] = tuned_model_dict[wave_shift_model_key]

                    # Perform wave prediction

                    metric_row, trained_models = wave_prediction_with_models(
                        shift_type, wave_start_shift_matrix, predictions,
                        prediction_length, shift, WAVE_history, waveID_history,
                        training_period, lag_reserve, lagged_amount, overlap,
                        confidence_interval, wave_shift_tuned_params, models)

                    wave_shift_prediction_length_key = f'wave {waveID_history} shift {shift} prediction_length {prediction_length}'

                    # Store evaluation metrics and trained models
                    model_evaluation_dictionary[wave_shift_prediction_length_key] = metric_row
                    trained_models_dictionary[wave_shift_prediction_length_key] = trained_models

            base_columns = ['Wave', 'Shift', 'Length', 'Observed']
            visualization_columns = base_columns + list(models.keys())
            #derive weights for history data
            task_name = 'weekly'
            concise_data, concise_data_dictionary = create_prediction_df(
                [WAVE_history], predictions, trend_adjustment_steps, models, model_evaluation_dictionary,
                inFoldername_pre, task_name, filename)

            diff_data = find_diff(concise_data, models, observed_column_name)

            stan_optimal_weights = find_stan_optimal_weights(
                diff_data, predictions, trend_adjustment_steps, observed_column_name, models)

            # Predict the current week's data
            all_predictions = []  # List to store individual DataFrames
            current_week_predictions = {}  # Dictionary to store predictions


            for prediction_length in predictions:
                for shift in trend_adjustment_steps:
                    wave_shift_prediction_length_key = f'wave {waveID_history} shift {shift} prediction_length {prediction_length}'
                    print(wave_shift_prediction_length_key)

                    if wave_shift_prediction_length_key not in trained_models_dictionary:
                        print(f"Warning: No trained models found for {wave_shift_prediction_length_key}")
                        continue

                    trained_models = trained_models_dictionary[wave_shift_prediction_length_key]
                    #here i update the the current_week_data, and current_week_data_ml as well

                    future_weeks = wave_df.loc[index:].head(prediction_length)  # Selects the next n weeks

                    future_weeks_df = future_weeks.reset_index()[['week', 'weekcase']].rename(columns={'week': 'Week'})
                    current_prediction_week = future_weeks_df.tail(1)


                    # Call function to get predictions
                    current_week_prediction_df = find_prediction_current_week(
                        trained_models, current_prediction_week, current_week_data_ml, lagged_amount
                    )

                    # Add metadata
                    current_week_prediction_df.insert(2, 'Wave', waveID_history)
                    current_week_prediction_df.insert(3, 'Shift', shift)
                    current_week_prediction_df.insert(4, 'Length', prediction_length)

                    # Store results
                    all_predictions.append(current_week_prediction_df)


            # Concatenate all results into a single DataFrame
            if all_predictions:
                current_week_predictions = pd.concat(all_predictions, ignore_index=True)
            else:
                current_week_predictions = pd.DataFrame()

            diff_data_future = find_diff(current_week_predictions, models, observed_column_name)

            theDD, theD_long, current_week_prediction_MLAMA_df = find_weighted_predictions(
                diff_data_future, stan_optimal_weights, observed_column_name, models)

            # Store the result in the dictionary
            weekly_prediction_future[week] = current_week_prediction_MLAMA_df#for this week all shift, length should be here
            # Append current week's data to historical WAVE data
            WAVE_history.add_one_week(row_df)

    return weekly_prediction_future