# -*- coding: utf-8 -*-
"""GridSearch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BVqTLEJnqhQUIeFdt23d1GtBEOgo51AN
"""

from .Wave import Wave
from .Models import stats_data, ARIMA_model, ml_model, gen_ml, check_stationarity #, arima_res_xgb
from .Processor import TimeSeriesSplit, extract_shift_from_key

# Commented out IPython magic to ensure Python compatibility.
### Import Library
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings("ignore")
from sklearn.model_selection import GridSearchCV#TimeSeriesSplit,
from sklearn.metrics import mean_squared_error, mean_absolute_error
from math import sqrt
from statsmodels.tsa.statespace.sarimax import SARIMAX
import xgboost as xgb
from xgboost.sklearn import XGBRegressor
from sklearn.ensemble import RandomForestRegressor
# %matplotlib inline
pd.set_option('display.max_rows', None)
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.model_selection import TimeSeriesSplit
from tqdm import tqdm
import datetime
import os.path
from os import path
from pmdarima.arima import auto_arima



import time
import re

import os
import json
import joblib
import pandas as pd
from sklearn.model_selection import GridSearchCV

#Grid Search.py
#from MLAMA Package 2025 Data.ipynb
def tune_models_for_one_condition(
    key,
    tuned_key_prefix,
    models,
    cv,
    lagged_amount,
    data,
    output_folder,
    tuned_on_filename,
    default_params,
    trend_adjustment_steps=[0],
    output_filename_template="{key}_{model_name}_results_{tuned_on_filename}.json",
    custom_grids=None,
    custom_arima_params=None,
    skip_existing_files=True
):
    """
    Tunes all models for a given condition (wave/shift).

    Parameters:
    -----------
    key : str
        Identifier for the data condition.
    tuned_key_prefix : str
        Prefix for the tuned model keys.
    models : dict
        Dictionary mapping model names to their class or constructor.
    cv : object
        Cross-validation splitter object.
    lagged_amount : int
        Number of lagged features to use for ML models.
    data : pd.DataFrame
        Input data for model tuning.
    output_folder : str
        Directory to save tuning outputs.
    tuned_on_filename : str
        Filename identifier used for storing results.
    default_params : dict
        Dictionary containing default parameters for each model.
    trend_adjustment_steps : list, optional
        List of integers indicating trend adjustment steps.
    output_filename_template : str, optional
        Template string for output filenames.
    custom_grids : dict, optional
        Custom parameter grids for specific models.
    custom_arima_params : dict, optional
        Parameters to pass to ARIMA model tuning.
    skip_existing_files : bool, optional
        If True, skip tuning if output already exists.

    Returns:
    --------
    tuple : (tuned_model_dict, input_params_dict)
    """
    tuned_model_dict = {}
    input_params_dict = {}

    for model_name, model_class in models.items():
        tuned_key = f"{tuned_key_prefix}{model_name}"
        parsed_params = {}

        if model_name.upper() == 'ARIMA':
            arima_result = run_auto_arima_for_each_key(
                data_dict={key: data},
                output_folder=output_folder,
                tuned_on_filename=tuned_on_filename,
                model_name=model_name,
                skip_existing_files=skip_existing_files,
                auto_arima_params=custom_arima_params,
                output_filename_template=output_filename_template
            )
            parsed_params = arima_result.get(key, {})
        else:
            custom_grid = custom_grids.get(model_name) if custom_grids else None
            if custom_grid:
                best_models, best_params = run_grid_search_for_each_key(
                    model_class=model_class,
                    model_name=model_name,
                    cv=cv,
                    param_grid=custom_grid,
                    lagged_amount=lagged_amount,
                    data_dict={key: data},
                    trend_adjustment_steps=trend_adjustment_steps,
                    output_folder=output_folder,
                    tuned_on_filename=tuned_on_filename,
                    output_filename_template=output_filename_template,
                    skip_existing_files=skip_existing_files
                )
                parsed_params = best_params.get(key, {})
            else:
                parsed_params = default_params.get(model_name, {})

        if isinstance(parsed_params, dict) and "output_params" in parsed_params and "input_params" in parsed_params:
            tuned_model_dict[tuned_key] = parsed_params["output_params"]
            input_params_dict[tuned_key] = parsed_params["input_params"]
        else:
            tuned_model_dict[tuned_key] = parsed_params
            input_params_dict[tuned_key] = None

    return tuned_model_dict, input_params_dict


def tune_models_for_all_conditions(
    WAVES,
    trend_adjustment_steps,
    predictions,
    lagged_amount,
    data_dict,
    models,
    default_params,
    tuned_on_filename,
    output_folder,
    output_filename_template="{key}_{model_name}_results_{tuned_on_filename}.json",
    custom_grids=None,
    custom_arima_params=None,
    cv=None,
    skip_existing_files=True
):
    """
    Tunes all models for each wave and shift condition.

    Parameters:
    -----------
    WAVES : list
        List of wave objects containing waveID.
    trend_adjustment_steps : list
        List of integers indicating shift values for trend adjustment (Responsiveness specific).
    predictions : list
        List of prediction step sizes.
    lagged_amount : int
        Number of lagged features to use.
    data_dict : dict
        Dictionary mapping condition keys to their corresponding data.
    models : dict
        Dictionary mapping model names to their class or constructor.
    default_params : dict
        Dictionary of default parameters per model.
    tuned_on_filename : str
        Identifier for output file naming.
    output_folder : str
        Path to the directory for saving model results.
    output_filename_template : str, optional
        Template string for output filenames.
    custom_grids : dict, optional
        Dictionary of custom hyperparameter grids per model.
    custom_arima_params : dict, optional
        Custom parameters for ARIMA tuning.
    cv : object, optional
        Cross-validation splitter object.
    skip_existing_files : bool, optional
        If True, skip tuning if result files already exist.

    Returns:
    --------
    tuple : (tuned_model_dict, input_params_dict)
    """
    tuned_model_dict = {}
    input_params_dict = {}

    for wave in WAVES:
        waveID = wave.waveID

        for shift in trend_adjustment_steps:
            data_dict_key = f"wave {waveID}_shift {shift}"
            key = f"wave {waveID} n{cv.n_splits}_t{cv.test_size}_shift {shift}"
            tuned_key_prefix = f"wave {waveID} shift {shift}"

            if data_dict_key not in data_dict:
                continue

            tuned_partial, input_partial = tune_models_for_one_condition(
                key=key,
                tuned_key_prefix=tuned_key_prefix,
                models=models,
                cv=cv,
                lagged_amount=lagged_amount,
                data=data_dict[data_dict_key],
                trend_adjustment_steps=trend_adjustment_steps,
                tuned_on_filename=tuned_on_filename,
                output_folder=output_folder,
                output_filename_template=output_filename_template,
                default_params=default_params,
                custom_grids=custom_grids,
                custom_arima_params=custom_arima_params,
                skip_existing_files=skip_existing_files
            )

            tuned_model_dict.update(tuned_partial)
            input_params_dict.update(input_partial)

    return tuned_model_dict, input_params_dict


def hyperparameter_tuning_forecast_current_weeks(
    df,
    training_window,
    tscv,
    lagged_amount,
    models,
    output_folder,
    tuned_on_filename,
    default_params,
    custom_grids,
    custom_arima_params,
    skip_existing_files=True
):
    """
    Performs hyperparameter tuning for forecasting models using the most recent training window.

    This function tunes model hyperparameters on the most recent `training_window`-sized
    subset of the data (`df`). It prepares the data in a generic form (not tied to any
    particular scenario) and uses the specified cross-validation strategy for tuning.

    Uses the same tune_models_for_one_condition function as other tuning

    Parameters:
    -----------
    df : pd.DataFrame
        Full time series dataset containing the target variable ('weekcase').
    training_window : int
        Number of most recent time points to use for training and hyperparameter tuning.
    tscv : sklearn.model_selection.TimeSeriesSplit
        Time series cross-validator object for splitting data into training/validation folds.
    models : dict
        Dictionary of model names mapped to their respective class constructors.
    output_folder : str
        Path to the directory where tuning result files will be stored.
    tuned_on_filename : str
        Identifier string to distinguish tuning runs (e.g., dataset name or date).
    default_params : dict
        Dictionary of default hyperparameters for each model.
    custom_grids : dict
        Dictionary of hyperparameter grids to use for each model during grid search.
    custom_arima_params : dict
        Dictionary of parameters for ARIMA tuning via auto_arima.
    skip_existing_files : bool, optional (default=True)
        Whether to skip tuning if result files already exist.

    Returns:
    --------
    dict
        A dictionary containing the tuned hyperparameters for each model.
    """
    # tune on the training_window chunk only, tune_df, generic hyperparameter tuning, not any scenario specific
    tune_df = df.tail(training_window).copy()
    data_dict = {}
    data_dict['train'] = tune_df.copy()

    # print(data_dict['train']['weekcase'])

    key = f"current_week_n{tscv.n_splits}_t{tscv.test_size}_w{training_window}"  # I can also use from to date here?
    tuned_key_prefix = 'current_week'
    output_filename_template = "{key}_{model_name}_results_{tuned_on_filename}.json"
    tuned_forecast, _ = tune_models_for_one_condition(
        key=key,
        tuned_key_prefix=tuned_key_prefix,
        models=models,
        cv=tscv,
        lagged_amount=lagged_amount,
        data=data_dict,
        tuned_on_filename=tuned_on_filename,
        output_folder=output_folder,
        output_filename_template=output_filename_template,
        default_params=default_params,
        custom_grids=custom_grids,
        custom_arima_params=custom_arima_params,
        skip_existing_files=skip_existing_files
    )

    return tuned_forecast

#from MLAMA Package 2025 Data.ipynb
def get_wave_data_dictionary(WAVES, trend_adjustment_steps, wave_start_shift_matrix, shift_type, max_prediction_length):
    """
    Constructs a dictionary containing wave data with the different shift values.

    Parameters:
        WAVES (list): A list of wave objects, each with a method `get_wave_dates_with_shift`.
        trend_adjustment_steps (list): A list of shift values to apply to each wave.
        wave_start_shift_matrix(dict): A dictionary mapping wave IDs to lists of shifted start values.
        shift_type: The definition or object specifying the shift settings for each wave.
        max_prediction_length: maximum forcast length

    Returns:
        dict: A dictionary with keys in the format 'wave {waveID} shift {shift}', containing data for each wave and shift.
    """
    data_dict = {}

    # Loop through each wave and shift to populate the dictionary
    for wave in WAVES:
        for shift in trend_adjustment_steps:
            # Get wave data, train data, and test data for each shift using get_wave_dates_with_shift
            wave_data, train_data, test_data = wave.get_wave_dates_with_shift(shift, wave_start_shift_matrix, shift_type)

            # Construct the key for each combination of wave and shift
            key = f'wave {wave.waveID}_shift {shift}'# n{n_splits}_t{test_size}

            # Populate the dictionary with the data for each wave and shift
            data_dict[key] = {
                'all': wave_data,
                'train': train_data,
                'test': test_data,
                'shift': shift,
                'max_prediction_length': max_prediction_length
            }

    return data_dict

def run_auto_arima_for_each_key(
    data_dict,
    output_folder,
    tuned_on_filename,
    model_name='ARIMA',
    skip_existing_files=False,
    output_filename_template='{key}_{model_name}_results_{tuned_on_filename}.json',
    auto_arima_params=None
):
    """
    Run the auto-ARIMA model for each dataset in the provided dictionary
    and save the ARIMA parameters and user-defined parameters to JSON files.

    Parameters:
        data_dict (dict): Dictionary containing training and testing data for each dataset.
        output_folder (str): Folder path where the JSON files will be saved.
        tuned_on_filename (str): Filename for the data the models are tuned on.
        model_name (str): Name of the model (default: 'ARIMA').
        skip_existing_files (bool): Whether to skip datasets with existing results (default: False).
        output_filename_template (str): Template for the output filenames.
        auto_arima_params (dict): Optional dictionary of parameters to pass to auto_arima.

    Returns:
        dict: A dictionary containing the ARIMA parameters for each dataset.
    """
    if auto_arima_params is not None:
        print('Custom auto_arima parameter:', auto_arima_params)
    else:
        print('Default auto_arima parameter')

    os.makedirs(output_folder, exist_ok=True)
    arima_results = {}

    for key, data in tqdm(data_dict.items(), desc="Running auto-ARIMA"):
        print(f"Processing dataset: {key}")

        output_filename = output_filename_template.format(
            key=key, model_name=model_name, tuned_on_filename=tuned_on_filename
        )
        result_path_all = os.path.join(output_folder, output_filename)
        result_path_best = os.path.join(output_folder, f'best_params_{output_filename}')

        if skip_existing_files and os.path.exists(result_path_best) and os.path.exists(result_path_all):
            print(f"Skipping {key}: results already exist at {result_path_best}")
            with open(result_path_best, "r") as file:
                arima_results[key] = json.load(file)
                print(f"ARIMA Parameters for {key}: {arima_results[key]}")
            continue

        #changed all to train
        arima_train = pd.DataFrame(data['train']['weekcase'])

        print(f"Fitting auto-ARIMA for dataset: {key}")
        if auto_arima_params is not None:
            model = auto_arima(arima_train, **auto_arima_params)
        else:
            model = auto_arima(arima_train)

        arima_parameters = model.get_params()
        print(f"ARIMA Parameters for {key}: {arima_parameters}")

        result_dict = {
            "output_params": arima_parameters,
            "input_params": auto_arima_params if auto_arima_params is not None else "default"
        }

        for path in [result_path_best, result_path_all]:
            with open(path, "w") as file:
                json.dump(result_dict, file, indent=4)
            print(f"Saved ARIMA result (input & output parameters) to: {path}")

        arima_results[key] = result_dict

    return arima_results

# #changed all to train # not needed. not used anywhere
# def stat_train(data):
#   return pd.DataFrame(data['train'].weekcase)

#Grid Search.py from MLAMA Package 2025 Data.ipynb
def run_grid_search(
    model,
    cv,
    param_grid,
    lagged_amount,
    key,
    data,
    trend_adjustment_steps,
    output_folder,
    output_filename,
    scoring='neg_mean_absolute_percentage_error',
    use_random_state=None,
    custom_steps=None,
    skip_existing_files=True,
    auto_generated_wave = True
):
    """
    Generalized function for grid search on any model.

    Parameters:
        model: Machine learning model instance.
        cv: Cross-validation object.
        param_grid: Dictionary of hyperparameters for grid search.
        lagged_amount: Number of lagged points to include in the dataset.
        key: Unique identifier for the current search.
        data: Dictionary containing training and testing data splits.
        trend_adjustment_steps: A list of shift values to apply to each wave.
        output_folder: Folder to save results.
        output_filename: Filename for saving grid search results.
        scoring: Scoring metric for grid search.
        use_random_state: Boolean; if True, sets model.random_state = 42.
        custom_steps: Optional preprocessing function.
        skip_existing_files: If True, skip if output already exists.
        auto_generated_wave: User defined (False) or auto generated (True)

    Returns:
        best_model: Trained model with the best parameters from the grid search.
        best_params: Dictionary of the best hyperparameters.
    """
    os.makedirs(output_folder, exist_ok=True)
    result_path_all = os.path.join(output_folder, output_filename)
    result_path_best = os.path.join(output_folder, f'best_params_{output_filename}')
    model_filename = os.path.join(output_folder, f'model_{output_filename}')

    # Check if already done
    if skip_existing_files:
        if os.path.exists(result_path_best) and os.path.exists(model_filename) and os.path.exists(result_path_all):
            print(f"{result_path_best}: already DONE.")
            with open(result_path_best, "r") as file:
              best_info = json.load(file)
              if "input_params" in best_info and "output_params" in best_info:
                  best_params = best_info["output_params"]
              else:
                  best_params = best_info
            best_model = joblib.load(model_filename)
            return best_model, best_params


    shift = extract_shift_from_key(key)
    # Generate train/test
    # for grid_search prediction length is 1.
    train_x, test_x, train_y, test_y, train_index, test_index = gen_ml(
      wave=pd.DataFrame(data['train'].weekcase),
      test_length=1,
      parameter_length=1,
      lag_reserve=lagged_amount,
      predictions=[1],
      resid=False,
      shift=shift,
      trend_adjustment_steps=trend_adjustment_steps,
      adjustment=0,
      auto_generated_wave=True
    )


    if custom_steps:
        custom_steps(model, train_x, train_y)

    if use_random_state is not None and hasattr(model, "random_state"):
        model.random_state = 42 if use_random_state else None

    grid_search = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        scoring=scoring,
        cv=cv,
        n_jobs=-1,
        error_score='raise',
        verbose=2
    )
    grid_search.fit(train_x, train_y)

    # Save all CV results
    pd.DataFrame(grid_search.cv_results_).to_json(result_path_all, orient="records", indent=4)

    # Save best parameters with input param_grid as JSON
    best_params = grid_search.best_params_
    best_param_info = {
        "input_params": param_grid,
        "output_params": best_params
    }
    with open(result_path_best, "w") as file:
        json.dump(best_param_info, file, indent=4)
        print(f"Best parameters saved to: {result_path_best}")

    # Save model
    best_model = grid_search.best_estimator_
    joblib.dump(best_model, model_filename)
    print(f"Best model saved to: {model_filename}")

    return best_model, best_params


def run_grid_search_for_each_key(
    model_class,
    model_name,
    cv,
    param_grid,
    lagged_amount,
    data_dict,
    output_folder,
    tuned_on_filename,
    trend_adjustment_steps = [0],
    output_filename_template='{key}_{model_name}_results_{tuned_on_filename}.json',
    use_random_state=None,
    custom_steps=None,
    skip_existing_files=True,
    auto_generated_wave = True
):
    """
    Generalized function to perform grid search for each key in a data dictionary.

    Parameters:
        model_class: Class of the machine learning model (e.g., XGBClassifier, RandomForestClassifier).
        model_name: String identifier for the model.
        cv: Cross-validation object (e.g., TimeSeriesSplit).
        param_grid: Dictionary of hyperparameters for grid search.
        lagged_amount: Number of lagged points to include in the dataset.
        data_dict: Dictionary containing data for each key.
        trend_adjustment_steps: A list of shift values to apply to each wave.
        output_folder: Folder to save the grid search results.
        tuned_on_filename: Filename for the data the models are tuned on.
        output_filename_template: Template for the output filename, should include a placeholder for `key`.
        use_random_state: Optional; Boolean to control whether random state should be used (for specific models).
        custom_steps: Optional; Function for any custom preprocessing or steps specific to the model.
        skip_existing_files: If True, skip fitting models whose output already exists.
        auto_generated_wave: User defined (False) or auto generated (True)

    Returns:
        best_models: Dictionary containing the best model for each key.
        best_params: Dictionary containing the best parameters for each key.
    """
    best_models = {}
    best_params = {}

    for key in data_dict.keys():
        print(f"\nRunning grid search for key: {key}, model: {model_name}")
        start_time = time.time()

        best_model, best_params_for_key = run_grid_search(
            model=model_class(),
            cv=cv,
            param_grid=param_grid,
            lagged_amount=lagged_amount,
            key=key,
            data=data_dict[key],  # Process one key at a time
            trend_adjustment_steps = trend_adjustment_steps,
            output_folder=output_folder,
            output_filename=output_filename_template.format(
                key=key, model_name=model_name, tuned_on_filename=tuned_on_filename),
            use_random_state=use_random_state,
            custom_steps=custom_steps,
            skip_existing_files=skip_existing_files,
            auto_generated_wave = auto_generated_wave
        )

        end_time = time.time()
        fit_time = end_time - start_time

        best_models[key] = best_model
        best_params[key] = best_params_for_key

        print(f"Key: {key} - Best Model: {best_model}")
        print(f"Key: {key} - Best Parameters: {best_params_for_key}")
        print(f"Total fit time: {fit_time:.3f} seconds")

    return best_models, best_params