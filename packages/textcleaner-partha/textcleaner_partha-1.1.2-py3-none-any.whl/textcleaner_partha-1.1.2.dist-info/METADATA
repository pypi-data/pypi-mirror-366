Metadata-Version: 2.4
Name: textcleaner-partha
Version: 1.1.2
Summary: A lightweight and reusable text preprocessing package for NLP tasks
Home-page: https://github.com/partha6369/textcleaner
Author: Dr. Partha Majumdar
Author-email: "Dr. Partha Majumdar" <partha.majumdar@icloud.com>
License-Expression: MIT
Keywords: NLP,text preprocessing,lemmatization,contractions,emoji removal,spacy,autocorrect,tokens
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Education
Classifier: Intended Audience :: Science/Research
Classifier: Topic :: Text Processing
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Programming Language :: Python :: 3
Classifier: Operating System :: OS Independent
Requires-Python: >=3.7
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: spacy>=3.0.0
Requires-Dist: autocorrect>=2.6.1
Requires-Dist: contractions>=0.0.55
Requires-Dist: beautifulsoup4>=4.12.0
Requires-Dist: python-docx>=0.8.11
Requires-Dist: pypdf>=3.0.0
Dynamic: author
Dynamic: home-page
Dynamic: license-file
Dynamic: requires-python

# ğŸ§¹ textcleaner-partha

[![PyPI version](https://img.shields.io/pypi/v/textcleaner-partha?color=blue)](https://pypi.org/project/textcleaner-partha/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

A lightweight and reusable text preprocessing package for NLP tasks.
It cleans text by removing HTML tags and emojis, expanding contractions, correcting spelling, and performing lemmatization using spaCy.

## âœ¨ Features
	â€¢	âœ… HTML tag and emoji removal
	â€¢	âœ… Stopword removal
	â€¢	âœ… Contraction expansion (e.g., â€œcanâ€™tâ€ â†’ â€œcannotâ€)
	â€¢	âœ… Abbreviation expansion (e.g., â€œasapâ€ â†’ â€œas soon as possibleâ€)
	â€¢	âœ… Spelling correction with autocorrect
	â€¢	âœ… Lemmatisation using spaCy (en_core_web_sm)
	â€¢	âœ… Filters out stopwords, punctuation, numbers
	â€¢	âœ… Retains only nouns, verbs, adjectives, and adverbs
	â€¢	âœ… Returns tokens in a text


## ğŸš€ Installation

### From PyPI:

```bash
pip install --upgrade textcleaner-partha
```

Install directly from GitHub:

```bash
pip install git+https://github.com/partha6369/textcleaner.git
```

## ğŸ§  Usage

```python
from textcleaner_partha import preprocess

text = "I can't believe it's already raining! ğŸ˜ <p>Click here</p>"

# Default usage (all features enabled)
cleaned = preprocess(text)
print(cleaned)

# Custom usage with optional features disabled
cleaned_partial = preprocess(
    text,
    lemmatise=False,            # Skip spaCy processing (lemmatisation, POS filtering)
    correct_spelling=False,     # Skip spelling correction
    expand_contraction=False    # Skip contraction expansion
)
print(cleaned_partial)
```

```python
from textcleaner_partha import get_tokens

text = "I can't believe it's already raining! ğŸ˜ <p>Click here</p>"

# Default usage (all features enabled)
tokens = get_tokens(text)
print(tokens)

# Custom usage with optional features disabled
tokens_partial = get_tokens(
    text,
    lemmatise=False,            # Skip spaCy processing (lemmatisation, POS filtering)
    correct_spelling=False,     # Skip spelling correction
    expand_contraction=False    # Skip contraction expansion
)
print(tokens_partial)
```

## ğŸ”§ Parameters

The preprocess() and get_tokens() functions offer flexible control over each text cleaning step. You can selectively enable or disable operations using the parameters below:

```python
def preprocess(
    text,
    lowercase=True,
    remove_html=True,
    remove_emoji=True,
    remove_whitespace=True,
    remove_punct=False,
    expand_contraction=True,
    expand_abbrev=True,
    correct_spelling=True,
    lemmatise=True,
)
```

```python
def get_tokens(
    text,
    lowercase=True,
    remove_html=True,
    remove_emoji=True,
    remove_whitespace=True,
    remove_punct=False,
    expand_contraction=True,
    expand_abbrev=True,
    correct_spelling=True,
    lemmatise=True,
)
```

## ğŸ“¦ Dependencies

	â€¢	spacy
	â€¢	autocorrect
	â€¢	contractions

You can install them manually or via the included requirements.txt:
```bash
pip install -r requirements.txt
```

And download the required spaCy model:
```bash
python -m spacy download en_core_web_sm
```


## ğŸ“„ License

MIT License Â© Dr. Partha Majumdar
