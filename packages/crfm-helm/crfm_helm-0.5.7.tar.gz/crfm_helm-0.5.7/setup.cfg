[metadata]
name = crfm-helm
version = 0.5.7
author = Stanford CRFM
author_email = contact-crfm@stanford.edu
description = Benchmark for language models
long_description = file: README.md, docs/tutorial.md
long_description_content_type = text/markdown
keywords = language models benchmarking
license = Apache License 2.0
classifiers = 
	Programming Language :: Python :: 3
	Programming Language :: Python :: 3 :: Only
	Programming Language :: Python :: 3.9
	Programming Language :: Python :: 3.10
	Programming Language :: Python :: 3.11
	License :: OSI Approved :: Apache Software License
url = https://github.com/stanford-crfm/helm

[options]
python_requires = >=3.9
package_dir = 
	=src
packages = find:
zip_safe = False
include_package_data = True
install_requires = 
	cattrs~=22.2
	colorlog~=6.9
	dacite~=1.6
	importlib-resources~=5.10
	Mako~=1.2
	numpy>=1.26,<3
	pandas~=2.0
	pyhocon~=0.3.59
	retrying~=1.3
	spacy~=3.5
	tqdm~=4.64
	zstandard~=0.18.0
	sqlitedict>=2.1.0,<3.0
	bottle~=0.12.23
	
	datasets~=3.1
	pyarrow>=11.0.0  # Pinned transitive dependency for datasets; workaround for #1026
	pyarrow-hotfix~=0.6  # Hotfix for CVE-2023-47248
	
	nltk~=3.7,!=3.9.0  # Cannot use 3.9.0 due to https://github.com/nltk/nltk/issues/3308
	rouge-score~=0.1.2
	scipy>=1.10
	uncertainty-calibration~=0.1.4
	scikit-learn>=1.1
	
	transformers~=4.40,<4.53.0  # For anthropic_client, vision_language.huggingface_vlm_client, huggingface_client, huggingface_tokenizer, test_openai_token_cost_estimator, model_summac (via summarization_metrics)
	torch>=1.13.1,<3.0.0  # For huggingface_client, yalm_tokenizer, model_summac (via summarization_metrics)
	torchvision>=0.14.1,<3.0.0  # For huggingface_client, yalm_tokenizer, model_summac (via summarization_metrics)

[options.extras_require]
proxy-server = 
	gunicorn>=20.1
human-evaluation = 
	scaleapi~=2.13
	surge-api~=1.1
scenarios = 
	gdown~=5.1  # For disinformation_scenario, med_mcqa_scenario, med_qa_scenario: used by ensure_file_downloaded()
	xlrd~=2.0  # For ice_scenario: used by pandas.read_excel()
metrics = 
	google-api-python-client~=2.64  # For perspective_api_client via toxicity_metrics
	numba~=0.56  # For copyright_metrics
	sacrebleu~=2.2  # For disinformation_metrics, machine_translation_metrics
	langdetect~=1.0  # For ifeval_metrics
	immutabledict~=4.2  # For ifeval_metrics
	gradio_client~=1.3  # For bigcodebench_metrics
ranking = 
	pytrec_eval==0.5  # For ranking_metrics
summarization = 
	summ-eval~=0.892  # For summarization_metrics
	bert-score~=0.3  # For summarization_metrics
plots = 
	colorcet~=3.0
	matplotlib>=3.6.0
	seaborn>=0.11.0
decodingtrust = 
	fairlearn~=0.9.0
slurm = 
	simple-slurm~=0.2.6
cleva = 
	unidecode~=1.3
	pypinyin~=0.49.0
	jieba~=0.42.1
	opencc~=1.1
	langdetect~=1.0
images = 
	crfm-helm[accelerate]
	pillow~=10.2
mongo = 
	pymongo~=4.2
unitxt = 
	evaluate~=0.4.1
seahelm = 
	pythainlp==5.0.0
	pyonmttok==1.37.0
	sacrebleu~=2.2
	python-crfsuite~=0.9.11
accelerate = 
	accelerate~=0.25
aleph-alpha = 
	aleph-alpha-client~=2.14
	tokenizers>=0.13.3
allenai = 
	ai2-olmo~=0.2
amazon = 
	boto3~=1.34
	awscli~=1.33
	botocore~=1.34
anthropic = 
	anthropic~=0.48
	websocket-client~=1.3  # For legacy stanford-online-all-v4-s3
	httpx<0.28.0  # TODO(#3324): Remove this tepmorary workaround
cohere = 
	cohere~=5.3
writer = 
	writerai~=4.0
mistral = 
	mistralai~=1.1
openai = 
	openai~=1.70
	tiktoken~=0.7
	pydantic~=2.0  # For model_dump(mode="json") - openai only requires pydantic>=1.9.0
google = 
	google-cloud-aiplatform~=1.48
together = 
	together~=1.1
yandex = 
	sentencepiece~=0.2.0
models = 
	crfm-helm[ai21]
	crfm-helm[accelerate]
	crfm-helm[aleph-alpha]
	crfm-helm[allenai]
	crfm-helm[amazon]
	crfm-helm[anthropic]
	crfm-helm[cohere]
	crfm-helm[google]
	crfm-helm[mistral]
	crfm-helm[openai]
	crfm-helm[reka]
	crfm-helm[together]
	crfm-helm[yandex]
	crfm-helm[writer]
	crfm-helm[ibm-enterprise-scenarios]
reka = 
	reka-api~=2.0
vlm = 
	crfm-helm[openai]
	
	einops~=0.7.0
	einops-exts~=0.0.4
	open-clip-torch~=2.24
	
	torch~=2.1
	
	transformers_stream_generator~=0.0.4
	scipy~=1.10
	torchvision>=0.14.1,<3.0.0
	
	crfm-helm[reka]
	
	crfm-helm[images]
	crfm-helm[image2struct]
	
	pycocoevalcap~=1.2
	
	transformers~=4.45
	qwen-vl-utils~=0.0.8
ibm-enterprise-scenarios = 
	openpyxl~=3.1
ibm = 
	ibm-watsonx-ai~=1.2
image2struct = 
	crfm-helm[images]
	
	latex~=0.7.0
	pdf2image~=1.16
	
	selenium~=4.17
	html2text~=2024.2.26
	
	opencv-python-headless>=4.7.0.68,<=4.11.0.86
	lpips~=0.1.4
	imagehash~=4.3 # for caching
heim = 
	gdown~=5.1
	
	diffusers~=0.34.0
	icetk~=0.0.4
	jax~=0.6.2; python_version >= "3.10"
	jax~=0.4.30; python_version < "3.10"
	jaxlib~=0.6.2; python_version >= "3.10"
	jaxlib~=0.4.30; python_version < "3.10"
	crfm-helm[openai]
	
	einops~=0.7.0
	omegaconf~=2.3
	pytorch-lightning~=2.0
	
	flax~=0.10.7; python_version >= "3.10"
	flax~=0.8.5; python_version < "3.10"
	ftfy~=6.1
	Unidecode~=1.3
	wandb~=0.16
	
	google-cloud-translate~=3.11
	
	autokeras~=1.0
	clip-anytorch~=2.5
	google-cloud-storage~=2.9
	lpips~=0.1.4
	multilingual-clip~=1.0
	NudeNet~=2.0
	opencv-python>=4.7.0.68,<4.8.2.0; python_version >= "3.10"
	opencv-python-headless>=4.7.0.68,<=4.11.0.86; python_version < "3.10"
	pytorch-fid~=0.3.0
	tensorflow~=2.11
	timm~=0.6.12
	torch-fidelity~=0.3.0
	torchmetrics~=0.11.1
	
	scikit-image>=0.22,==0.*,!=0.23.*
	
	crfm-helm[images]
medhelm = 
	accelerate~=0.25
	crfm-helm[openai]
	crfm-helm[yandex]
	
	bert_score~=0.3.13
	lxml~=5.3
	openpyxl~=3.1
	python-docx~=1.1
	transformers~=4.45,<4.50
audiolm = 
	crfm-helm[openai]
	crfm-helm[google]
	
	pydub~=0.25.1
	
	ffmpeg-python~=0.2.0
	
	soundfile~=0.12
	librosa~=0.10
	einops~=0.7.0
	
	openai-whisper==20240930
	
	transformers~=4.48
	transformers_stream_generator~=0.0.4
	av~=14.3
	scipy~=1.10
	torchvision>=0.14.1,<3.0.0
	flash-attn~=2.7
	
	pycocoevalcap~=1.2
	jiwer~=3.0
	rapidfuzz~=3.10
	jieba~=0.42.1
codeinsights = 
	clang~=20.1
	Levenshtein~=0.27
lmkt = 
	sentence_transformers~=4.1
all = 
	crfm-helm[proxy-server]
	crfm-helm[human-evaluation]
	crfm-helm[scenarios]
	crfm-helm[metrics]
	crfm-helm[plots]
	crfm-helm[decodingtrust]
	crfm-helm[slurm]
	crfm-helm[cleva]
	crfm-helm[images]
	crfm-helm[models]
	crfm-helm[mongo]
	crfm-helm[heim]
	crfm-helm[vlm]
	crfm-helm[codeinsights]
	crfm-helm[lmkt]
dev = 
	pytest~=7.2.0
	xdoctest~=1.2.0
	pre-commit~=2.20.0
	black==24.3.0
	mypy==1.16.0
	flake8==5.0.4

[options.entry_points]
console_scripts = 
	helm-run = helm.benchmark.run:main
	helm-summarize = helm.benchmark.presentation.summarize:main
	helm-server = helm.benchmark.server:main
	helm-create-plots = helm.benchmark.presentation.create_plots:main
	crfm-proxy-server = helm.proxy.server:main
	crfm-proxy-cli = helm.proxy.cli:main

[options.packages.find]
where = src
exclude = 
	tests*

[flake8]
max-line-length = 120
exclude = 
	venv/*
	src/helm/clients/image_generation/dalle_mini/*
	src/helm/clients/image_generation/mindalle/*
	src/helm/clients/vision_language/open_flamingo/*
	src/helm/clients/audio_language/llama_omni/*
	src/helm/clients/audio_language/qwen_omni/*
ignore = E203,E231,E731,W503,W605,E501

[mypy]
ignore_missing_imports = True
check_untyped_defs = True
disable_error_code = annotation-unchecked
disallow_untyped_defs = False
exclude = dalle_mini|mindalle|open_flamingo|llama_omni|qwen_omni

[tool:pytest]
addopts = 
	-m 'not models and not scenarios'
	--xdoctest-modules
	--xdoctest-style=google
markers = 
	models
	scenarios

[egg_info]
tag_build = 
tag_date = 0

